{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset \n",
    "X_dev=pd.read_csv(\"../data/X_dev.csv\")\n",
    "y_dev=pd.read_csv(\"../data/y_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MEE(y_real, y_pred, **kwarg):\n",
    "    sum_t = 0\n",
    "    for i in range(len(y_real)):\n",
    "        sum_t += np.sqrt(np.power((y_real[i][0]-y_pred[i][0]), 2)+np.power((y_real[i][1]-y_pred[i][1]), 2))\n",
    "    return sum_t / len(y_real)\n",
    "\n",
    "MEE=make_scorer(MEE, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MEE_k(y_real, y_pred):\n",
    "     return K.mean(K.sqrt(K.sum(K.square(y_pred - y_real), axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=5, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=0.1, mom=0.1, alpha=0.01, unit1=10, unit2=10, act='sigmoid'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(unit1, input_dim=10, activation=act, \n",
    "                    kernel_regularizer=l2(alpha), \n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(unit2, activation=act, \n",
    "                    kernel_regularizer=l2(alpha),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=lr, momentum=mom))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/600\n",
      "1295/1295 [==============================] - 0s 91us/step - loss: 55.8135\n",
      "Epoch 2/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 48.1541\n",
      "Epoch 3/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 34.0795\n",
      "Epoch 4/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 20.1376\n",
      "Epoch 5/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 16.2789\n",
      "Epoch 6/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 14.7377\n",
      "Epoch 7/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 13.3138\n",
      "Epoch 8/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 11.0600\n",
      "Epoch 9/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 9.0459\n",
      "Epoch 10/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.6002\n",
      "Epoch 11/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 8.3568\n",
      "Epoch 12/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 8.1258\n",
      "Epoch 13/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 7.8565\n",
      "Epoch 14/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 7.6171\n",
      "Epoch 15/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 7.3178\n",
      "Epoch 16/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 7.1077\n",
      "Epoch 17/600\n",
      "1295/1295 [==============================] - 0s 76us/step - loss: 6.7693\n",
      "Epoch 18/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 6.4776\n",
      "Epoch 19/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 6.1946\n",
      "Epoch 20/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 6.0574\n",
      "Epoch 21/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 5.5559\n",
      "Epoch 22/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 5.2458\n",
      "Epoch 23/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 4.9609\n",
      "Epoch 24/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 4.7744\n",
      "Epoch 25/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 4.5305\n",
      "Epoch 26/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 4.2739\n",
      "Epoch 27/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 4.2744\n",
      "Epoch 28/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 4.3079\n",
      "Epoch 29/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 4.1603\n",
      "Epoch 30/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.8552\n",
      "Epoch 31/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.6233\n",
      "Epoch 32/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.5428\n",
      "Epoch 33/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.4774\n",
      "Epoch 34/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.4412\n",
      "Epoch 35/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.4480\n",
      "Epoch 36/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.3328\n",
      "Epoch 37/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.2998\n",
      "Epoch 38/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.2895\n",
      "Epoch 39/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.4987\n",
      "Epoch 40/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.9423\n",
      "Epoch 41/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.5197\n",
      "Epoch 42/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.1934\n",
      "Epoch 43/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.3408\n",
      "Epoch 44/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.8923\n",
      "Epoch 45/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.3581\n",
      "Epoch 46/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2302\n",
      "Epoch 47/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2377\n",
      "Epoch 48/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 3.1387\n",
      "Epoch 49/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1199\n",
      "Epoch 50/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.2980\n",
      "Epoch 51/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.2729\n",
      "Epoch 52/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1943\n",
      "Epoch 53/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0575\n",
      "Epoch 54/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.0081\n",
      "Epoch 55/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0230\n",
      "Epoch 56/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1815\n",
      "Epoch 57/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1084\n",
      "Epoch 58/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.1471\n",
      "Epoch 59/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.3635\n",
      "Epoch 60/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.5570\n",
      "Epoch 61/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0045\n",
      "Epoch 62/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9434\n",
      "Epoch 63/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 3.0175\n",
      "Epoch 64/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.1856\n",
      "Epoch 65/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.5283\n",
      "Epoch 66/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9853\n",
      "Epoch 67/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.1896\n",
      "Epoch 68/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.2012\n",
      "Epoch 69/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0237\n",
      "Epoch 70/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9764\n",
      "Epoch 71/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0216\n",
      "Epoch 72/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.9537\n",
      "Epoch 73/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0239\n",
      "Epoch 74/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0239\n",
      "Epoch 75/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8724\n",
      "Epoch 76/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.0204\n",
      "Epoch 77/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9117\n",
      "Epoch 78/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0224\n",
      "Epoch 79/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8961\n",
      "Epoch 80/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8749\n",
      "Epoch 81/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0005\n",
      "Epoch 82/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.9202\n",
      "Epoch 83/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8794\n",
      "Epoch 84/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8357\n",
      "Epoch 85/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8411\n",
      "Epoch 86/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9498\n",
      "Epoch 87/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.9248\n",
      "Epoch 88/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0021\n",
      "Epoch 89/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8342\n",
      "Epoch 90/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9212\n",
      "Epoch 91/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8163\n",
      "Epoch 92/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8732\n",
      "Epoch 93/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8128\n",
      "Epoch 94/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8392\n",
      "Epoch 95/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8923\n",
      "Epoch 96/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8293\n",
      "Epoch 97/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7789\n",
      "Epoch 98/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7932\n",
      "Epoch 99/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8378\n",
      "Epoch 100/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7879\n",
      "Epoch 101/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8146\n",
      "Epoch 102/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8581\n",
      "Epoch 103/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9507\n",
      "Epoch 104/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0475\n",
      "Epoch 105/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9282\n",
      "Epoch 106/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8296\n",
      "Epoch 107/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7591\n",
      "Epoch 108/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8265\n",
      "Epoch 109/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8477\n",
      "Epoch 110/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8509\n",
      "Epoch 111/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1754\n",
      "Epoch 112/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8843\n",
      "Epoch 113/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.8964\n",
      "Epoch 114/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8158\n",
      "Epoch 115/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9319\n",
      "Epoch 116/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7222\n",
      "Epoch 117/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8643\n",
      "Epoch 118/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.9818\n",
      "Epoch 119/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.7253\n",
      "Epoch 120/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7235\n",
      "Epoch 121/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7391\n",
      "Epoch 122/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.7691\n",
      "Epoch 123/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.7556\n",
      "Epoch 124/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 3.1009\n",
      "Epoch 125/600\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.8900\n",
      "Epoch 126/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7916\n",
      "Epoch 127/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7472\n",
      "Epoch 128/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8383\n",
      "Epoch 129/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7234\n",
      "Epoch 130/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7895\n",
      "Epoch 131/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9725\n",
      "Epoch 132/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6886\n",
      "Epoch 133/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8011\n",
      "Epoch 134/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6960\n",
      "Epoch 135/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6909\n",
      "Epoch 136/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7069\n",
      "Epoch 137/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6755\n",
      "Epoch 138/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9002\n",
      "Epoch 139/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8683\n",
      "Epoch 140/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8475\n",
      "Epoch 141/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9347\n",
      "Epoch 142/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7599\n",
      "Epoch 143/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7233\n",
      "Epoch 144/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6995\n",
      "Epoch 145/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7720\n",
      "Epoch 146/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6642\n",
      "Epoch 147/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7806\n",
      "Epoch 148/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9879\n",
      "Epoch 149/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7512\n",
      "Epoch 150/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7627\n",
      "Epoch 151/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8916\n",
      "Epoch 152/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6585\n",
      "Epoch 153/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8173\n",
      "Epoch 154/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6656\n",
      "Epoch 155/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7053\n",
      "Epoch 156/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6575\n",
      "Epoch 157/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6559\n",
      "Epoch 158/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7282\n",
      "Epoch 159/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8164\n",
      "Epoch 160/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6784\n",
      "Epoch 161/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6349\n",
      "Epoch 162/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6404\n",
      "Epoch 163/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7562\n",
      "Epoch 164/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8499\n",
      "Epoch 165/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7351\n",
      "Epoch 166/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6291\n",
      "Epoch 167/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6727\n",
      "Epoch 168/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8163\n",
      "Epoch 169/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8625\n",
      "Epoch 170/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6658\n",
      "Epoch 171/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7364\n",
      "Epoch 172/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7241\n",
      "Epoch 173/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7112\n",
      "Epoch 174/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6265\n",
      "Epoch 175/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6726\n",
      "Epoch 176/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6446\n",
      "Epoch 177/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6873\n",
      "Epoch 178/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6813\n",
      "Epoch 179/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6921\n",
      "Epoch 180/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7451\n",
      "Epoch 181/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6846\n",
      "Epoch 182/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6546\n",
      "Epoch 183/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6672\n",
      "Epoch 184/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7207\n",
      "Epoch 185/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8896\n",
      "Epoch 186/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6407\n",
      "Epoch 187/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7571\n",
      "Epoch 188/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7991\n",
      "Epoch 189/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8250\n",
      "Epoch 190/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6758\n",
      "Epoch 191/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6143\n",
      "Epoch 192/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5877\n",
      "Epoch 193/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6397\n",
      "Epoch 194/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6915\n",
      "Epoch 195/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6082\n",
      "Epoch 196/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7073\n",
      "Epoch 197/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9096\n",
      "Epoch 198/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7408\n",
      "Epoch 199/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8552\n",
      "Epoch 200/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7805\n",
      "Epoch 201/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8722\n",
      "Epoch 202/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6163\n",
      "Epoch 203/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6179\n",
      "Epoch 204/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7002\n",
      "Epoch 205/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5848\n",
      "Epoch 206/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6290\n",
      "Epoch 207/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6438\n",
      "Epoch 208/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6009\n",
      "Epoch 209/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6024\n",
      "Epoch 210/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6154\n",
      "Epoch 211/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5819\n",
      "Epoch 212/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6909\n",
      "Epoch 213/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5936\n",
      "Epoch 214/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5958\n",
      "Epoch 215/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6059\n",
      "Epoch 216/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6303\n",
      "Epoch 217/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7662\n",
      "Epoch 218/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5740\n",
      "Epoch 219/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5978\n",
      "Epoch 220/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7156\n",
      "Epoch 221/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6791\n",
      "Epoch 222/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5727\n",
      "Epoch 223/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7827\n",
      "Epoch 224/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5994\n",
      "Epoch 225/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6204\n",
      "Epoch 226/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6798\n",
      "Epoch 227/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7134\n",
      "Epoch 228/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5606\n",
      "Epoch 229/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5744\n",
      "Epoch 230/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6291\n",
      "Epoch 231/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6270\n",
      "Epoch 232/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7850\n",
      "Epoch 233/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5642\n",
      "Epoch 234/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5560\n",
      "Epoch 235/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5518\n",
      "Epoch 236/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6181\n",
      "Epoch 237/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6093\n",
      "Epoch 238/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.6073\n",
      "Epoch 239/600\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.5715\n",
      "Epoch 240/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5357\n",
      "Epoch 241/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.565 - 0s 14us/step - loss: 2.5743\n",
      "Epoch 242/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6325\n",
      "Epoch 243/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5969\n",
      "Epoch 244/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5842\n",
      "Epoch 245/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6341\n",
      "Epoch 246/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5753\n",
      "Epoch 247/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5320\n",
      "Epoch 248/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5646\n",
      "Epoch 249/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6535\n",
      "Epoch 250/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7721\n",
      "Epoch 251/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5880\n",
      "Epoch 252/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5277\n",
      "Epoch 253/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6031\n",
      "Epoch 254/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6977\n",
      "Epoch 255/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5750\n",
      "Epoch 256/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5055\n",
      "Epoch 257/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5073\n",
      "Epoch 258/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5893\n",
      "Epoch 259/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6407\n",
      "Epoch 260/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5520\n",
      "Epoch 261/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5288\n",
      "Epoch 262/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5093\n",
      "Epoch 263/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5231\n",
      "Epoch 264/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5525\n",
      "Epoch 265/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5954\n",
      "Epoch 266/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5125\n",
      "Epoch 267/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5544\n",
      "Epoch 268/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6679\n",
      "Epoch 269/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6044\n",
      "Epoch 270/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6027\n",
      "Epoch 271/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6404\n",
      "Epoch 272/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5059\n",
      "Epoch 273/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5939\n",
      "Epoch 274/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6383\n",
      "Epoch 275/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5343\n",
      "Epoch 276/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5113\n",
      "Epoch 277/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5739\n",
      "Epoch 278/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5546\n",
      "Epoch 279/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6283\n",
      "Epoch 280/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5230\n",
      "Epoch 281/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6426\n",
      "Epoch 282/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5118\n",
      "Epoch 283/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5808\n",
      "Epoch 284/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5708\n",
      "Epoch 285/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5011\n",
      "Epoch 286/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7793\n",
      "Epoch 287/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5720\n",
      "Epoch 288/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5301\n",
      "Epoch 289/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5084\n",
      "Epoch 290/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4917\n",
      "Epoch 291/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6143\n",
      "Epoch 292/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5365\n",
      "Epoch 293/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5121\n",
      "Epoch 294/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6645\n",
      "Epoch 295/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5389\n",
      "Epoch 296/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4858\n",
      "Epoch 297/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6199\n",
      "Epoch 298/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5159\n",
      "Epoch 299/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4873\n",
      "Epoch 300/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4886\n",
      "Epoch 301/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7411\n",
      "Epoch 302/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5115\n",
      "Epoch 303/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6740\n",
      "Epoch 304/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5315\n",
      "Epoch 305/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4695\n",
      "Epoch 306/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4923\n",
      "Epoch 307/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5466\n",
      "Epoch 308/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6667\n",
      "Epoch 309/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.8343\n",
      "Epoch 310/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5100\n",
      "Epoch 311/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4970\n",
      "Epoch 312/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5038\n",
      "Epoch 313/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5041\n",
      "Epoch 314/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5330\n",
      "Epoch 315/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4813\n",
      "Epoch 316/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4947\n",
      "Epoch 317/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7107\n",
      "Epoch 318/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5379\n",
      "Epoch 319/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4678\n",
      "Epoch 320/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4795\n",
      "Epoch 321/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5767\n",
      "Epoch 322/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5317\n",
      "Epoch 323/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4494\n",
      "Epoch 324/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4570\n",
      "Epoch 325/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4622\n",
      "Epoch 326/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5939\n",
      "Epoch 327/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4662\n",
      "Epoch 328/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5228\n",
      "Epoch 329/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6534\n",
      "Epoch 330/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4438\n",
      "Epoch 331/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4866\n",
      "Epoch 332/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5286\n",
      "Epoch 333/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5141\n",
      "Epoch 334/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5569\n",
      "Epoch 335/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4767\n",
      "Epoch 336/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7275\n",
      "Epoch 337/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5413\n",
      "Epoch 338/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4844\n",
      "Epoch 339/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4534\n",
      "Epoch 340/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4554\n",
      "Epoch 341/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4737\n",
      "Epoch 342/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5472\n",
      "Epoch 343/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5710\n",
      "Epoch 344/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5942\n",
      "Epoch 345/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5640\n",
      "Epoch 346/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4967\n",
      "Epoch 347/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4786\n",
      "Epoch 348/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4878\n",
      "Epoch 349/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5286\n",
      "Epoch 350/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5234\n",
      "Epoch 351/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4874\n",
      "Epoch 352/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.459 - 0s 9us/step - loss: 2.4333\n",
      "Epoch 353/600\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.4870\n",
      "Epoch 354/600\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4513\n",
      "Epoch 355/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.580 - 0s 30us/step - loss: 2.5308\n",
      "Epoch 356/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.4413\n",
      "Epoch 357/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5089\n",
      "Epoch 358/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.7244\n",
      "Epoch 359/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5827\n",
      "Epoch 360/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5082\n",
      "Epoch 361/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5541\n",
      "Epoch 362/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6726\n",
      "Epoch 363/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4735\n",
      "Epoch 364/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4229\n",
      "Epoch 365/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4513\n",
      "Epoch 366/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4625\n",
      "Epoch 367/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7287\n",
      "Epoch 368/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4860\n",
      "Epoch 369/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5381\n",
      "Epoch 370/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6752\n",
      "Epoch 371/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6230\n",
      "Epoch 372/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5198\n",
      "Epoch 373/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5978\n",
      "Epoch 374/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4739\n",
      "Epoch 375/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4584\n",
      "Epoch 376/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5779\n",
      "Epoch 377/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6157\n",
      "Epoch 378/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5720\n",
      "Epoch 379/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4253\n",
      "Epoch 380/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4548\n",
      "Epoch 381/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4634\n",
      "Epoch 382/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4371\n",
      "Epoch 383/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5702\n",
      "Epoch 384/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5230\n",
      "Epoch 385/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4302\n",
      "Epoch 386/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4680\n",
      "Epoch 387/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4965\n",
      "Epoch 388/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4348\n",
      "Epoch 389/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4342\n",
      "Epoch 390/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5362\n",
      "Epoch 391/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7717\n",
      "Epoch 392/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5500\n",
      "Epoch 393/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6382\n",
      "Epoch 394/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4534\n",
      "Epoch 395/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4434\n",
      "Epoch 396/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4678\n",
      "Epoch 397/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4835\n",
      "Epoch 398/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4464\n",
      "Epoch 399/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4378\n",
      "Epoch 400/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5898\n",
      "Epoch 401/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5006\n",
      "Epoch 402/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4188\n",
      "Epoch 403/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4261\n",
      "Epoch 404/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4028\n",
      "Epoch 405/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4215\n",
      "Epoch 406/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4311\n",
      "Epoch 407/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5395\n",
      "Epoch 408/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5207\n",
      "Epoch 409/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6295\n",
      "Epoch 410/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4564\n",
      "Epoch 411/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5171\n",
      "Epoch 412/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6870\n",
      "Epoch 413/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4125\n",
      "Epoch 414/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4334\n",
      "Epoch 415/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5406\n",
      "Epoch 416/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5505\n",
      "Epoch 417/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6713\n",
      "Epoch 418/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4891\n",
      "Epoch 419/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4343\n",
      "Epoch 420/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4023\n",
      "Epoch 421/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5045\n",
      "Epoch 422/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6104\n",
      "Epoch 423/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4280\n",
      "Epoch 424/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4757\n",
      "Epoch 425/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6038\n",
      "Epoch 426/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4269\n",
      "Epoch 427/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5159\n",
      "Epoch 428/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5041\n",
      "Epoch 429/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4595\n",
      "Epoch 430/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4518\n",
      "Epoch 431/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4123\n",
      "Epoch 432/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4267\n",
      "Epoch 433/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3772\n",
      "Epoch 434/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3966\n",
      "Epoch 435/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.482 - 0s 8us/step - loss: 2.4649\n",
      "Epoch 436/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5325\n",
      "Epoch 437/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5413\n",
      "Epoch 438/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5748\n",
      "Epoch 439/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3849\n",
      "Epoch 440/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4340\n",
      "Epoch 441/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4121\n",
      "Epoch 442/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4702\n",
      "Epoch 443/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.3888\n",
      "Epoch 444/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.3754\n",
      "Epoch 445/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4102\n",
      "Epoch 446/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6587\n",
      "Epoch 447/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.4277\n",
      "Epoch 448/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3731\n",
      "Epoch 449/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5518\n",
      "Epoch 450/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4118\n",
      "Epoch 451/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6461\n",
      "Epoch 452/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4007\n",
      "Epoch 453/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4085\n",
      "Epoch 454/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4853\n",
      "Epoch 455/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3851\n",
      "Epoch 456/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3619\n",
      "Epoch 457/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4332\n",
      "Epoch 458/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4409\n",
      "Epoch 459/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4260\n",
      "Epoch 460/600\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.5156\n",
      "Epoch 461/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4688\n",
      "Epoch 462/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3959\n",
      "Epoch 463/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4447\n",
      "Epoch 464/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4554\n",
      "Epoch 465/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4966\n",
      "Epoch 466/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4460\n",
      "Epoch 467/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5409\n",
      "Epoch 468/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3644\n",
      "Epoch 469/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4789\n",
      "Epoch 470/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3928\n",
      "Epoch 471/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3991\n",
      "Epoch 472/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5199\n",
      "Epoch 473/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4261\n",
      "Epoch 474/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3919\n",
      "Epoch 475/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4123\n",
      "Epoch 476/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3629\n",
      "Epoch 477/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4173\n",
      "Epoch 478/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5025\n",
      "Epoch 479/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5681\n",
      "Epoch 480/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3954\n",
      "Epoch 481/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3864\n",
      "Epoch 482/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4219\n",
      "Epoch 483/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5054\n",
      "Epoch 484/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4297\n",
      "Epoch 485/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4323\n",
      "Epoch 486/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3798\n",
      "Epoch 487/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.3827\n",
      "Epoch 488/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5455\n",
      "Epoch 489/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4563\n",
      "Epoch 490/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4816\n",
      "Epoch 491/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3498\n",
      "Epoch 492/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4066\n",
      "Epoch 493/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3828\n",
      "Epoch 494/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3408\n",
      "Epoch 495/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3617\n",
      "Epoch 496/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3768\n",
      "Epoch 497/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4282\n",
      "Epoch 498/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3822\n",
      "Epoch 499/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3717\n",
      "Epoch 500/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3739\n",
      "Epoch 501/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3548\n",
      "Epoch 502/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5426\n",
      "Epoch 503/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4252\n",
      "Epoch 504/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5299\n",
      "Epoch 505/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3366\n",
      "Epoch 506/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3795\n",
      "Epoch 507/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4417\n",
      "Epoch 508/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4381\n",
      "Epoch 509/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5039\n",
      "Epoch 510/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4372\n",
      "Epoch 511/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4401\n",
      "Epoch 512/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3616\n",
      "Epoch 513/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3337\n",
      "Epoch 514/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4217\n",
      "Epoch 515/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4836\n",
      "Epoch 516/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3900\n",
      "Epoch 517/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3346\n",
      "Epoch 518/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4288\n",
      "Epoch 519/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4316\n",
      "Epoch 520/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4871\n",
      "Epoch 521/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4585\n",
      "Epoch 522/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4400\n",
      "Epoch 523/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5225\n",
      "Epoch 524/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3478\n",
      "Epoch 525/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5198\n",
      "Epoch 526/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3622\n",
      "Epoch 527/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3719\n",
      "Epoch 528/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3790\n",
      "Epoch 529/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3444\n",
      "Epoch 530/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3282\n",
      "Epoch 531/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3478\n",
      "Epoch 532/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4199\n",
      "Epoch 533/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3960\n",
      "Epoch 534/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5254\n",
      "Epoch 535/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5058\n",
      "Epoch 536/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4994\n",
      "Epoch 537/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3624\n",
      "Epoch 538/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3736\n",
      "Epoch 539/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5232\n",
      "Epoch 540/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4223\n",
      "Epoch 541/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4424\n",
      "Epoch 542/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4184\n",
      "Epoch 543/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4565\n",
      "Epoch 544/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4323\n",
      "Epoch 545/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3890\n",
      "Epoch 546/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.426 - 0s 6us/step - loss: 2.3887\n",
      "Epoch 547/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4624\n",
      "Epoch 548/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3284\n",
      "Epoch 549/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4422\n",
      "Epoch 550/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4115\n",
      "Epoch 551/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5200\n",
      "Epoch 552/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3183\n",
      "Epoch 553/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3665\n",
      "Epoch 554/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4293\n",
      "Epoch 555/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3047\n",
      "Epoch 556/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4019\n",
      "Epoch 557/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6237\n",
      "Epoch 558/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3477\n",
      "Epoch 559/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4404\n",
      "Epoch 560/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3568\n",
      "Epoch 561/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3885\n",
      "Epoch 562/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4186\n",
      "Epoch 563/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4617\n",
      "Epoch 564/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4656\n",
      "Epoch 565/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4323\n",
      "Epoch 566/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3929\n",
      "Epoch 567/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4506\n",
      "Epoch 568/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4555\n",
      "Epoch 569/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5004\n",
      "Epoch 570/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3207\n",
      "Epoch 571/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3320\n",
      "Epoch 572/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5536\n",
      "Epoch 573/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4418\n",
      "Epoch 574/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3651\n",
      "Epoch 575/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4180\n",
      "Epoch 576/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3959\n",
      "Epoch 577/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4009\n",
      "Epoch 578/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4821\n",
      "Epoch 579/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4646\n",
      "Epoch 580/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3241\n",
      "Epoch 581/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4084\n",
      "Epoch 582/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3260\n",
      "Epoch 583/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.752 - 0s 7us/step - loss: 2.6663\n",
      "Epoch 584/600\n",
      "1295/1295 [==============================] - 0s 34us/step - loss: 2.3963\n",
      "Epoch 585/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.3611\n",
      "Epoch 586/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3021\n",
      "Epoch 587/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3301\n",
      "Epoch 588/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5058\n",
      "Epoch 589/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.3275\n",
      "Epoch 590/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4208\n",
      "Epoch 591/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5515\n",
      "Epoch 592/600\n",
      "1295/1295 [==============================] - 0s 45us/step - loss: 2.5044\n",
      "Epoch 593/600\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.3010\n",
      "Epoch 594/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.3406\n",
      "Epoch 595/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3563\n",
      "Epoch 596/600\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.3094\n",
      "Epoch 597/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4409\n",
      "Epoch 598/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.2895\n",
      "Epoch 599/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4306\n",
      "Epoch 600/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4069\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "hyper_params_space = {\n",
    "        'unit1' : [25, 30, 40, 50, 80],\n",
    "        'lr' : [0.001, 0.01, 0.1, 0.2, 0.3, 0.5],\n",
    "        'unit2': [5, 15, 25, 35],\n",
    "        'mom' : [0.01, 0.1, 0.5, 0.7, 0.9],\n",
    "        'alpha' : [0.1, 1e-5, 1e-8, 1e-10, 1e-20],\n",
    "        'act' : ['sigmoid', 'softmax']\n",
    "    },\n",
    "\n",
    "\n",
    "print('===================================')\n",
    "model = KerasRegressor(build_fn=create_model, batch_size=1036, epochs=600)\n",
    "mlpr = GridSearchCV( model\n",
    "                    , hyper_params_space, scoring='r2', cv=kf, \n",
    "                    refit='r2', \n",
    "                    n_jobs=2)\n",
    "mlpr.fit(X_dev, y_dev)\n",
    "print(\"DONE\")\n",
    "mlpr.best_estimator_.model.save(\"Models/test_model2HL.h5\")\n",
    "resultGSCV=pd.DataFrame(mlpr.cv_results_)\n",
    "#print(pd.DataFrame(mlpr.cv_results_))\n",
    "resultGSCV.to_csv(r'try2HL.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': 'sigmoid',\n",
       " 'alpha': 1e-05,\n",
       " 'lr': 0.3,\n",
       " 'mom': 0.5,\n",
       " 'unit1': 50,\n",
       " 'unit2': 25}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949160150566584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev=X_dev.to_numpy()\n",
    "y_dev=y_dev.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modelLC():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=mlpr.best_params_['lr'], \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 141us/step - loss: 56.6659 - val_loss: 53.9298\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 53.8348 - val_loss: 49.3813\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.2688 - val_loss: 43.2751\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 43.1380 - val_loss: 35.7936\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 35.6133 - val_loss: 27.8206\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 27.4906 - val_loss: 21.6895\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.1031 - val_loss: 18.6911\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.0720 - val_loss: 17.2001\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.5651 - val_loss: 16.4146\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.7939 - val_loss: 15.8139\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.2060 - val_loss: 15.0435\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.4325 - val_loss: 13.9514\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 13.3195 - val_loss: 12.7246\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.0894 - val_loss: 11.5492\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.9890 - val_loss: 10.4430\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.9446 - val_loss: 9.6529\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 9.2140 - val_loss: 9.2811\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.8805 - val_loss: 9.0373\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.6757 - val_loss: 8.8656\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5444 - val_loss: 8.7394\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4280 - val_loss: 8.6281\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3024 - val_loss: 8.5088\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1724 - val_loss: 8.3860\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.0455 - val_loss: 8.2719\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9258 - val_loss: 8.1602\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.8082 - val_loss: 8.0421\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6895 - val_loss: 7.9157\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.5665 - val_loss: 7.7835\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.4380 - val_loss: 7.6466\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.3036 - val_loss: 7.5042\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.1630 - val_loss: 7.3558\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.0164 - val_loss: 7.2024\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.8643 - val_loss: 7.0452\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.7073 - val_loss: 6.8845\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 6.5464 - val_loss: 6.7218\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.3827 - val_loss: 6.5583\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.2176 - val_loss: 6.3944\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.0524 - val_loss: 6.2303\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.8875 - val_loss: 6.0661\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.7235 - val_loss: 5.9017\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.5602 - val_loss: 5.7369\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.3975 - val_loss: 5.5712\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.2360 - val_loss: 5.4081\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 5.0780 - val_loss: 5.2541\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.9278 - val_loss: 5.1148\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.7891 - val_loss: 4.9800\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.6625 - val_loss: 4.8625\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.5491 - val_loss: 4.7491\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.4491 - val_loss: 4.6636\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.3609 - val_loss: 4.5693\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.2824 - val_loss: 4.5047\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.2114 - val_loss: 4.4191\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.1480 - val_loss: 4.3779\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0944 - val_loss: 4.3039\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.0574 - val_loss: 4.3195\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.0446 - val_loss: 4.2606\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0472 - val_loss: 4.2969\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0314 - val_loss: 4.1694\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.9755 - val_loss: 4.1561\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.8993 - val_loss: 4.0217\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8220 - val_loss: 4.0040\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7521 - val_loss: 3.9011\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7047 - val_loss: 3.9012\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.6636 - val_loss: 3.8157\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6286 - val_loss: 3.8296\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.6025 - val_loss: 3.7465\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5754 - val_loss: 3.7646\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.5521 - val_loss: 3.6826\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.5205 - val_loss: 3.6929\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.4930 - val_loss: 3.6154\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4616 - val_loss: 3.6342\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4403 - val_loss: 3.5617\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4168 - val_loss: 3.5937\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4048 - val_loss: 3.5241\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.3925 - val_loss: 3.5624\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3798 - val_loss: 3.4856\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3640 - val_loss: 3.5375\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3559 - val_loss: 3.4543\n",
      "Epoch 79/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3445 - val_loss: 3.5111\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3340 - val_loss: 3.4230\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3221 - val_loss: 3.4791\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3057 - val_loss: 3.3901\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2959 - val_loss: 3.4491\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2803 - val_loss: 3.3599\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2713 - val_loss: 3.4236\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2591 - val_loss: 3.3345\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2506 - val_loss: 3.4019\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2415 - val_loss: 3.3138\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2360 - val_loss: 3.3856\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.2282 - val_loss: 3.2949\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2222 - val_loss: 3.3720\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2160 - val_loss: 3.2799\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2119 - val_loss: 3.3591\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2063 - val_loss: 3.2651\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1974 - val_loss: 3.3387\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1894 - val_loss: 3.2429\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1729 - val_loss: 3.3071\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1611 - val_loss: 3.2158\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1406 - val_loss: 3.2637\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1227 - val_loss: 3.1878\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1091 - val_loss: 3.2382\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0980 - val_loss: 3.1713\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0915 - val_loss: 3.2259\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0865 - val_loss: 3.1641\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0846 - val_loss: 3.2250\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0856 - val_loss: 3.1615\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0836 - val_loss: 3.2277\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0875 - val_loss: 3.1600\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0832 - val_loss: 3.2365\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0948 - val_loss: 3.1618\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0897 - val_loss: 3.2544\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1105 - val_loss: 3.1721\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1077 - val_loss: 3.2747\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1292 - val_loss: 3.1722\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1113 - val_loss: 3.2621\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1167 - val_loss: 3.1538\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0888 - val_loss: 3.2297\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0846 - val_loss: 3.1310\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0608 - val_loss: 3.1981\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0535 - val_loss: 3.1069\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0291 - val_loss: 3.1516\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0102 - val_loss: 3.0740\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9851 - val_loss: 3.1142\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9714 - val_loss: 3.0587\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9622 - val_loss: 3.1053\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9622 - val_loss: 3.0558\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9619 - val_loss: 3.1103\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9670 - val_loss: 3.0594\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9708 - val_loss: 3.1257\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9818 - val_loss: 3.0667\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9868 - val_loss: 3.1525\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0051 - val_loss: 3.0795\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0082 - val_loss: 3.1845\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0348 - val_loss: 3.0985\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0355 - val_loss: 3.2067\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0578 - val_loss: 3.0978\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0344 - val_loss: 3.1876\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0377 - val_loss: 3.0769\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0064 - val_loss: 3.1504\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0022 - val_loss: 3.0478\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.9686 - val_loss: 3.0815\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9375 - val_loss: 3.0058\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9065 - val_loss: 3.0310\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8904 - val_loss: 2.9885\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8785 - val_loss: 3.0199\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8789 - val_loss: 2.9862\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8781 - val_loss: 3.0253\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8845 - val_loss: 2.9887\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8846 - val_loss: 3.0328\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8901 - val_loss: 2.9886\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8850 - val_loss: 3.0303\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8866 - val_loss: 2.9841\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8791 - val_loss: 3.0248\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8807 - val_loss: 2.9799\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8734 - val_loss: 3.0210\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8763 - val_loss: 2.9765\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8694 - val_loss: 3.0187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8730 - val_loss: 2.9735\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8658 - val_loss: 3.0160\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8694 - val_loss: 2.9701\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8616 - val_loss: 3.0130\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8653 - val_loss: 2.9670\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8581 - val_loss: 3.0108\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8622 - val_loss: 2.9646\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8554 - val_loss: 3.0092\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8596 - val_loss: 2.9619\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8519 - val_loss: 3.0064\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8559 - val_loss: 2.9585\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8472 - val_loss: 3.0027\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8512 - val_loss: 2.9547\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8422 - val_loss: 2.9926\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8420 - val_loss: 2.9468\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8319 - val_loss: 2.9820\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8318 - val_loss: 2.9413\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8247 - val_loss: 2.9773\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8266 - val_loss: 2.9385\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8213 - val_loss: 2.9765\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8245 - val_loss: 2.9371\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8197 - val_loss: 2.9769\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8236 - val_loss: 2.9362\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8185 - val_loss: 2.9780\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8235 - val_loss: 2.9361\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8181 - val_loss: 2.9812\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8255 - val_loss: 2.9388\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8213 - val_loss: 2.9913\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8340 - val_loss: 2.9450\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8294 - val_loss: 3.0065\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8463 - val_loss: 2.9494\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8369 - val_loss: 3.0181\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8546 - val_loss: 2.9530\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8435 - val_loss: 3.0287\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8625 - val_loss: 2.9597\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8552 - val_loss: 3.0558\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8853 - val_loss: 2.9777\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8828 - val_loss: 3.1085\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9297 - val_loss: 3.0114\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9318 - val_loss: 3.1541\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9678 - val_loss: 3.0158\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9367 - val_loss: 3.1220\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9369 - val_loss: 2.9822\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8889 - val_loss: 3.0591\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.8800 - val_loss: 2.9435\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8340 - val_loss: 2.9845\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8146 - val_loss: 2.9036\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7733 - val_loss: 2.9230\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7559 - val_loss: 2.8801\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7397 - val_loss: 2.8990\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7323 - val_loss: 2.8715\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7298 - val_loss: 2.9006\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7322 - val_loss: 2.8738\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7348 - val_loss: 2.9094\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7396 - val_loss: 2.8781\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7406 - val_loss: 2.9196\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7482 - val_loss: 2.8826\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7472 - val_loss: 2.9304\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7573 - val_loss: 2.8866\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7541 - val_loss: 2.9396\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7640 - val_loss: 2.8906\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7587 - val_loss: 2.9464\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7688 - val_loss: 2.8926\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7609 - val_loss: 2.9500\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7705 - val_loss: 2.8922\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7603 - val_loss: 2.9509\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7698 - val_loss: 2.8908\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7584 - val_loss: 2.9509\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7682 - val_loss: 2.8899\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7570 - val_loss: 2.9522\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7679 - val_loss: 2.8892\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7562 - val_loss: 2.9543\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7680 - val_loss: 2.8888\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7554 - val_loss: 2.9557\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7677 - val_loss: 2.8882\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7543 - val_loss: 2.9565\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7668 - val_loss: 2.8872\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7527 - val_loss: 2.9566\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7653 - val_loss: 2.8858\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7506 - val_loss: 2.9562\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7633 - val_loss: 2.8843\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7483 - val_loss: 2.9557\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7612 - val_loss: 2.8828\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7459 - val_loss: 2.9551\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7590 - val_loss: 2.8813\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7436 - val_loss: 2.9546\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7571 - val_loss: 2.8804\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7420 - val_loss: 2.9551\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7563 - val_loss: 2.8809\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7423 - val_loss: 2.9580\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7578 - val_loss: 2.8816\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7435 - val_loss: 2.9610\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7591 - val_loss: 2.8807\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7425 - val_loss: 2.9603\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7565 - val_loss: 2.8785\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7392 - val_loss: 2.9558\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7510 - val_loss: 2.8753\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7341 - val_loss: 2.9479\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7431 - val_loss: 2.8693\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7254 - val_loss: 2.9341\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7297 - val_loss: 2.8608\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7129 - val_loss: 2.9152\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7106 - val_loss: 2.8493\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6958 - val_loss: 2.8967\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6934 - val_loss: 2.8419\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6839 - val_loss: 2.8921\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6883 - val_loss: 2.8408\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6826 - val_loss: 2.8950\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6904 - val_loss: 2.8451\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6898 - val_loss: 2.9069\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6998 - val_loss: 2.8519\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6988 - val_loss: 2.9250\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7158 - val_loss: 2.8612\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7122 - val_loss: 2.9477\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7350 - val_loss: 2.8716\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7275 - val_loss: 2.9644\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7480 - val_loss: 2.8798\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7402 - val_loss: 2.9794\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7588 - val_loss: 2.8827\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7435 - val_loss: 2.9730\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7517 - val_loss: 2.8739\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7307 - val_loss: 2.9564\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7362 - val_loss: 2.8615\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7115 - val_loss: 2.9330\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7138 - val_loss: 2.8506\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6953 - val_loss: 2.9142\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6948 - val_loss: 2.8419\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6829 - val_loss: 2.9016\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6826 - val_loss: 2.8368\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6740 - val_loss: 2.8944\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6759 - val_loss: 2.8343\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6696 - val_loss: 2.8947\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6762 - val_loss: 2.8355\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6713 - val_loss: 2.8997\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6796 - val_loss: 2.8375\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6745 - val_loss: 2.9054\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6834 - val_loss: 2.8395\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6770 - val_loss: 2.9104\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6865 - val_loss: 2.8412\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6791 - val_loss: 2.9155\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6898 - val_loss: 2.8431\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6817 - val_loss: 2.9206\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6930 - val_loss: 2.8449\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6839 - val_loss: 2.9242\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6948 - val_loss: 2.8454\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6842 - val_loss: 2.9248\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6939 - val_loss: 2.8443\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6821 - val_loss: 2.9222\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6904 - val_loss: 2.8421\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6782 - val_loss: 2.9173\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6849 - val_loss: 2.8389\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6728 - val_loss: 2.9103\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6776 - val_loss: 2.8347\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6662 - val_loss: 2.9007\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6682 - val_loss: 2.8292\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6573 - val_loss: 2.8934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6605 - val_loss: 2.8260\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6522 - val_loss: 2.8906\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6570 - val_loss: 2.8251\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6507 - val_loss: 2.8917\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6568 - val_loss: 2.8256\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6510 - val_loss: 2.8939\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6575 - val_loss: 2.8263\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6516 - val_loss: 2.8955\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6577 - val_loss: 2.8267\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6514 - val_loss: 2.8962\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6571 - val_loss: 2.8265\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6506 - val_loss: 2.8962\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6559 - val_loss: 2.8261\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6493 - val_loss: 2.8958\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6544 - val_loss: 2.8254\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6478 - val_loss: 2.8952\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6526 - val_loss: 2.8247\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6461 - val_loss: 2.8944\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6507 - val_loss: 2.8239\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6443 - val_loss: 2.8935\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6488 - val_loss: 2.8231\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6425 - val_loss: 2.8926\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6468 - val_loss: 2.8222\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6406 - val_loss: 2.8916\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6448 - val_loss: 2.8214\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6387 - val_loss: 2.8907\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6428 - val_loss: 2.8205\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6369 - val_loss: 2.8897\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6408 - val_loss: 2.8197\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6351 - val_loss: 2.8888\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6389 - val_loss: 2.8190\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6335 - val_loss: 2.8880\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6371 - val_loss: 2.8183\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6320 - val_loss: 2.8873\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6354 - val_loss: 2.8178\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6307 - val_loss: 2.8867\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6338 - val_loss: 2.8173\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6294 - val_loss: 2.8862\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6324 - val_loss: 2.8169\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6283 - val_loss: 2.8858\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6310 - val_loss: 2.8165\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6270 - val_loss: 2.8852\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6295 - val_loss: 2.8159\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6256 - val_loss: 2.8848\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6282 - val_loss: 2.8156\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6246 - val_loss: 2.8858\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6281 - val_loss: 2.8166\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6253 - val_loss: 2.8899\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6310 - val_loss: 2.8198\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6292 - val_loss: 2.8997\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6394 - val_loss: 2.8260\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6370 - val_loss: 2.9112\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6495 - val_loss: 2.8309\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6428 - val_loss: 2.9142\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6512 - val_loss: 2.8296\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6401 - val_loss: 2.9079\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6440 - val_loss: 2.8244\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6322 - val_loss: 2.8965\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6321 - val_loss: 2.8180\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6223 - val_loss: 2.8798\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6153 - val_loss: 2.8047\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6029 - val_loss: 2.8558\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5930 - val_loss: 2.7908\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5805 - val_loss: 2.8336\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5738 - val_loss: 2.7805\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5636 - val_loss: 2.8179\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5597 - val_loss: 2.7745\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5552 - val_loss: 2.8147\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5559 - val_loss: 2.7752\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5568 - val_loss: 2.8236\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5624 - val_loss: 2.7824\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5677 - val_loss: 2.8425\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5779 - val_loss: 2.7943\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5835 - val_loss: 2.8593\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5920 - val_loss: 2.8029\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5955 - val_loss: 2.8709\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6014 - val_loss: 2.8098\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6043 - val_loss: 2.8828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6117 - val_loss: 2.8183\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6148 - val_loss: 2.8975\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6244 - val_loss: 2.8263\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6246 - val_loss: 2.9121\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6367 - val_loss: 2.8342\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6347 - val_loss: 2.9217\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6442 - val_loss: 2.8376\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6385 - val_loss: 2.9216\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6425 - val_loss: 2.8335\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6323 - val_loss: 2.9137\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6337 - val_loss: 2.8275\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6235 - val_loss: 2.9030\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6229 - val_loss: 2.8207\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6136 - val_loss: 2.8932\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6131 - val_loss: 2.8132\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6027 - val_loss: 2.8823\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6022 - val_loss: 2.8090\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5967 - val_loss: 2.8713\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5916 - val_loss: 2.8017\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5856 - val_loss: 2.8573\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5793 - val_loss: 2.7900\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5652 - val_loss: 2.8318\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5570 - val_loss: 2.7765\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5447 - val_loss: 2.8120\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5381 - val_loss: 2.7691\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5340 - val_loss: 2.8111\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5349 - val_loss: 2.7719\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5375 - val_loss: 2.8227\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5439 - val_loss: 2.7798\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5485 - val_loss: 2.8370\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5562 - val_loss: 2.7873\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5585 - val_loss: 2.8468\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5641 - val_loss: 2.7926\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5652 - val_loss: 2.8535\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5688 - val_loss: 2.7963\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5695 - val_loss: 2.8585\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5723 - val_loss: 2.7993\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5729 - val_loss: 2.8648\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5778 - val_loss: 2.8045\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5791 - val_loss: 2.8713\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5833 - val_loss: 2.8087\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5842 - val_loss: 2.8762\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5867 - val_loss: 2.8119\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5878 - val_loss: 2.8817\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5908 - val_loss: 2.8157\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5925 - val_loss: 2.8901\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5983 - val_loss: 2.8227\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6016 - val_loss: 2.9006\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6079 - val_loss: 2.8301\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6111 - val_loss: 2.9086\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6152 - val_loss: 2.8339\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6153 - val_loss: 2.9090\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6146 - val_loss: 2.8293\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6075 - val_loss: 2.9015\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6056 - val_loss: 2.8233\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5986 - val_loss: 2.8972\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6004 - val_loss: 2.8258\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6008 - val_loss: 2.9056\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6086 - val_loss: 2.8326\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6093 - val_loss: 2.9069\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6103 - val_loss: 2.8272\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6019 - val_loss: 2.8945\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5982 - val_loss: 2.8185\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5905 - val_loss: 2.8849\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5885 - val_loss: 2.8085\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5778 - val_loss: 2.8612\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5654 - val_loss: 2.7882\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5520 - val_loss: 2.8411\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5475 - val_loss: 2.7800\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5380 - val_loss: 2.8236\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5342 - val_loss: 2.7727\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5247 - val_loss: 2.8094\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5198 - val_loss: 2.7655\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5143 - val_loss: 2.8083\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5154 - val_loss: 2.7663\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5153 - val_loss: 2.8178\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5217 - val_loss: 2.7741\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5256 - val_loss: 2.8322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5342 - val_loss: 2.7842\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5379 - val_loss: 2.8426\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5436 - val_loss: 2.7906\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5454 - val_loss: 2.8481\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5477 - val_loss: 2.7931\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5477 - val_loss: 2.8523\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5502 - val_loss: 2.7960\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5506 - val_loss: 2.8566\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5532 - val_loss: 2.7987\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5533 - val_loss: 2.8599\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5554 - val_loss: 2.8006\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5549 - val_loss: 2.8622\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5566 - val_loss: 2.8017\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5557 - val_loss: 2.8638\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5573 - val_loss: 2.8026\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5562 - val_loss: 2.8651\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5578 - val_loss: 2.8052\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5593 - val_loss: 2.8701\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5621 - val_loss: 2.8077\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5624 - val_loss: 2.8738\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5649 - val_loss: 2.8108\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5656 - val_loss: 2.8780\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5679 - val_loss: 2.8127\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5677 - val_loss: 2.8818\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5708 - val_loss: 2.8139\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5687 - val_loss: 2.8832\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5719 - val_loss: 2.8148\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5694 - val_loss: 2.8859\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5746 - val_loss: 2.8179\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5730 - val_loss: 2.8886\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5772 - val_loss: 2.8193\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5743 - val_loss: 2.8874\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5759 - val_loss: 2.8177\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5717 - val_loss: 2.8831\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5718 - val_loss: 2.8137\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5663 - val_loss: 2.8767\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5653 - val_loss: 2.8086\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5598 - val_loss: 2.8711\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5594 - val_loss: 2.8050\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5553 - val_loss: 2.8635\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5519 - val_loss: 2.8001\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5472 - val_loss: 2.8524\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5414 - val_loss: 2.7922\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5339 - val_loss: 2.8394\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5286 - val_loss: 2.7834\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5190 - val_loss: 2.8288\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5162 - val_loss: 2.7741\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5043 - val_loss: 2.8185\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5043 - val_loss: 2.7693\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4977 - val_loss: 2.8185\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5027 - val_loss: 2.7719\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5009 - val_loss: 2.8266\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5097 - val_loss: 2.7803\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5119 - val_loss: 2.8379\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5207 - val_loss: 2.7927\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5286 - val_loss: 2.8551\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5384 - val_loss: 2.8076\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5487 - val_loss: 2.8749\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5598 - val_loss: 2.8196\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5649 - val_loss: 2.8797\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5638 - val_loss: 2.8163\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5593 - val_loss: 2.8735\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5548 - val_loss: 2.8111\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5511 - val_loss: 2.8741\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5529 - val_loss: 2.8115\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5514 - val_loss: 2.8804\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5584 - val_loss: 2.8162\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5571 - val_loss: 2.8850\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5631 - val_loss: 2.8210\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5625 - val_loss: 2.8870\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5653 - val_loss: 2.8221\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5632 - val_loss: 2.8860\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5648 - val_loss: 2.8204\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5610 - val_loss: 2.8820\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5611 - val_loss: 2.8159\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5557 - val_loss: 2.8747\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5541 - val_loss: 2.8084\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5462 - val_loss: 2.8650\n",
      "Epoch 548/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5441 - val_loss: 2.8019\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5381 - val_loss: 2.8586\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5380 - val_loss: 2.8029\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5389 - val_loss: 2.8563\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5374 - val_loss: 2.8028\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5374 - val_loss: 2.8524\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5348 - val_loss: 2.8020\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5336 - val_loss: 2.8504\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5325 - val_loss: 2.8018\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5307 - val_loss: 2.8468\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5273 - val_loss: 2.7978\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5230 - val_loss: 2.8383\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5168 - val_loss: 2.7882\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5076 - val_loss: 2.8280\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5040 - val_loss: 2.7789\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4947 - val_loss: 2.8213\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.4951 - val_loss: 2.7744\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4888 - val_loss: 2.8223\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.4941 - val_loss: 2.7772\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4924 - val_loss: 2.8318\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5026 - val_loss: 2.7867\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5056 - val_loss: 2.8477\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5188 - val_loss: 2.8019\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5276 - val_loss: 2.8694\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5413 - val_loss: 2.8152\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5462 - val_loss: 2.8805\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5519 - val_loss: 2.8171\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5479 - val_loss: 2.8765\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5470 - val_loss: 2.8104\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5372 - val_loss: 2.8683\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5377 - val_loss: 2.8076\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5325 - val_loss: 2.8681\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5372 - val_loss: 2.8092\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5343 - val_loss: 2.8697\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5388 - val_loss: 2.8101\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5353 - val_loss: 2.8693\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5382 - val_loss: 2.8085\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5326 - val_loss: 2.8656\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5343 - val_loss: 2.8048\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5274 - val_loss: 2.8608\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5292 - val_loss: 2.8017\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5230 - val_loss: 2.8549\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5231 - val_loss: 2.7999\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5202 - val_loss: 2.8532\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5223 - val_loss: 2.8020\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5223 - val_loss: 2.8555\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5261 - val_loss: 2.8052\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5250 - val_loss: 2.8536\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5244 - val_loss: 2.8033\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5198 - val_loss: 2.8487\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5186 - val_loss: 2.7990\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5109 - val_loss: 2.8401\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5092 - val_loss: 2.7957\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 155us/step - loss: 58.2569 - val_loss: 52.7666\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 55.4911 - val_loss: 48.7597\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 51.4783 - val_loss: 43.8098\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 46.5177 - val_loss: 37.7336\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 40.3991 - val_loss: 30.6462\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 33.1546 - val_loss: 23.8138\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 25.7503 - val_loss: 19.5506\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.5143 - val_loss: 17.5301\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.7630 - val_loss: 16.5290\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.1871 - val_loss: 16.0410\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 15.3287 - val_loss: 15.6470\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.7747 - val_loss: 15.0729\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.1902 - val_loss: 14.2958\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.4836 - val_loss: 13.5389\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.8067 - val_loss: 12.6878\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.0394 - val_loss: 11.4351\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 10.8986 - val_loss: 10.3041\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 9.8183 - val_loss: 9.6045\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.1317 - val_loss: 9.2842\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7921 - val_loss: 9.1108\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.6154 - val_loss: 8.9673\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4809 - val_loss: 8.8336\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3596 - val_loss: 8.6944\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2310 - val_loss: 8.5387\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.0896 - val_loss: 8.3769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9487 - val_loss: 8.2256\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8165 - val_loss: 8.0813\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6884 - val_loss: 7.9385\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.5588 - val_loss: 7.7944\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.4256 - val_loss: 7.6496\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.2898 - val_loss: 7.5056\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.1534 - val_loss: 7.3638\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.0177 - val_loss: 7.2246\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.8836 - val_loss: 7.0871\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.7519 - val_loss: 6.9526\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.6225 - val_loss: 6.8217\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.4952 - val_loss: 6.6931\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.3697 - val_loss: 6.5665\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.2457 - val_loss: 6.4416\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.1227 - val_loss: 6.3173\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.0004 - val_loss: 6.1931\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.8783 - val_loss: 6.0690\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.7559 - val_loss: 5.9440\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.6329 - val_loss: 5.8170\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.5087 - val_loss: 5.6874\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.3832 - val_loss: 5.5548\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.2562 - val_loss: 5.4193\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 5.1282 - val_loss: 5.2820\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.0002 - val_loss: 5.1446\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.8737 - val_loss: 5.0102\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.7511 - val_loss: 4.8832\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.6344 - val_loss: 4.7640\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.5249 - val_loss: 4.6534\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.4229 - val_loss: 4.5499\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.3284 - val_loss: 4.4553\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2411 - val_loss: 4.3626\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.1603 - val_loss: 4.2815\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0851 - val_loss: 4.1978\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0153 - val_loss: 4.1274\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9509 - val_loss: 4.0586\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8914 - val_loss: 3.9978\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8359 - val_loss: 3.9383\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7845 - val_loss: 3.8912\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7363 - val_loss: 3.8357\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6912 - val_loss: 3.7972\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6495 - val_loss: 3.7480\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6124 - val_loss: 3.7258\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5807 - val_loss: 3.6781\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5595 - val_loss: 3.6929\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5521 - val_loss: 3.6443\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5639 - val_loss: 3.6988\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.5650 - val_loss: 3.6129\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.5546 - val_loss: 3.6573\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.5281 - val_loss: 3.5498\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4956 - val_loss: 3.5853\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4619 - val_loss: 3.4840\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4250 - val_loss: 3.5104\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3943 - val_loss: 3.4267\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3645 - val_loss: 3.4517\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3439 - val_loss: 3.3808\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3193 - val_loss: 3.4082\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.3063 - val_loss: 3.3425\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2857 - val_loss: 3.3748\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.2779 - val_loss: 3.3089\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2594 - val_loss: 3.3463\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2542 - val_loss: 3.2781\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2363 - val_loss: 3.3201\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2324 - val_loss: 3.2467\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2121 - val_loss: 3.2921\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2061 - val_loss: 3.2163\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1858 - val_loss: 3.2612\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1789 - val_loss: 3.1878\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1622 - val_loss: 3.2372\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1567 - val_loss: 3.1634\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1438 - val_loss: 3.2222\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1412 - val_loss: 3.1433\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1291 - val_loss: 3.2122\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1314 - val_loss: 3.1310\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1183 - val_loss: 3.1982\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1162 - val_loss: 3.1130\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1005 - val_loss: 3.1784\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0944 - val_loss: 3.0929\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0784 - val_loss: 3.1570\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0713 - val_loss: 3.0770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0609 - val_loss: 3.1455\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0571 - val_loss: 3.0672\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0502 - val_loss: 3.1344\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0443 - val_loss: 3.0546\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0356 - val_loss: 3.1236\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0303 - val_loss: 3.0448\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0216 - val_loss: 3.1128\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0183 - val_loss: 3.0369\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0114 - val_loss: 3.1077\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0111 - val_loss: 3.0306\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0031 - val_loss: 3.1019\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0040 - val_loss: 3.0237\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9940 - val_loss: 3.0947\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9960 - val_loss: 3.0164\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9841 - val_loss: 3.0863\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9871 - val_loss: 3.0077\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9725 - val_loss: 3.0756\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9763 - val_loss: 2.9989\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9623 - val_loss: 3.0687\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9680 - val_loss: 2.9924\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9546 - val_loss: 3.0633\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9614 - val_loss: 2.9874\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9482 - val_loss: 3.0588\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9556 - val_loss: 2.9822\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9420 - val_loss: 3.0539\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.9492 - val_loss: 2.9750\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9352 - val_loss: 3.0478\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9416 - val_loss: 2.9688\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9302 - val_loss: 3.0432\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9365 - val_loss: 2.9643\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9249 - val_loss: 3.0381\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9307 - val_loss: 2.9594\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9191 - val_loss: 3.0332\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9248 - val_loss: 2.9543\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9131 - val_loss: 3.0280\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9186 - val_loss: 2.9494\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9072 - val_loss: 3.0230\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9126 - val_loss: 2.9447\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9017 - val_loss: 3.0183\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9068 - val_loss: 2.9400\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8963 - val_loss: 3.0137\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9010 - val_loss: 2.9347\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8905 - val_loss: 3.0076\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8934 - val_loss: 2.9270\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8815 - val_loss: 2.9997\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8830 - val_loss: 2.9185\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8727 - val_loss: 2.9938\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8754 - val_loss: 2.9144\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8688 - val_loss: 2.9919\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8735 - val_loss: 2.9133\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8694 - val_loss: 2.9935\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8749 - val_loss: 2.9126\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8712 - val_loss: 2.9938\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8753 - val_loss: 2.9090\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8685 - val_loss: 2.9887\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8698 - val_loss: 2.9028\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8621 - val_loss: 2.9827\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8629 - val_loss: 2.8969\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8567 - val_loss: 2.9780\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8572 - val_loss: 2.8906\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8504 - val_loss: 2.9710\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8478 - val_loss: 2.8804\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8403 - val_loss: 2.9601\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8345 - val_loss: 2.8705\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8273 - val_loss: 2.9511\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8241 - val_loss: 2.8641\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8182 - val_loss: 2.9446\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8175 - val_loss: 2.8617\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8143 - val_loss: 2.9434\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8159 - val_loss: 2.8612\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8153 - val_loss: 2.9459\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8182 - val_loss: 2.8625\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8196 - val_loss: 2.9497\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8228 - val_loss: 2.8644\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8248 - val_loss: 2.9520\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8267 - val_loss: 2.8650\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8252 - val_loss: 2.9498\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8253 - val_loss: 2.8625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8216 - val_loss: 2.9458\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8207 - val_loss: 2.8572\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8158 - val_loss: 2.9405\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8142 - val_loss: 2.8513\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8085 - val_loss: 2.9338\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8063 - val_loss: 2.8459\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8014 - val_loss: 2.9248\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7970 - val_loss: 2.8412\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7933 - val_loss: 2.9205\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7916 - val_loss: 2.8383\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7908 - val_loss: 2.9209\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7909 - val_loss: 2.8369\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7912 - val_loss: 2.9218\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7914 - val_loss: 2.8358\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7908 - val_loss: 2.9201\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7898 - val_loss: 2.8339\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7881 - val_loss: 2.9171\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7872 - val_loss: 2.8320\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7856 - val_loss: 2.9186\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7882 - val_loss: 2.8310\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7862 - val_loss: 2.9179\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7873 - val_loss: 2.8282\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7832 - val_loss: 2.9121\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7816 - val_loss: 2.8241\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7773 - val_loss: 2.9071\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7760 - val_loss: 2.8201\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7727 - val_loss: 2.9031\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7710 - val_loss: 2.8159\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7679 - val_loss: 2.9000\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7656 - val_loss: 2.8090\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7618 - val_loss: 2.8958\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7583 - val_loss: 2.8018\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7530 - val_loss: 2.8873\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7483 - val_loss: 2.7950\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7420 - val_loss: 2.8785\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7395 - val_loss: 2.7908\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7351 - val_loss: 2.8730\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7342 - val_loss: 2.7881\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7312 - val_loss: 2.8709\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7311 - val_loss: 2.7860\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7296 - val_loss: 2.8696\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7291 - val_loss: 2.7832\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7274 - val_loss: 2.8656\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7247 - val_loss: 2.7786\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7215 - val_loss: 2.8575\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7173 - val_loss: 2.7747\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7150 - val_loss: 2.8520\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7118 - val_loss: 2.7723\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7118 - val_loss: 2.8506\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7100 - val_loss: 2.7711\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7112 - val_loss: 2.8510\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7096 - val_loss: 2.7702\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7111 - val_loss: 2.8513\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7093 - val_loss: 2.7691\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7107 - val_loss: 2.8511\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7086 - val_loss: 2.7679\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7098 - val_loss: 2.8506\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7077 - val_loss: 2.7666\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7090 - val_loss: 2.8505\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7070 - val_loss: 2.7655\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7085 - val_loss: 2.8520\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7074 - val_loss: 2.7649\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7097 - val_loss: 2.8556\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7098 - val_loss: 2.7653\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7120 - val_loss: 2.8572\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7123 - val_loss: 2.7665\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7128 - val_loss: 2.8577\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7148 - val_loss: 2.7701\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7157 - val_loss: 2.8626\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7207 - val_loss: 2.7743\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7203 - val_loss: 2.8662\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7246 - val_loss: 2.7745\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7204 - val_loss: 2.8633\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7216 - val_loss: 2.7701\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7152 - val_loss: 2.8561\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7142 - val_loss: 2.7639\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7081 - val_loss: 2.8466\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7050 - val_loss: 2.7576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7003 - val_loss: 2.8361\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6959 - val_loss: 2.7523\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6903 - val_loss: 2.8252\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6859 - val_loss: 2.7477\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6835 - val_loss: 2.8227\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6823 - val_loss: 2.7460\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6826 - val_loss: 2.8234\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6810 - val_loss: 2.7435\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6810 - val_loss: 2.8212\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6774 - val_loss: 2.7391\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6759 - val_loss: 2.8148\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6710 - val_loss: 2.7342\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6686 - val_loss: 2.8068\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6643 - val_loss: 2.7308\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6631 - val_loss: 2.8029\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6605 - val_loss: 2.7299\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6609 - val_loss: 2.8036\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6596 - val_loss: 2.7298\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6603 - val_loss: 2.8051\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6593 - val_loss: 2.7295\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6594 - val_loss: 2.8056\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6585 - val_loss: 2.7289\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6582 - val_loss: 2.8055\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6573 - val_loss: 2.7281\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6567 - val_loss: 2.8051\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6560 - val_loss: 2.7273\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6552 - val_loss: 2.8047\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6547 - val_loss: 2.7264\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6536 - val_loss: 2.8042\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6534 - val_loss: 2.7256\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6521 - val_loss: 2.8038\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6521 - val_loss: 2.7247\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6507 - val_loss: 2.8034\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6509 - val_loss: 2.7239\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6492 - val_loss: 2.8031\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6497 - val_loss: 2.7231\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6478 - val_loss: 2.8028\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6486 - val_loss: 2.7224\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6465 - val_loss: 2.8026\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6474 - val_loss: 2.7216\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6452 - val_loss: 2.8024\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6464 - val_loss: 2.7209\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6439 - val_loss: 2.8023\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6454 - val_loss: 2.7203\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6428 - val_loss: 2.8023\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6445 - val_loss: 2.7197\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6419 - val_loss: 2.8028\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6439 - val_loss: 2.7193\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6416 - val_loss: 2.8044\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6442 - val_loss: 2.7191\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6426 - val_loss: 2.8083\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6460 - val_loss: 2.7196\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6447 - val_loss: 2.8148\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6494 - val_loss: 2.7213\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6493 - val_loss: 2.8228\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6553 - val_loss: 2.7251\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6548 - val_loss: 2.8279\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6606 - val_loss: 2.7321\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6604 - val_loss: 2.8287\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6664 - val_loss: 2.7419\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6681 - val_loss: 2.8330\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6757 - val_loss: 2.7507\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6753 - val_loss: 2.8327\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6791 - val_loss: 2.7505\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6734 - val_loss: 2.8272\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6731 - val_loss: 2.7444\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6655 - val_loss: 2.8191\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6626 - val_loss: 2.7354\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6548 - val_loss: 2.8120\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6526 - val_loss: 2.7291\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6472 - val_loss: 2.8083\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6467 - val_loss: 2.7251\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6429 - val_loss: 2.8061\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6429 - val_loss: 2.7222\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6393 - val_loss: 2.8036\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6394 - val_loss: 2.7182\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6370 - val_loss: 2.8026\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6376 - val_loss: 2.7154\n",
      "Epoch 339/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6330 - val_loss: 2.7977\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6320 - val_loss: 2.7117\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6267 - val_loss: 2.7935\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6269 - val_loss: 2.7083\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6219 - val_loss: 2.7906\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6226 - val_loss: 2.7038\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6171 - val_loss: 2.7852\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6171 - val_loss: 2.6993\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6121 - val_loss: 2.7805\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6126 - val_loss: 2.6968\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6083 - val_loss: 2.7775\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6094 - val_loss: 2.6953\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6052 - val_loss: 2.7748\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6065 - val_loss: 2.6932\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6022 - val_loss: 2.7724\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6043 - val_loss: 2.6922\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6003 - val_loss: 2.7722\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6030 - val_loss: 2.6916\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6000 - val_loss: 2.7731\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6029 - val_loss: 2.6915\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6002 - val_loss: 2.7745\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6030 - val_loss: 2.6917\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6008 - val_loss: 2.7763\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6034 - val_loss: 2.6921\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6011 - val_loss: 2.7778\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6039 - val_loss: 2.6930\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6024 - val_loss: 2.7809\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6057 - val_loss: 2.6937\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6045 - val_loss: 2.7833\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6075 - val_loss: 2.6948\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6065 - val_loss: 2.7865\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6099 - val_loss: 2.6968\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6111 - val_loss: 2.7975\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6184 - val_loss: 2.7020\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6197 - val_loss: 2.8009\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6229 - val_loss: 2.7054\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6208 - val_loss: 2.7985\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6235 - val_loss: 2.7113\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6227 - val_loss: 2.7998\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6267 - val_loss: 2.7177\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6269 - val_loss: 2.8076\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6360 - val_loss: 2.7264\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6375 - val_loss: 2.8198\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6517 - val_loss: 2.7370\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6489 - val_loss: 2.8201\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6567 - val_loss: 2.7398\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6478 - val_loss: 2.8121\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6516 - val_loss: 2.7339\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6393 - val_loss: 2.8038\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6404 - val_loss: 2.7227\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6270 - val_loss: 2.7888\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6191 - val_loss: 2.7045\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6061 - val_loss: 2.7755\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6001 - val_loss: 2.6885\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5921 - val_loss: 2.7701\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5915 - val_loss: 2.6834\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5852 - val_loss: 2.7654\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5841 - val_loss: 2.6778\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5794 - val_loss: 2.7606\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5797 - val_loss: 2.6752\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5741 - val_loss: 2.7567\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5754 - val_loss: 2.6742\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5713 - val_loss: 2.7569\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5747 - val_loss: 2.6747\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5721 - val_loss: 2.7599\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5760 - val_loss: 2.6751\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5758 - val_loss: 2.7647\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5794 - val_loss: 2.6765\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5785 - val_loss: 2.7666\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5803 - val_loss: 2.6772\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5783 - val_loss: 2.7668\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5801 - val_loss: 2.6782\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5780 - val_loss: 2.7683\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5810 - val_loss: 2.6797\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5793 - val_loss: 2.7703\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5828 - val_loss: 2.6819\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5813 - val_loss: 2.7723\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5850 - val_loss: 2.6848\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5840 - val_loss: 2.7750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5881 - val_loss: 2.6889\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5879 - val_loss: 2.7791\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5922 - val_loss: 2.6922\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5912 - val_loss: 2.7808\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5944 - val_loss: 2.6935\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5919 - val_loss: 2.7807\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5944 - val_loss: 2.6939\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5914 - val_loss: 2.7804\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5937 - val_loss: 2.6935\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5904 - val_loss: 2.7796\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5922 - val_loss: 2.6922\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5884 - val_loss: 2.7777\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5895 - val_loss: 2.6898\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5850 - val_loss: 2.7748\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5855 - val_loss: 2.6870\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5811 - val_loss: 2.7723\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5817 - val_loss: 2.6847\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5785 - val_loss: 2.7718\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5797 - val_loss: 2.6826\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5780 - val_loss: 2.7732\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5797 - val_loss: 2.6818\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5778 - val_loss: 2.7733\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5787 - val_loss: 2.6808\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5760 - val_loss: 2.7719\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5767 - val_loss: 2.6801\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5726 - val_loss: 2.7685\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5731 - val_loss: 2.6796\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5693 - val_loss: 2.7674\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5713 - val_loss: 2.6795\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5693 - val_loss: 2.7697\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5726 - val_loss: 2.6804\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5736 - val_loss: 2.7773\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5784 - val_loss: 2.6829\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5778 - val_loss: 2.7796\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5800 - val_loss: 2.6833\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5775 - val_loss: 2.7784\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5790 - val_loss: 2.6837\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5760 - val_loss: 2.7770\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5776 - val_loss: 2.6838\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5744 - val_loss: 2.7760\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5766 - val_loss: 2.6855\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5750 - val_loss: 2.7771\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5781 - val_loss: 2.6883\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5778 - val_loss: 2.7812\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5825 - val_loss: 2.6926\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5824 - val_loss: 2.7877\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5896 - val_loss: 2.6965\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5873 - val_loss: 2.7918\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5941 - val_loss: 2.6981\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5887 - val_loss: 2.7914\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5938 - val_loss: 2.6966\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5862 - val_loss: 2.7881\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5895 - val_loss: 2.6928\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5814 - val_loss: 2.7827\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5825 - val_loss: 2.6870\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5748 - val_loss: 2.7741\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5715 - val_loss: 2.6768\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5635 - val_loss: 2.7653\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5596 - val_loss: 2.6694\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5559 - val_loss: 2.7635\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5552 - val_loss: 2.6658\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5530 - val_loss: 2.7625\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5517 - val_loss: 2.6629\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5490 - val_loss: 2.7604\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5482 - val_loss: 2.6610\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5457 - val_loss: 2.7598\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5463 - val_loss: 2.6604\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5445 - val_loss: 2.7608\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5463 - val_loss: 2.6607\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5448 - val_loss: 2.7628\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5471 - val_loss: 2.6615\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5458 - val_loss: 2.7702\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5516 - val_loss: 2.6645\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5521 - val_loss: 2.7812\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5601 - val_loss: 2.6688\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5596 - val_loss: 2.7813\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5617 - val_loss: 2.6700\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5575 - val_loss: 2.7740\n",
      "Epoch 496/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5575 - val_loss: 2.6714\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5549 - val_loss: 2.7721\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5575 - val_loss: 2.6754\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5572 - val_loss: 2.7757\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5615 - val_loss: 2.6793\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5602 - val_loss: 2.7777\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5646 - val_loss: 2.6844\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5643 - val_loss: 2.7847\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5735 - val_loss: 2.6900\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5710 - val_loss: 2.7901\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5792 - val_loss: 2.6917\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5723 - val_loss: 2.7885\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5774 - val_loss: 2.6890\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5682 - val_loss: 2.7839\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5712 - val_loss: 2.6842\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5626 - val_loss: 2.7791\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5641 - val_loss: 2.6793\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5578 - val_loss: 2.7734\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5552 - val_loss: 2.6710\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5490 - val_loss: 2.7697\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5481 - val_loss: 2.6660\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5456 - val_loss: 2.7733\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5474 - val_loss: 2.6643\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5458 - val_loss: 2.7752\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5466 - val_loss: 2.6622\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5435 - val_loss: 2.7740\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5439 - val_loss: 2.6601\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5405 - val_loss: 2.7741\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5416 - val_loss: 2.6584\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5397 - val_loss: 2.7719\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5380 - val_loss: 2.6555\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5355 - val_loss: 2.7672\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5325 - val_loss: 2.6529\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5286 - val_loss: 2.7604\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5254 - val_loss: 2.6505\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5281 - val_loss: 2.7692\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5334 - val_loss: 2.6552\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5418 - val_loss: 2.7777\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5433 - val_loss: 2.6625\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5531 - val_loss: 2.7796\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5496 - val_loss: 2.6693\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5599 - val_loss: 2.7811\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5561 - val_loss: 2.6743\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5620 - val_loss: 2.7777\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5534 - val_loss: 2.6722\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5567 - val_loss: 2.7749\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5484 - val_loss: 2.6685\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5529 - val_loss: 2.7772\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5455 - val_loss: 2.6649\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5508 - val_loss: 2.7814\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5433 - val_loss: 2.6610\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5449 - val_loss: 2.7769\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5351 - val_loss: 2.6561\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5374 - val_loss: 2.7793\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5340 - val_loss: 2.6567\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5401 - val_loss: 2.7827\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5349 - val_loss: 2.6541\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5357 - val_loss: 2.7776\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5295 - val_loss: 2.6512\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5297 - val_loss: 2.7750\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5262 - val_loss: 2.6504\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5268 - val_loss: 2.7729\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5234 - val_loss: 2.6498\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5233 - val_loss: 2.7704\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5202 - val_loss: 2.6503\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5212 - val_loss: 2.7695\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5190 - val_loss: 2.6523\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5218 - val_loss: 2.7725\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5219 - val_loss: 2.6559\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5273 - val_loss: 2.7766\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5273 - val_loss: 2.6601\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5290 - val_loss: 2.7782\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5308 - val_loss: 2.6656\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5325 - val_loss: 2.7823\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5374 - val_loss: 2.6734\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5395 - val_loss: 2.7857\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5441 - val_loss: 2.6817\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5453 - val_loss: 2.7883\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5515 - val_loss: 2.6861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5480 - val_loss: 2.7888\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5531 - val_loss: 2.6858\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5467 - val_loss: 2.7864\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5489 - val_loss: 2.6813\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5423 - val_loss: 2.7837\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5432 - val_loss: 2.6769\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5380 - val_loss: 2.7816\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5386 - val_loss: 2.6733\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5345 - val_loss: 2.7806\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5353 - val_loss: 2.6697\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5313 - val_loss: 2.7831\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5330 - val_loss: 2.6670\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5308 - val_loss: 2.7790\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5267 - val_loss: 2.6617\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5241 - val_loss: 2.7737\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5201 - val_loss: 2.6577\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5186 - val_loss: 2.7706\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5152 - val_loss: 2.6554\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5160 - val_loss: 2.7689\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5115 - val_loss: 2.6535\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5138 - val_loss: 2.7675\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5087 - val_loss: 2.6525\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5120 - val_loss: 2.7669\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5067 - val_loss: 2.6519\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5109 - val_loss: 2.7668\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5054 - val_loss: 2.6514\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 172us/step - loss: 56.6117 - val_loss: 55.3750\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 53.7947 - val_loss: 50.7990\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.2235 - val_loss: 44.5799\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 43.0258 - val_loss: 36.6419\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 35.1687 - val_loss: 27.8029\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 26.5816 - val_loss: 21.1084\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.4211 - val_loss: 17.8373\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 17.7893 - val_loss: 16.0261\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.5443 - val_loss: 15.0363\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 15.8757 - val_loss: 14.3042\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.2246 - val_loss: 13.4993\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.3283 - val_loss: 12.5674\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.2153 - val_loss: 11.6785\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 12.1770 - val_loss: 10.6441\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 11.1100 - val_loss: 9.5514\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 10.0701 - val_loss: 8.7842\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.3068 - val_loss: 8.4056\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.9304 - val_loss: 8.2431\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7590 - val_loss: 8.1852\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6498 - val_loss: 8.1448\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5703 - val_loss: 8.0620\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4785 - val_loss: 7.9425\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3660 - val_loss: 7.8168\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2450 - val_loss: 7.6902\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1189 - val_loss: 7.5599\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9881 - val_loss: 7.4269\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 7.8527 - val_loss: 7.2896\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.7110 - val_loss: 7.1468\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.5643 - val_loss: 7.0015\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.4130 - val_loss: 6.8523\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.2564 - val_loss: 6.6947\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.0934 - val_loss: 6.5278\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.9230 - val_loss: 6.3521\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.7449 - val_loss: 6.1703\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.5592 - val_loss: 5.9833\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.3671 - val_loss: 5.7926\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 6.1704 - val_loss: 5.6032\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.9706 - val_loss: 5.4145\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.7708 - val_loss: 5.2308\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.5753 - val_loss: 5.0533\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.3880 - val_loss: 4.8811\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 5.2111 - val_loss: 4.7195\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.0460 - val_loss: 4.5648\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.8939 - val_loss: 4.4294\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.7552 - val_loss: 4.2968\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.6321 - val_loss: 4.2056\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.5255 - val_loss: 4.0911\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.4376 - val_loss: 4.0569\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3613 - val_loss: 3.9553\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.3058 - val_loss: 3.9836\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.2655 - val_loss: 3.8929\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2483 - val_loss: 4.0077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.2535 - val_loss: 3.9052\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.2748 - val_loss: 4.0317\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.2414 - val_loss: 3.8511\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.2132 - val_loss: 3.9440\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.1184 - val_loss: 3.7303\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0603 - val_loss: 3.8427\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.9889 - val_loss: 3.6381\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.9408 - val_loss: 3.7700\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8948 - val_loss: 3.5724\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8590 - val_loss: 3.7291\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8290 - val_loss: 3.5285\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8008 - val_loss: 3.7092\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.7813 - val_loss: 3.4952\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7597 - val_loss: 3.6968\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.7394 - val_loss: 3.4622\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7158 - val_loss: 3.6859\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6981 - val_loss: 3.4309\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.6691 - val_loss: 3.6578\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6456 - val_loss: 3.3969\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.6167 - val_loss: 3.6319\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5963 - val_loss: 3.3703\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.5780 - val_loss: 3.6261\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5686 - val_loss: 3.3571\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5544 - val_loss: 3.6083\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5374 - val_loss: 3.3285\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5095 - val_loss: 3.5609\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4821 - val_loss: 3.2910\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4518 - val_loss: 3.5193\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4339 - val_loss: 3.2699\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4153 - val_loss: 3.4899\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3987 - val_loss: 3.2517\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3827 - val_loss: 3.4675\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3673 - val_loss: 3.2331\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3475 - val_loss: 3.4291\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3269 - val_loss: 3.2060\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3019 - val_loss: 3.3893\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2868 - val_loss: 3.1894\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2708 - val_loss: 3.3745\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2664 - val_loss: 3.1845\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2584 - val_loss: 3.3661\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2529 - val_loss: 3.1777\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2422 - val_loss: 3.3446\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2307 - val_loss: 3.1648\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2182 - val_loss: 3.3202\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2067 - val_loss: 3.1513\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1899 - val_loss: 3.2947\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1813 - val_loss: 3.1411\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1688 - val_loss: 3.2816\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1654 - val_loss: 3.1349\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1555 - val_loss: 3.2624\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1489 - val_loss: 3.1259\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1369 - val_loss: 3.2440\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1305 - val_loss: 3.1131\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1122 - val_loss: 3.2214\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1062 - val_loss: 3.0999\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0884 - val_loss: 3.1963\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0804 - val_loss: 3.0862\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0659 - val_loss: 3.1748\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0584 - val_loss: 3.0750\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0476 - val_loss: 3.1605\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0428 - val_loss: 3.0676\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0365 - val_loss: 3.1577\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0362 - val_loss: 3.0647\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0331 - val_loss: 3.1626\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0366 - val_loss: 3.0656\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0362 - val_loss: 3.1712\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0414 - val_loss: 3.0693\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0441 - val_loss: 3.1829\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0497 - val_loss: 3.0727\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0509 - val_loss: 3.1899\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0546 - val_loss: 3.0723\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0512 - val_loss: 3.1871\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0496 - val_loss: 3.0662\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.0422 - val_loss: 3.1756\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0362 - val_loss: 3.0569\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0283 - val_loss: 3.1630\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0227 - val_loss: 3.0503\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0179 - val_loss: 3.1545\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0127 - val_loss: 3.0439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0093 - val_loss: 3.1461\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0024 - val_loss: 3.0364\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9988 - val_loss: 3.1359\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9907 - val_loss: 3.0279\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9859 - val_loss: 3.1253\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9787 - val_loss: 3.0201\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9742 - val_loss: 3.1183\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9695 - val_loss: 3.0145\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.9668 - val_loss: 3.1157\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.9645 - val_loss: 3.0096\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9624 - val_loss: 3.1146\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9615 - val_loss: 3.0066\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9600 - val_loss: 3.1143\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9594 - val_loss: 3.0050\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9596 - val_loss: 3.1169\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9597 - val_loss: 3.0047\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9610 - val_loss: 3.1210\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9619 - val_loss: 3.0043\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9618 - val_loss: 3.1235\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9623 - val_loss: 3.0022\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9596 - val_loss: 3.1227\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9596 - val_loss: 2.9990\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9549 - val_loss: 3.1188\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9540 - val_loss: 2.9944\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9482 - val_loss: 3.1102\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9446 - val_loss: 2.9872\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9375 - val_loss: 3.0921\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9268 - val_loss: 2.9750\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.9177 - val_loss: 3.0703\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9051 - val_loss: 2.9630\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8982 - val_loss: 3.0533\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8880 - val_loss: 2.9529\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8828 - val_loss: 3.0382\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8749 - val_loss: 2.9472\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8751 - val_loss: 3.0370\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8724 - val_loss: 2.9454\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8750 - val_loss: 3.0435\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8769 - val_loss: 2.9455\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8786 - val_loss: 3.0506\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8830 - val_loss: 2.9478\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8844 - val_loss: 3.0595\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8906 - val_loss: 2.9509\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8901 - val_loss: 3.0737\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9027 - val_loss: 2.9573\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9008 - val_loss: 3.0876\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9137 - val_loss: 2.9638\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9115 - val_loss: 3.1052\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9244 - val_loss: 2.9696\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9232 - val_loss: 3.1134\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9312 - val_loss: 2.9666\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9203 - val_loss: 3.0981\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9189 - val_loss: 2.9546\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8997 - val_loss: 3.0715\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8947 - val_loss: 2.9377\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8706 - val_loss: 3.0288\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8568 - val_loss: 2.9181\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8409 - val_loss: 2.9940\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8262 - val_loss: 2.9045\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8192 - val_loss: 2.9779\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8117 - val_loss: 2.9011\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8141 - val_loss: 2.9833\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8146 - val_loss: 2.9033\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8206 - val_loss: 3.0012\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8285 - val_loss: 2.9088\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8343 - val_loss: 3.0238\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8470 - val_loss: 2.9161\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8487 - val_loss: 3.0408\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8629 - val_loss: 2.9270\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8650 - val_loss: 3.0530\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8747 - val_loss: 2.9322\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8703 - val_loss: 3.0515\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8744 - val_loss: 2.9309\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8663 - val_loss: 3.0445\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8664 - val_loss: 2.9225\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8535 - val_loss: 3.0275\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8487 - val_loss: 2.9093\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8342 - val_loss: 3.0032\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8260 - val_loss: 2.8951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8134 - val_loss: 2.9834\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8079 - val_loss: 2.8870\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8020 - val_loss: 2.9761\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8003 - val_loss: 2.8838\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7986 - val_loss: 2.9766\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8000 - val_loss: 2.8854\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8013 - val_loss: 2.9834\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8056 - val_loss: 2.8903\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8090 - val_loss: 2.9956\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8166 - val_loss: 2.8967\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8186 - val_loss: 3.0100\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8303 - val_loss: 2.9053\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8304 - val_loss: 3.0203\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8407 - val_loss: 2.9069\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8313 - val_loss: 3.0117\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8329 - val_loss: 2.8994\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8189 - val_loss: 2.9952\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8154 - val_loss: 2.8907\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8055 - val_loss: 2.9809\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8003 - val_loss: 2.8824\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7939 - val_loss: 2.9720\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7914 - val_loss: 2.8788\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7890 - val_loss: 2.9708\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7899 - val_loss: 2.8786\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7888 - val_loss: 2.9735\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7921 - val_loss: 2.8796\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7902 - val_loss: 2.9756\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7938 - val_loss: 2.8797\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7899 - val_loss: 2.9752\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7931 - val_loss: 2.8791\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7881 - val_loss: 2.9727\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7900 - val_loss: 2.8768\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7840 - val_loss: 2.9679\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7846 - val_loss: 2.8733\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7786 - val_loss: 2.9635\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7796 - val_loss: 2.8706\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7745 - val_loss: 2.9614\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7768 - val_loss: 2.8694\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7723 - val_loss: 2.9607\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7755 - val_loss: 2.8686\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7706 - val_loss: 2.9597\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7739 - val_loss: 2.8677\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7684 - val_loss: 2.9581\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7715 - val_loss: 2.8663\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7656 - val_loss: 2.9561\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7686 - val_loss: 2.8648\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7628 - val_loss: 2.9544\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7660 - val_loss: 2.8635\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7601 - val_loss: 2.9528\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7635 - val_loss: 2.8623\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7576 - val_loss: 2.9514\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7612 - val_loss: 2.8612\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7552 - val_loss: 2.9501\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7589 - val_loss: 2.8601\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7527 - val_loss: 2.9487\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7564 - val_loss: 2.8589\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7502 - val_loss: 2.9472\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7539 - val_loss: 2.8581\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7486 - val_loss: 2.9469\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7526 - val_loss: 2.8580\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7481 - val_loss: 2.9475\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7523 - val_loss: 2.8577\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7467 - val_loss: 2.9464\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7503 - val_loss: 2.8565\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7432 - val_loss: 2.9435\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7461 - val_loss: 2.8540\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7376 - val_loss: 2.9393\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7402 - val_loss: 2.8513\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7320 - val_loss: 2.9357\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7353 - val_loss: 2.8487\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7272 - val_loss: 2.9328\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7312 - val_loss: 2.8477\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7242 - val_loss: 2.9321\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7292 - val_loss: 2.8474\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7226 - val_loss: 2.9325\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7283 - val_loss: 2.8492\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7235 - val_loss: 2.9361\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7305 - val_loss: 2.8510\n",
      "Epoch 288/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7253 - val_loss: 2.9400\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7331 - val_loss: 2.8529\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7273 - val_loss: 2.9414\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7334 - val_loss: 2.8523\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7255 - val_loss: 2.9369\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7281 - val_loss: 2.8480\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7185 - val_loss: 2.9276\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7178 - val_loss: 2.8417\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7084 - val_loss: 2.9212\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7102 - val_loss: 2.8402\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7050 - val_loss: 2.9229\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7101 - val_loss: 2.8417\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7062 - val_loss: 2.9267\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7122 - val_loss: 2.8438\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7082 - val_loss: 2.9301\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7140 - val_loss: 2.8454\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7098 - val_loss: 2.9336\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7161 - val_loss: 2.8482\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7132 - val_loss: 2.9392\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7202 - val_loss: 2.8501\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7157 - val_loss: 2.9402\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7197 - val_loss: 2.8478\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7122 - val_loss: 2.9350\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7133 - val_loss: 2.8423\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7043 - val_loss: 2.9290\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7060 - val_loss: 2.8369\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6953 - val_loss: 2.9220\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6978 - val_loss: 2.8349\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6909 - val_loss: 2.9217\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6962 - val_loss: 2.8369\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6924 - val_loss: 2.9257\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6988 - val_loss: 2.8407\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6972 - val_loss: 2.9327\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7043 - val_loss: 2.8432\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7001 - val_loss: 2.9351\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7051 - val_loss: 2.8420\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6975 - val_loss: 2.9309\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6997 - val_loss: 2.8382\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6915 - val_loss: 2.9254\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6933 - val_loss: 2.8351\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6860 - val_loss: 2.9215\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6885 - val_loss: 2.8337\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6827 - val_loss: 2.9202\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6862 - val_loss: 2.8338\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6817 - val_loss: 2.9216\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6862 - val_loss: 2.8348\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6823 - val_loss: 2.9248\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6874 - val_loss: 2.8358\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6829 - val_loss: 2.9270\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6878 - val_loss: 2.8358\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6819 - val_loss: 2.9269\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6860 - val_loss: 2.8347\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6794 - val_loss: 2.9254\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6830 - val_loss: 2.8331\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6754 - val_loss: 2.9226\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6786 - val_loss: 2.8299\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6692 - val_loss: 2.9182\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6729 - val_loss: 2.8273\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6640 - val_loss: 2.9157\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6690 - val_loss: 2.8272\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6621 - val_loss: 2.9157\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6678 - val_loss: 2.8278\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6617 - val_loss: 2.9165\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6673 - val_loss: 2.8297\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6650 - val_loss: 2.9239\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6732 - val_loss: 2.8339\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6693 - val_loss: 2.9275\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6751 - val_loss: 2.8329\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6672 - val_loss: 2.9239\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6703 - val_loss: 2.8289\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6593 - val_loss: 2.9168\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6612 - val_loss: 2.8236\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6509 - val_loss: 2.9136\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6565 - val_loss: 2.8238\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6494 - val_loss: 2.9154\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6569 - val_loss: 2.8256\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6505 - val_loss: 2.9170\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6573 - val_loss: 2.8259\n",
      "Epoch 366/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6497 - val_loss: 2.9164\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6553 - val_loss: 2.8250\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6470 - val_loss: 2.9151\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6525 - val_loss: 2.8243\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6445 - val_loss: 2.9145\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6505 - val_loss: 2.8241\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6430 - val_loss: 2.9147\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6493 - val_loss: 2.8242\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6418 - val_loss: 2.9151\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6483 - val_loss: 2.8242\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6407 - val_loss: 2.9159\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6474 - val_loss: 2.8242\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6397 - val_loss: 2.9171\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6468 - val_loss: 2.8243\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6391 - val_loss: 2.9190\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6468 - val_loss: 2.8247\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6387 - val_loss: 2.9213\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6469 - val_loss: 2.8250\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6384 - val_loss: 2.9234\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6470 - val_loss: 2.8252\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6380 - val_loss: 2.9251\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6468 - val_loss: 2.8253\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6372 - val_loss: 2.9260\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6459 - val_loss: 2.8251\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6358 - val_loss: 2.9255\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6442 - val_loss: 2.8246\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6336 - val_loss: 2.9237\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6414 - val_loss: 2.8238\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6307 - val_loss: 2.9206\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6378 - val_loss: 2.8228\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6274 - val_loss: 2.9169\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6338 - val_loss: 2.8219\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6240 - val_loss: 2.9131\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6298 - val_loss: 2.8211\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6210 - val_loss: 2.9098\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6260 - val_loss: 2.8206\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6183 - val_loss: 2.9069\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6225 - val_loss: 2.8209\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6158 - val_loss: 2.9037\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6196 - val_loss: 2.8210\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6133 - val_loss: 2.8985\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6147 - val_loss: 2.8183\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6072 - val_loss: 2.8919\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6076 - val_loss: 2.8155\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6021 - val_loss: 2.8908\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6049 - val_loss: 2.8159\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6020 - val_loss: 2.8953\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6072 - val_loss: 2.8180\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6049 - val_loss: 2.9028\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6117 - val_loss: 2.8218\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6107 - val_loss: 2.9189\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6226 - val_loss: 2.8259\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6186 - val_loss: 2.9383\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6354 - val_loss: 2.8282\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6256 - val_loss: 2.9490\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6407 - val_loss: 2.8257\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6244 - val_loss: 2.9473\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6370 - val_loss: 2.8234\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6232 - val_loss: 2.9526\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6401 - val_loss: 2.8248\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6251 - val_loss: 2.9548\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6406 - val_loss: 2.8240\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6218 - val_loss: 2.9471\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6335 - val_loss: 2.8209\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.6133 - val_loss: 2.9347\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6226 - val_loss: 2.8189\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6048 - val_loss: 2.9213\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6120 - val_loss: 2.8185\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5995 - val_loss: 2.9151\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6077 - val_loss: 2.8207\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5983 - val_loss: 2.9120\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6052 - val_loss: 2.8225\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5976 - val_loss: 2.9105\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6029 - val_loss: 2.8234\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5960 - val_loss: 2.9079\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5997 - val_loss: 2.8225\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5933 - val_loss: 2.9063\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5967 - val_loss: 2.8205\n",
      "Epoch 444/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5903 - val_loss: 2.9065\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5951 - val_loss: 2.8205\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5905 - val_loss: 2.9122\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5983 - val_loss: 2.8220\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5931 - val_loss: 2.9207\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6040 - val_loss: 2.8221\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5954 - val_loss: 2.9290\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6076 - val_loss: 2.8213\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5961 - val_loss: 2.9337\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6087 - val_loss: 2.8200\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5949 - val_loss: 2.9340\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6075 - val_loss: 2.8189\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5927 - val_loss: 2.9333\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6062 - val_loss: 2.8189\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5915 - val_loss: 2.9320\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6045 - val_loss: 2.8192\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5898 - val_loss: 2.9279\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6006 - val_loss: 2.8189\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5865 - val_loss: 2.9231\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5962 - val_loss: 2.8186\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5835 - val_loss: 2.9203\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5930 - val_loss: 2.8186\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5815 - val_loss: 2.9194\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5913 - val_loss: 2.8190\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5806 - val_loss: 2.9200\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5909 - val_loss: 2.8196\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5804 - val_loss: 2.9209\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5905 - val_loss: 2.8197\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5797 - val_loss: 2.9215\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5897 - val_loss: 2.8193\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5786 - val_loss: 2.9227\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5888 - val_loss: 2.8177\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5765 - val_loss: 2.9231\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5869 - val_loss: 2.8154\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5739 - val_loss: 2.9251\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5871 - val_loss: 2.8156\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5746 - val_loss: 2.9305\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5903 - val_loss: 2.8169\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5767 - val_loss: 2.9345\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5921 - val_loss: 2.8167\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5757 - val_loss: 2.9328\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5895 - val_loss: 2.8150\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5721 - val_loss: 2.9293\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5856 - val_loss: 2.8143\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5694 - val_loss: 2.9276\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5834 - val_loss: 2.8147\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5683 - val_loss: 2.9274\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5825 - val_loss: 2.8158\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5678 - val_loss: 2.9271\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5817 - val_loss: 2.8172\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5675 - val_loss: 2.9254\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5799 - val_loss: 2.8186\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5666 - val_loss: 2.9231\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5780 - val_loss: 2.8203\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5660 - val_loss: 2.9183\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5738 - val_loss: 2.8221\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5643 - val_loss: 2.9151\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5712 - val_loss: 2.8263\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5656 - val_loss: 2.9137\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5703 - val_loss: 2.8262\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5626 - val_loss: 2.9054\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5619 - val_loss: 2.8199\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5516 - val_loss: 2.8961\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5520 - val_loss: 2.8152\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5441 - val_loss: 2.8931\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5483 - val_loss: 2.8150\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5440 - val_loss: 2.8974\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5510 - val_loss: 2.8181\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5485 - val_loss: 2.9039\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5556 - val_loss: 2.8209\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5526 - val_loss: 2.9133\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5630 - val_loss: 2.8280\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5621 - val_loss: 2.9246\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5716 - val_loss: 2.8275\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5626 - val_loss: 2.9266\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5703 - val_loss: 2.8213\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5550 - val_loss: 2.9260\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5642 - val_loss: 2.8149\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5522 - val_loss: 2.9389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5733 - val_loss: 2.8172\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5620 - val_loss: 2.9575\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5870 - val_loss: 2.8206\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5694 - val_loss: 2.9599\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5866 - val_loss: 2.8181\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5634 - val_loss: 2.9539\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5803 - val_loss: 2.8278\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5660 - val_loss: 2.9674\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5932 - val_loss: 2.8503\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5789 - val_loss: 2.9710\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6048 - val_loss: 2.8517\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5697 - val_loss: 2.9364\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5791 - val_loss: 2.8319\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5389 - val_loss: 2.9014\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5494 - val_loss: 2.8250\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5262 - val_loss: 2.8994\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5444 - val_loss: 2.8298\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5289 - val_loss: 2.9117\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5531 - val_loss: 2.8354\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5351 - val_loss: 2.9174\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5561 - val_loss: 2.8335\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5320 - val_loss: 2.9116\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5490 - val_loss: 2.8294\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5254 - val_loss: 2.9063\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5427 - val_loss: 2.8283\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5218 - val_loss: 2.9061\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5406 - val_loss: 2.8294\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5210 - val_loss: 2.9076\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5409 - val_loss: 2.8306\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5204 - val_loss: 2.9072\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5400 - val_loss: 2.8306\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5180 - val_loss: 2.9037\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5368 - val_loss: 2.8299\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5145 - val_loss: 2.8991\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5325 - val_loss: 2.8294\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5110 - val_loss: 2.8956\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5290 - val_loss: 2.8296\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5085 - val_loss: 2.8940\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5271 - val_loss: 2.8301\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5072 - val_loss: 2.8934\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5258 - val_loss: 2.8302\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5057 - val_loss: 2.8926\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5241 - val_loss: 2.8299\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5041 - val_loss: 2.8921\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5223 - val_loss: 2.8295\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5025 - val_loss: 2.8922\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5208 - val_loss: 2.8292\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5011 - val_loss: 2.8928\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5197 - val_loss: 2.8289\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4999 - val_loss: 2.8940\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5188 - val_loss: 2.8286\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4987 - val_loss: 2.8958\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5179 - val_loss: 2.8279\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4975 - val_loss: 2.8991\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5175 - val_loss: 2.8275\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.4977 - val_loss: 2.9054\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5192 - val_loss: 2.8271\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4990 - val_loss: 2.9100\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5200 - val_loss: 2.8248\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4975 - val_loss: 2.9088\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5162 - val_loss: 2.8215\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4941 - val_loss: 2.9083\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5127 - val_loss: 2.8186\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4918 - val_loss: 2.9116\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5127 - val_loss: 2.8161\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4914 - val_loss: 2.9138\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5116 - val_loss: 2.8115\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4875 - val_loss: 2.9091\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5052 - val_loss: 2.8057\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4833 - val_loss: 2.9093\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5021 - val_loss: 2.8027\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4831 - val_loss: 2.9147\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5038 - val_loss: 2.8068\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4906 - val_loss: 2.9308\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5196 - val_loss: 2.8417\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5421 - val_loss: 2.9932\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5967 - val_loss: 2.8722\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5776 - val_loss: 2.9520\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 192us/step - loss: 57.1524 - val_loss: 54.6419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.4097 - val_loss: 50.4971\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 50.2723 - val_loss: 45.1621\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 44.9420 - val_loss: 38.3905\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 38.1726 - val_loss: 30.4107\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 30.2034 - val_loss: 22.9530\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.9188 - val_loss: 18.9102\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.0052 - val_loss: 17.1117\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.1829 - val_loss: 16.2574\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.3655 - val_loss: 15.7759\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 15.9318 - val_loss: 15.3344\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 15.5158 - val_loss: 14.7688\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.9561 - val_loss: 13.9252\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.1237 - val_loss: 12.7523\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.0024 - val_loss: 11.2137\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 11.6160 - val_loss: 9.9227\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 10.4623 - val_loss: 8.9812\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.5425 - val_loss: 8.4277\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.0064 - val_loss: 8.2556\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.8072 - val_loss: 8.1504\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.6713 - val_loss: 8.0676\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5757 - val_loss: 7.9847\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4899 - val_loss: 7.8985\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3996 - val_loss: 7.8095\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3032 - val_loss: 7.7260\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2124 - val_loss: 7.6453\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1213 - val_loss: 7.5595\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.0237 - val_loss: 7.4654\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9167 - val_loss: 7.3626\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8007 - val_loss: 7.2509\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6755 - val_loss: 7.1291\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.5395 - val_loss: 6.9960\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.3917 - val_loss: 6.8510\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.2315 - val_loss: 6.6959\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.0598 - val_loss: 6.5339\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 6.8792 - val_loss: 6.3679\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.6924 - val_loss: 6.1992\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 6.5008 - val_loss: 6.0278\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.3056 - val_loss: 5.8552\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.1094 - val_loss: 5.6850\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.9138 - val_loss: 5.5191\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.7202 - val_loss: 5.3565\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.5297 - val_loss: 5.1978\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.3432 - val_loss: 5.0461\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.1623 - val_loss: 4.9022\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.9897 - val_loss: 4.7686\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.8275 - val_loss: 4.6472\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.6788 - val_loss: 4.5402\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.5455 - val_loss: 4.4446\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.4281 - val_loss: 4.3618\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.3237 - val_loss: 4.2898\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2302 - val_loss: 4.2206\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.1455 - val_loss: 4.1635\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0704 - val_loss: 4.1112\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.0048 - val_loss: 4.0689\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.9482 - val_loss: 4.0168\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.9007 - val_loss: 4.0180\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8701 - val_loss: 3.9796\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.8606 - val_loss: 4.0725\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8947 - val_loss: 4.0382\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.9352 - val_loss: 4.1414\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.9465 - val_loss: 4.0279\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.9200 - val_loss: 4.0591\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.8588 - val_loss: 3.8992\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.7817 - val_loss: 3.9322\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7345 - val_loss: 3.7819\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6572 - val_loss: 3.8166\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6245 - val_loss: 3.6933\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.5705 - val_loss: 3.7568\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5644 - val_loss: 3.6438\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5253 - val_loss: 3.7255\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5306 - val_loss: 3.6092\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4983 - val_loss: 3.7137\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.5134 - val_loss: 3.5946\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4895 - val_loss: 3.7133\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5088 - val_loss: 3.5864\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4873 - val_loss: 3.7218\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.5149 - val_loss: 3.5848\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4908 - val_loss: 3.7101\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5017 - val_loss: 3.5494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4563 - val_loss: 3.6451\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4397 - val_loss: 3.4831\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3846 - val_loss: 3.5719\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.3699 - val_loss: 3.4241\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3227 - val_loss: 3.5152\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3165 - val_loss: 3.3915\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2912 - val_loss: 3.4946\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2970 - val_loss: 3.3798\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.2856 - val_loss: 3.4939\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.2950 - val_loss: 3.3698\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2812 - val_loss: 3.4873\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2890 - val_loss: 3.3566\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2716 - val_loss: 3.4678\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2683 - val_loss: 3.3299\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2454 - val_loss: 3.4369\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2370 - val_loss: 3.2973\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2105 - val_loss: 3.3998\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1987 - val_loss: 3.2676\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1777 - val_loss: 3.3769\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1757 - val_loss: 3.2519\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1598 - val_loss: 3.3616\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1594 - val_loss: 3.2368\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1432 - val_loss: 3.3367\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1366 - val_loss: 3.2154\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1159 - val_loss: 3.3110\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1108 - val_loss: 3.1970\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0949 - val_loss: 3.2960\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0942 - val_loss: 3.1864\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0826 - val_loss: 3.2877\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0835 - val_loss: 3.1779\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0731 - val_loss: 3.2795\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0725 - val_loss: 3.1680\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0616 - val_loss: 3.2669\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0581 - val_loss: 3.1546\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0414 - val_loss: 3.2398\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0341 - val_loss: 3.1373\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0174 - val_loss: 3.2135\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0056 - val_loss: 3.1169\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9897 - val_loss: 3.1910\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9830 - val_loss: 3.1046\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9717 - val_loss: 3.1794\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9694 - val_loss: 3.0988\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9638 - val_loss: 3.1785\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9638 - val_loss: 3.0982\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9635 - val_loss: 3.1843\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9633 - val_loss: 3.0978\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9625 - val_loss: 3.1876\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9626 - val_loss: 3.0986\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9612 - val_loss: 3.1860\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9563 - val_loss: 3.0909\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9508 - val_loss: 3.1758\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9438 - val_loss: 3.0817\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9364 - val_loss: 3.1626\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9295 - val_loss: 3.0724\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9218 - val_loss: 3.1500\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9152 - val_loss: 3.0634\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9079 - val_loss: 3.1383\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9018 - val_loss: 3.0552\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8949 - val_loss: 3.1270\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8884 - val_loss: 3.0470\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8818 - val_loss: 3.1147\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8741 - val_loss: 3.0369\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8660 - val_loss: 3.0988\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8568 - val_loss: 3.0254\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8458 - val_loss: 3.0779\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8358 - val_loss: 3.0150\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8222 - val_loss: 3.0543\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8136 - val_loss: 3.0080\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8041 - val_loss: 3.0388\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7966 - val_loss: 3.0024\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7892 - val_loss: 3.0229\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7829 - val_loss: 3.0035\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7784 - val_loss: 3.0139\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7743 - val_loss: 3.0044\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7708 - val_loss: 3.0094\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7675 - val_loss: 3.0025\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7643 - val_loss: 3.0064\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7613 - val_loss: 2.9996\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7583 - val_loss: 3.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7555 - val_loss: 2.9965\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7526 - val_loss: 3.0027\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7500 - val_loss: 2.9935\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7474 - val_loss: 3.0041\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7453 - val_loss: 2.9877\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7434 - val_loss: 3.0072\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7422 - val_loss: 2.9825\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7411 - val_loss: 3.0142\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7413 - val_loss: 2.9812\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7430 - val_loss: 3.0326\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7493 - val_loss: 2.9881\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7602 - val_loss: 3.0704\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7728 - val_loss: 3.0041\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7982 - val_loss: 3.1392\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8240 - val_loss: 3.0501\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8789 - val_loss: 3.2530\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9194 - val_loss: 3.0938\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9470 - val_loss: 3.2411\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9052 - val_loss: 3.0426\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8729 - val_loss: 3.1607\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8364 - val_loss: 3.0120\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8143 - val_loss: 3.1119\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7914 - val_loss: 2.9990\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7937 - val_loss: 3.1114\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7869 - val_loss: 2.9985\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7945 - val_loss: 3.1206\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7913 - val_loss: 3.0025\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7994 - val_loss: 3.1270\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7953 - val_loss: 3.0040\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7998 - val_loss: 3.1239\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7904 - val_loss: 2.9985\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7907 - val_loss: 3.1079\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7744 - val_loss: 2.9869\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7675 - val_loss: 3.0752\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7484 - val_loss: 2.9757\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7332 - val_loss: 3.0324\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7151 - val_loss: 2.9652\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6995 - val_loss: 2.9893\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6873 - val_loss: 2.9636\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6796 - val_loss: 2.9764\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6751 - val_loss: 2.9569\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6718 - val_loss: 2.9752\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6696 - val_loss: 2.9523\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6679 - val_loss: 2.9759\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6665 - val_loss: 2.9515\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6657 - val_loss: 2.9794\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6657 - val_loss: 2.9493\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6666 - val_loss: 2.9917\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6694 - val_loss: 2.9495\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6747 - val_loss: 3.0093\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6783 - val_loss: 2.9552\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6842 - val_loss: 3.0238\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6861 - val_loss: 2.9597\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6901 - val_loss: 3.0327\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6901 - val_loss: 2.9609\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6948 - val_loss: 3.0418\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6941 - val_loss: 2.9637\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7030 - val_loss: 3.0579\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7037 - val_loss: 2.9670\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7218 - val_loss: 3.1011\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7336 - val_loss: 2.9856\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7703 - val_loss: 3.1621\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7839 - val_loss: 3.0078\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8021 - val_loss: 3.1556\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7785 - val_loss: 2.9884\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7696 - val_loss: 3.1100\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7403 - val_loss: 2.9718\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7367 - val_loss: 3.0855\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7208 - val_loss: 2.9667\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7132 - val_loss: 3.0521\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6940 - val_loss: 2.9492\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6769 - val_loss: 3.0043\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6576 - val_loss: 2.9369\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6446 - val_loss: 2.9758\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6366 - val_loss: 2.9358\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6313 - val_loss: 2.9690\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6299 - val_loss: 2.9332\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6298 - val_loss: 2.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6311 - val_loss: 2.9332\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6334 - val_loss: 2.9878\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6349 - val_loss: 2.9357\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6380 - val_loss: 2.9971\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6401 - val_loss: 2.9381\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6438 - val_loss: 3.0086\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6456 - val_loss: 2.9386\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6506 - val_loss: 3.0223\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6522 - val_loss: 2.9407\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6589 - val_loss: 3.0326\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6585 - val_loss: 2.9450\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6666 - val_loss: 3.0421\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6660 - val_loss: 2.9531\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6782 - val_loss: 3.0683\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6893 - val_loss: 2.9733\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7092 - val_loss: 3.0973\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7138 - val_loss: 2.9766\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7249 - val_loss: 3.1112\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7196 - val_loss: 2.9700\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7238 - val_loss: 3.1051\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7099 - val_loss: 2.9596\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7049 - val_loss: 3.0806\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6912 - val_loss: 2.9558\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6842 - val_loss: 3.0530\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6724 - val_loss: 2.9533\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6637 - val_loss: 3.0229\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6486 - val_loss: 2.9459\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6425 - val_loss: 3.0006\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6274 - val_loss: 2.9276\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6148 - val_loss: 2.9710\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6040 - val_loss: 2.9229\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5986 - val_loss: 2.9608\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5952 - val_loss: 2.9235\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5953 - val_loss: 2.9647\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5948 - val_loss: 2.9239\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5979 - val_loss: 2.9744\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5991 - val_loss: 2.9273\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6052 - val_loss: 2.9888\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6078 - val_loss: 2.9320\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6157 - val_loss: 3.0092\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6206 - val_loss: 2.9378\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6336 - val_loss: 3.0304\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6385 - val_loss: 2.9488\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6509 - val_loss: 3.0531\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6625 - val_loss: 2.9591\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6677 - val_loss: 3.0648\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6700 - val_loss: 2.9550\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6686 - val_loss: 3.0656\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6666 - val_loss: 2.9500\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6634 - val_loss: 3.0582\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6592 - val_loss: 2.9475\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6538 - val_loss: 3.0431\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6482 - val_loss: 2.9453\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6413 - val_loss: 3.0206\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6306 - val_loss: 2.9411\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6259 - val_loss: 3.0063\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6177 - val_loss: 2.9374\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6176 - val_loss: 3.0033\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6137 - val_loss: 2.9362\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6164 - val_loss: 3.0065\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6148 - val_loss: 2.9374\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6186 - val_loss: 3.0108\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6178 - val_loss: 2.9392\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6210 - val_loss: 3.0135\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6199 - val_loss: 2.9396\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6215 - val_loss: 3.0137\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6194 - val_loss: 2.9387\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6200 - val_loss: 3.0124\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6175 - val_loss: 2.9378\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6178 - val_loss: 3.0106\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6154 - val_loss: 2.9370\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6158 - val_loss: 3.0091\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6135 - val_loss: 2.9364\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6140 - val_loss: 3.0080\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6119 - val_loss: 2.9357\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6123 - val_loss: 3.0071\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6104 - val_loss: 2.9351\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6108 - val_loss: 3.0061\n",
      "Epoch 316/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6089 - val_loss: 2.9345\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6091 - val_loss: 3.0051\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6073 - val_loss: 2.9339\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6074 - val_loss: 3.0039\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6056 - val_loss: 2.9334\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6057 - val_loss: 3.0025\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6039 - val_loss: 2.9329\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6037 - val_loss: 3.0006\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6017 - val_loss: 2.9321\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6008 - val_loss: 2.9974\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5987 - val_loss: 2.9309\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5970 - val_loss: 2.9937\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5949 - val_loss: 2.9296\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5936 - val_loss: 2.9913\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5921 - val_loss: 2.9289\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5916 - val_loss: 2.9908\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5908 - val_loss: 2.9286\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5907 - val_loss: 2.9914\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5904 - val_loss: 2.9285\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5904 - val_loss: 2.9921\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5901 - val_loss: 2.9283\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5900 - val_loss: 2.9927\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5896 - val_loss: 2.9280\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5895 - val_loss: 2.9931\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5891 - val_loss: 2.9275\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5888 - val_loss: 2.9933\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5883 - val_loss: 2.9270\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5879 - val_loss: 2.9931\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5873 - val_loss: 2.9264\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5866 - val_loss: 2.9924\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5859 - val_loss: 2.9257\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5849 - val_loss: 2.9911\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5841 - val_loss: 2.9250\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5828 - val_loss: 2.9893\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5819 - val_loss: 2.9242\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5805 - val_loss: 2.9875\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5797 - val_loss: 2.9235\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5785 - val_loss: 2.9862\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5779 - val_loss: 2.9230\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5769 - val_loss: 2.9856\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5766 - val_loss: 2.9226\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5758 - val_loss: 2.9855\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5756 - val_loss: 2.9223\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5750 - val_loss: 2.9857\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5748 - val_loss: 2.9219\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5742 - val_loss: 2.9859\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5741 - val_loss: 2.9216\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5734 - val_loss: 2.9860\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5732 - val_loss: 2.9212\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5726 - val_loss: 2.9859\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5722 - val_loss: 2.9208\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5715 - val_loss: 2.9855\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5711 - val_loss: 2.9204\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5703 - val_loss: 2.9850\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5698 - val_loss: 2.9200\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5691 - val_loss: 2.9843\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5685 - val_loss: 2.9196\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5677 - val_loss: 2.9836\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5671 - val_loss: 2.9192\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5664 - val_loss: 2.9830\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5658 - val_loss: 2.9190\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5652 - val_loss: 2.9824\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5645 - val_loss: 2.9187\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5641 - val_loss: 2.9820\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5633 - val_loss: 2.9185\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5630 - val_loss: 2.9817\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5622 - val_loss: 2.9183\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5620 - val_loss: 2.9814\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5611 - val_loss: 2.9182\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5610 - val_loss: 2.9811\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5600 - val_loss: 2.9180\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5600 - val_loss: 2.9808\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5589 - val_loss: 2.9178\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5590 - val_loss: 2.9804\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5578 - val_loss: 2.9177\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5579 - val_loss: 2.9800\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5566 - val_loss: 2.9175\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5568 - val_loss: 2.9795\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5554 - val_loss: 2.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5557 - val_loss: 2.9790\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5541 - val_loss: 2.9172\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5546 - val_loss: 2.9785\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5529 - val_loss: 2.9170\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5535 - val_loss: 2.9780\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5517 - val_loss: 2.9168\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5524 - val_loss: 2.9776\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5506 - val_loss: 2.9167\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5514 - val_loss: 2.9773\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5495 - val_loss: 2.9165\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5504 - val_loss: 2.9770\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5484 - val_loss: 2.9164\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5496 - val_loss: 2.9768\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5474 - val_loss: 2.9164\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5489 - val_loss: 2.9769\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5467 - val_loss: 2.9167\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5486 - val_loss: 2.9773\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5462 - val_loss: 2.9173\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5487 - val_loss: 2.9780\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5461 - val_loss: 2.9179\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5489 - val_loss: 2.9786\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5458 - val_loss: 2.9179\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5484 - val_loss: 2.9784\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5448 - val_loss: 2.9170\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5469 - val_loss: 2.9776\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5431 - val_loss: 2.9162\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5452 - val_loss: 2.9766\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5414 - val_loss: 2.9163\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5442 - val_loss: 2.9759\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5404 - val_loss: 2.9179\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5443 - val_loss: 2.9760\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5404 - val_loss: 2.9205\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5453 - val_loss: 2.9762\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5406 - val_loss: 2.9214\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5446 - val_loss: 2.9748\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5388 - val_loss: 2.9154\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5375 - val_loss: 2.9683\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5311 - val_loss: 2.9124\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5318 - val_loss: 2.9658\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5285 - val_loss: 2.9145\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5315 - val_loss: 2.9649\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5285 - val_loss: 2.9203\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5341 - val_loss: 2.9663\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5307 - val_loss: 2.9225\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5350 - val_loss: 2.9662\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5297 - val_loss: 2.9209\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5329 - val_loss: 2.9661\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5279 - val_loss: 2.9186\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5309 - val_loss: 2.9661\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5263 - val_loss: 2.9179\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5305 - val_loss: 2.9678\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5265 - val_loss: 2.9207\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5339 - val_loss: 2.9722\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5302 - val_loss: 2.9250\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5402 - val_loss: 2.9792\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5358 - val_loss: 2.9260\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5444 - val_loss: 2.9830\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5375 - val_loss: 2.9231\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5438 - val_loss: 2.9830\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5348 - val_loss: 2.9157\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5380 - val_loss: 2.9791\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5280 - val_loss: 2.9075\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5304 - val_loss: 2.9748\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5221 - val_loss: 2.9046\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5270 - val_loss: 2.9739\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5209 - val_loss: 2.9066\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5277 - val_loss: 2.9742\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5222 - val_loss: 2.9114\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5302 - val_loss: 2.9744\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5240 - val_loss: 2.9188\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5340 - val_loss: 2.9746\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5276 - val_loss: 2.9247\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5344 - val_loss: 2.9700\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5269 - val_loss: 2.9286\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5315 - val_loss: 2.9633\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5239 - val_loss: 2.9308\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5266 - val_loss: 2.9549\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5179 - val_loss: 2.9302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5196 - val_loss: 2.9469\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5106 - val_loss: 2.9248\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5095 - val_loss: 2.9369\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4996 - val_loss: 2.9155\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4956 - val_loss: 2.9266\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4883 - val_loss: 2.9122\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.4878 - val_loss: 2.9229\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4846 - val_loss: 2.9143\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.4872 - val_loss: 2.9243\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4858 - val_loss: 2.9180\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4901 - val_loss: 2.9291\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4899 - val_loss: 2.9216\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4951 - val_loss: 2.9357\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4949 - val_loss: 2.9259\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5020 - val_loss: 2.9442\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5019 - val_loss: 2.9360\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5163 - val_loss: 2.9602\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5172 - val_loss: 2.9389\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5281 - val_loss: 2.9701\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5214 - val_loss: 2.9323\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5312 - val_loss: 2.9805\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5254 - val_loss: 2.9296\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5382 - val_loss: 2.9929\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5320 - val_loss: 2.9255\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5424 - val_loss: 3.0024\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5360 - val_loss: 2.9166\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5403 - val_loss: 3.0011\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5292 - val_loss: 2.9031\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5296 - val_loss: 2.9938\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5180 - val_loss: 2.8902\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5155 - val_loss: 2.9830\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5044 - val_loss: 2.8768\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5005 - val_loss: 2.9654\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4856 - val_loss: 2.8631\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4831 - val_loss: 2.9524\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4725 - val_loss: 2.8588\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.4746 - val_loss: 2.9440\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4658 - val_loss: 2.8586\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4705 - val_loss: 2.9394\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4633 - val_loss: 2.8634\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4709 - val_loss: 2.9392\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4657 - val_loss: 2.8745\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4765 - val_loss: 2.9446\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4759 - val_loss: 2.9019\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4963 - val_loss: 2.9661\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5083 - val_loss: 2.9485\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5370 - val_loss: 2.9735\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5283 - val_loss: 2.9547\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5368 - val_loss: 2.9602\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5137 - val_loss: 2.9338\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5113 - val_loss: 2.9433\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4940 - val_loss: 2.9264\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4969 - val_loss: 2.9361\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4892 - val_loss: 2.9301\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4950 - val_loss: 2.9328\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4878 - val_loss: 2.9308\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4923 - val_loss: 2.9306\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4853 - val_loss: 2.9284\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4886 - val_loss: 2.9286\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.4816 - val_loss: 2.9266\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.4859 - val_loss: 2.9283\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4801 - val_loss: 2.9271\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4858 - val_loss: 2.9296\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4804 - val_loss: 2.9289\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4875 - val_loss: 2.9325\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4823 - val_loss: 2.9322\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4917 - val_loss: 2.9372\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4859 - val_loss: 2.9361\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4971 - val_loss: 2.9417\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4886 - val_loss: 2.9359\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4989 - val_loss: 2.9439\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4880 - val_loss: 2.9327\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4981 - val_loss: 2.9461\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4871 - val_loss: 2.9312\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4997 - val_loss: 2.9504\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4890 - val_loss: 2.9321\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5043 - val_loss: 2.9569\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4928 - val_loss: 2.9330\n",
      "Epoch 551/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5095 - val_loss: 2.9632\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4964 - val_loss: 2.9322\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5124 - val_loss: 2.9668\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4976 - val_loss: 2.9296\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5118 - val_loss: 2.9666\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4957 - val_loss: 2.9265\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5087 - val_loss: 2.9639\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4921 - val_loss: 2.9239\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5050 - val_loss: 2.9605\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4881 - val_loss: 2.9217\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5007 - val_loss: 2.9565\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4837 - val_loss: 2.9125\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4891 - val_loss: 2.9447\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4703 - val_loss: 2.9013\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4752 - val_loss: 2.9377\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4607 - val_loss: 2.8931\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4651 - val_loss: 2.9313\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4526 - val_loss: 2.8890\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.4588 - val_loss: 2.9290\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4494 - val_loss: 2.8936\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4613 - val_loss: 2.9341\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4563 - val_loss: 2.9079\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.4728 - val_loss: 2.9439\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4718 - val_loss: 2.9350\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.4947 - val_loss: 2.9488\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4832 - val_loss: 2.9453\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4999 - val_loss: 2.9437\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4816 - val_loss: 2.9428\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.4894 - val_loss: 2.9305\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4698 - val_loss: 2.9355\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4731 - val_loss: 2.9154\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4564 - val_loss: 2.9317\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.4593 - val_loss: 2.9013\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4472 - val_loss: 2.9283\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4455 - val_loss: 2.8873\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4401 - val_loss: 2.9321\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4414 - val_loss: 2.8806\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4387 - val_loss: 2.9321\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4383 - val_loss: 2.8777\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4365 - val_loss: 2.9318\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4362 - val_loss: 2.8786\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4359 - val_loss: 2.9330\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4372 - val_loss: 2.8811\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4373 - val_loss: 2.9367\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4411 - val_loss: 2.8869\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4432 - val_loss: 2.9467\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4517 - val_loss: 2.8971\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4568 - val_loss: 2.9602\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4656 - val_loss: 2.9033\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4642 - val_loss: 2.9603\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 206us/step - loss: 56.7974 - val_loss: 54.9506\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 54.1347 - val_loss: 50.8249\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 50.0058 - val_loss: 45.2078\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 44.3860 - val_loss: 37.6848\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 36.8789 - val_loss: 28.6659\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 28.0520 - val_loss: 21.3098\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 21.3476 - val_loss: 17.8502\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.1817 - val_loss: 16.1651\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 16.5542 - val_loss: 15.2833\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.5946 - val_loss: 14.3700\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.6188 - val_loss: 13.1922\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.4339 - val_loss: 12.1408\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 12.3360 - val_loss: 11.2551\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 11.3078 - val_loss: 10.4268\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.3079 - val_loss: 9.7822\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.5578 - val_loss: 9.3834\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.0835 - val_loss: 9.1800\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.8198 - val_loss: 9.1013\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7057 - val_loss: 8.9946\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.6252 - val_loss: 8.8797\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5557 - val_loss: 8.7991\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4890 - val_loss: 8.7394\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4119 - val_loss: 8.6762\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3313 - val_loss: 8.5940\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2451 - val_loss: 8.5045\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1538 - val_loss: 8.4146\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.0561 - val_loss: 8.3186\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9514 - val_loss: 8.2107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8395 - val_loss: 8.0942\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.7209 - val_loss: 7.9715\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.5963 - val_loss: 7.8420\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.4663 - val_loss: 7.7076\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.3315 - val_loss: 7.5657\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.1918 - val_loss: 7.4187\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.0470 - val_loss: 7.2666\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.8971 - val_loss: 7.1068\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.7420 - val_loss: 6.9398\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.5814 - val_loss: 6.7690\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.4149 - val_loss: 6.5938\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.2421 - val_loss: 6.4124\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 6.0637 - val_loss: 6.2265\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.8806 - val_loss: 6.0392\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.6949 - val_loss: 5.8513\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.5093 - val_loss: 5.6637\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.3265 - val_loss: 5.4796\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.1488 - val_loss: 5.3017\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.9785 - val_loss: 5.1326\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.8177 - val_loss: 4.9780\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.6693 - val_loss: 4.8362\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.5357 - val_loss: 4.7171\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.4171 - val_loss: 4.6154\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.3138 - val_loss: 4.5259\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.2233 - val_loss: 4.4470\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.1429 - val_loss: 4.3792\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0707 - val_loss: 4.3081\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0054 - val_loss: 4.2488\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9470 - val_loss: 4.1814\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8947 - val_loss: 4.1375\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8475 - val_loss: 4.0808\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.8043 - val_loss: 4.0445\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7640 - val_loss: 3.9841\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7273 - val_loss: 3.9745\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.6944 - val_loss: 3.9007\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6694 - val_loss: 3.9491\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6514 - val_loss: 3.8504\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6557 - val_loss: 3.9846\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6555 - val_loss: 3.8320\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6865 - val_loss: 4.0240\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6706 - val_loss: 3.8049\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6756 - val_loss: 3.9582\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6067 - val_loss: 3.7199\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5840 - val_loss: 3.8581\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5251 - val_loss: 3.6494\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4988 - val_loss: 3.7845\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4546 - val_loss: 3.5970\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4398 - val_loss: 3.7426\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4122 - val_loss: 3.5559\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4035 - val_loss: 3.7204\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3825 - val_loss: 3.5169\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3755 - val_loss: 3.7037\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3574 - val_loss: 3.4822\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3466 - val_loss: 3.6735\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3222 - val_loss: 3.4444\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3087 - val_loss: 3.6366\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2837 - val_loss: 3.4090\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2709 - val_loss: 3.6024\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2466 - val_loss: 3.3774\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2375 - val_loss: 3.5789\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2207 - val_loss: 3.3515\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2111 - val_loss: 3.5571\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1966 - val_loss: 3.3287\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1950 - val_loss: 3.5467\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1840 - val_loss: 3.3084\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1788 - val_loss: 3.5274\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1629 - val_loss: 3.2842\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1525 - val_loss: 3.4981\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1366 - val_loss: 3.2618\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1248 - val_loss: 3.4679\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1104 - val_loss: 3.2416\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1020 - val_loss: 3.4463\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0913 - val_loss: 3.2271\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0860 - val_loss: 3.4315\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0763 - val_loss: 3.2146\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0724 - val_loss: 3.4164\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0616 - val_loss: 3.2024\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0565 - val_loss: 3.3960\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0430 - val_loss: 3.1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0368 - val_loss: 3.3732\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0239 - val_loss: 3.1749\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0153 - val_loss: 3.3510\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0050 - val_loss: 3.1649\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0016 - val_loss: 3.3415\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9957 - val_loss: 3.1581\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9963 - val_loss: 3.3412\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9922 - val_loss: 3.1531\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9942 - val_loss: 3.3404\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9889 - val_loss: 3.1469\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9891 - val_loss: 3.3369\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9832 - val_loss: 3.1401\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9826 - val_loss: 3.3322\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9775 - val_loss: 3.1326\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9756 - val_loss: 3.3232\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9693 - val_loss: 3.1243\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9656 - val_loss: 3.3110\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9587 - val_loss: 3.1160\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9547 - val_loss: 3.2979\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9478 - val_loss: 3.1084\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9445 - val_loss: 3.2855\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9376 - val_loss: 3.1013\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9351 - val_loss: 3.2753\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9283 - val_loss: 3.0948\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9267 - val_loss: 3.2666\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9195 - val_loss: 3.0882\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9179 - val_loss: 3.2569\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9097 - val_loss: 3.0809\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9069 - val_loss: 3.2454\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8989 - val_loss: 3.0746\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8962 - val_loss: 3.2363\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8896 - val_loss: 3.0708\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8868 - val_loss: 3.2263\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8804 - val_loss: 3.0649\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8783 - val_loss: 3.2192\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8726 - val_loss: 3.0591\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8704 - val_loss: 3.2134\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8648 - val_loss: 3.0531\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8626 - val_loss: 3.2061\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8570 - val_loss: 3.0481\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8559 - val_loss: 3.2000\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8511 - val_loss: 3.0446\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8515 - val_loss: 3.1971\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8474 - val_loss: 3.0416\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8491 - val_loss: 3.1973\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8457 - val_loss: 3.0386\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8484 - val_loss: 3.1999\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8453 - val_loss: 3.0358\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8490 - val_loss: 3.2032\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8455 - val_loss: 3.0330\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8497 - val_loss: 3.2057\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8451 - val_loss: 3.0301\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8492 - val_loss: 3.2062\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8434 - val_loss: 3.0268\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8469 - val_loss: 3.2044\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8402 - val_loss: 3.0231\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8435 - val_loss: 3.2014\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8362 - val_loss: 3.0193\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8397 - val_loss: 3.1985\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8323 - val_loss: 3.0130\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8334 - val_loss: 3.1914\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8257 - val_loss: 3.0081\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8265 - val_loss: 3.1846\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8193 - val_loss: 3.0044\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8210 - val_loss: 3.1799\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8148 - val_loss: 3.0015\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8173 - val_loss: 3.1772\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8116 - val_loss: 2.9988\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8143 - val_loss: 3.1755\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8087 - val_loss: 2.9957\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8114 - val_loss: 3.1739\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8059 - val_loss: 2.9922\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8080 - val_loss: 3.1718\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8026 - val_loss: 2.9886\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8042 - val_loss: 3.1692\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7991 - val_loss: 2.9853\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8004 - val_loss: 3.1664\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7955 - val_loss: 2.9824\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7966 - val_loss: 3.1634\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7920 - val_loss: 2.9796\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7928 - val_loss: 3.1600\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7883 - val_loss: 2.9770\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7889 - val_loss: 3.1561\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7844 - val_loss: 2.9744\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7849 - val_loss: 3.1514\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7801 - val_loss: 2.9718\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7806 - val_loss: 3.1456\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7754 - val_loss: 2.9692\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7758 - val_loss: 3.1391\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7705 - val_loss: 2.9668\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7708 - val_loss: 3.1337\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7662 - val_loss: 2.9646\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7667 - val_loss: 3.1310\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7632 - val_loss: 2.9625\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7639 - val_loss: 3.1314\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7616 - val_loss: 2.9605\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7624 - val_loss: 3.1345\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7610 - val_loss: 2.9587\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7618 - val_loss: 3.1385\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7605 - val_loss: 2.9568\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7611 - val_loss: 3.1430\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7600 - val_loss: 2.9550\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7606 - val_loss: 3.1530\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7620 - val_loss: 2.9535\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7634 - val_loss: 3.1623\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7654 - val_loss: 2.9525\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7681 - val_loss: 3.1726\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7698 - val_loss: 2.9531\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7700 - val_loss: 3.1739\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7684 - val_loss: 2.9507\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7659 - val_loss: 3.1679\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7628 - val_loss: 2.9479\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7606 - val_loss: 3.1626\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7576 - val_loss: 2.9456\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7559 - val_loss: 3.1576\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7527 - val_loss: 2.9436\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7505 - val_loss: 3.1460\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7445 - val_loss: 2.9407\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7424 - val_loss: 3.1331\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7360 - val_loss: 2.9375\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7353 - val_loss: 3.1277\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7321 - val_loss: 2.9370\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7313 - val_loss: 3.1242\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7296 - val_loss: 2.9364\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7277 - val_loss: 3.1203\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7266 - val_loss: 2.9354\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7237 - val_loss: 3.1165\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7229 - val_loss: 2.9337\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7199 - val_loss: 3.1138\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7193 - val_loss: 2.9317\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7171 - val_loss: 3.1129\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7165 - val_loss: 2.9299\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7154 - val_loss: 3.1135\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7146 - val_loss: 2.9284\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7143 - val_loss: 3.1144\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7130 - val_loss: 2.9272\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.7134 - val_loss: 3.1151\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7114 - val_loss: 2.9262\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7125 - val_loss: 3.1161\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7097 - val_loss: 2.9252\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7112 - val_loss: 3.1170\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7071 - val_loss: 2.9217\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7055 - val_loss: 3.1095\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6971 - val_loss: 2.9148\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6915 - val_loss: 3.0972\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6854 - val_loss: 2.9111\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6806 - val_loss: 3.0755\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6716 - val_loss: 2.9082\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6684 - val_loss: 3.0637\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6627 - val_loss: 2.9076\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6627 - val_loss: 3.0618\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6597 - val_loss: 2.9061\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6613 - val_loss: 3.0646\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6597 - val_loss: 2.9060\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6629 - val_loss: 3.0710\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6618 - val_loss: 2.9061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.6659 - val_loss: 3.0773\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6643 - val_loss: 2.9062\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6681 - val_loss: 3.0817\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6657 - val_loss: 2.9061\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6688 - val_loss: 3.0842\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6659 - val_loss: 2.9057\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6687 - val_loss: 3.0865\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6661 - val_loss: 2.9055\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6691 - val_loss: 3.0910\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6674 - val_loss: 2.9061\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6710 - val_loss: 3.0971\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6700 - val_loss: 2.9071\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6738 - val_loss: 3.1021\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6719 - val_loss: 2.9078\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6756 - val_loss: 3.1049\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6725 - val_loss: 2.9085\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6767 - val_loss: 3.1097\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6749 - val_loss: 2.9106\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6822 - val_loss: 3.1193\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6817 - val_loss: 2.9135\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6896 - val_loss: 3.1342\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6917 - val_loss: 2.9200\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7009 - val_loss: 3.1501\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7059 - val_loss: 2.9252\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7090 - val_loss: 3.1463\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7056 - val_loss: 2.9233\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7002 - val_loss: 3.1283\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6926 - val_loss: 2.9178\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6862 - val_loss: 3.1104\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6783 - val_loss: 2.9131\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6739 - val_loss: 3.0962\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6679 - val_loss: 2.9117\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6667 - val_loss: 3.0890\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6618 - val_loss: 2.9112\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6606 - val_loss: 3.0819\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6559 - val_loss: 2.9098\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6541 - val_loss: 3.0749\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6494 - val_loss: 2.9078\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6476 - val_loss: 3.0687\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6428 - val_loss: 2.9055\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6414 - val_loss: 3.0636\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6360 - val_loss: 2.9005\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6335 - val_loss: 3.0528\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6240 - val_loss: 2.8948\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6233 - val_loss: 3.0481\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6181 - val_loss: 2.8910\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6212 - val_loss: 3.0439\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6166 - val_loss: 2.8978\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6189 - val_loss: 3.0383\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6112 - val_loss: 2.9004\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6066 - val_loss: 3.0189\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5991 - val_loss: 2.9009\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5937 - val_loss: 3.0015\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5887 - val_loss: 2.9014\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5851 - val_loss: 2.9864\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5805 - val_loss: 2.9009\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5781 - val_loss: 2.9768\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5752 - val_loss: 2.9015\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5738 - val_loss: 2.9709\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5720 - val_loss: 2.9095\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5716 - val_loss: 2.9618\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5713 - val_loss: 2.9170\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.5707 - val_loss: 2.9575\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5714 - val_loss: 2.9174\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5701 - val_loss: 2.9590\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5705 - val_loss: 2.9139\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5688 - val_loss: 2.9613\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5690 - val_loss: 2.9118\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5674 - val_loss: 2.9624\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5679 - val_loss: 2.9113\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5664 - val_loss: 2.9626\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5672 - val_loss: 2.9109\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5656 - val_loss: 2.9631\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5664 - val_loss: 2.9102\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5646 - val_loss: 2.9639\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5655 - val_loss: 2.9092\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5636 - val_loss: 2.9650\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5647 - val_loss: 2.9082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5628 - val_loss: 2.9665\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5641 - val_loss: 2.9072\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5622 - val_loss: 2.9684\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5637 - val_loss: 2.9059\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5618 - val_loss: 2.9716\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5637 - val_loss: 2.9039\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5622 - val_loss: 2.9788\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5652 - val_loss: 2.9010\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5650 - val_loss: 2.9959\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5709 - val_loss: 2.8984\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5723 - val_loss: 3.0199\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5798 - val_loss: 2.8929\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5872 - val_loss: 3.0620\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6012 - val_loss: 2.8992\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6276 - val_loss: 3.1371\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6547 - val_loss: 2.9221\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6973 - val_loss: 3.2207\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7237 - val_loss: 2.9473\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7389 - val_loss: 3.2157\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7258 - val_loss: 2.9373\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7143 - val_loss: 3.1783\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6961 - val_loss: 2.9226\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6880 - val_loss: 3.1564\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6765 - val_loss: 2.9138\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6760 - val_loss: 3.1476\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6680 - val_loss: 2.9090\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6701 - val_loss: 3.1442\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6612 - val_loss: 2.9024\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6570 - val_loss: 3.1199\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6410 - val_loss: 2.8921\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6330 - val_loss: 3.0822\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6141 - val_loss: 2.8846\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6067 - val_loss: 3.0492\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5906 - val_loss: 2.8823\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5855 - val_loss: 3.0360\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5780 - val_loss: 2.8828\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5811 - val_loss: 3.0399\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5799 - val_loss: 2.8845\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5845 - val_loss: 3.0470\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5832 - val_loss: 2.8854\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5885 - val_loss: 3.0524\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5864 - val_loss: 2.8859\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5915 - val_loss: 3.0563\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5884 - val_loss: 2.8862\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5930 - val_loss: 3.0585\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5891 - val_loss: 2.8863\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5931 - val_loss: 3.0590\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5886 - val_loss: 2.8861\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5923 - val_loss: 3.0588\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5875 - val_loss: 2.8859\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5911 - val_loss: 3.0584\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5863 - val_loss: 2.8857\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5900 - val_loss: 3.0582\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5852 - val_loss: 2.8855\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5889 - val_loss: 3.0580\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5842 - val_loss: 2.8854\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5881 - val_loss: 3.0581\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5834 - val_loss: 2.8853\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5873 - val_loss: 3.0584\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5827 - val_loss: 2.8852\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5867 - val_loss: 3.0587\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5821 - val_loss: 2.8852\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5860 - val_loss: 3.0589\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5814 - val_loss: 2.8851\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5854 - val_loss: 3.0592\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5807 - val_loss: 2.8850\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5847 - val_loss: 3.0594\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5800 - val_loss: 2.8849\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5839 - val_loss: 3.0595\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5792 - val_loss: 2.8847\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5830 - val_loss: 3.0594\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5784 - val_loss: 2.8846\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5821 - val_loss: 3.0593\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5774 - val_loss: 2.8844\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5811 - val_loss: 3.0591\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5764 - val_loss: 2.8842\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5801 - val_loss: 3.0588\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5754 - val_loss: 2.8840\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5790 - val_loss: 3.0585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5743 - val_loss: 2.8838\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5778 - val_loss: 3.0582\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5732 - val_loss: 2.8836\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5767 - val_loss: 3.0579\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5721 - val_loss: 2.8834\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5756 - val_loss: 3.0576\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5711 - val_loss: 2.8832\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5745 - val_loss: 3.0573\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5700 - val_loss: 2.8830\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5733 - val_loss: 3.0569\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5689 - val_loss: 2.8828\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5718 - val_loss: 3.0556\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5672 - val_loss: 2.8828\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5662 - val_loss: 3.0460\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5594 - val_loss: 2.8807\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5585 - val_loss: 3.0399\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5541 - val_loss: 2.8799\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5550 - val_loss: 3.0387\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5523 - val_loss: 2.8796\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5540 - val_loss: 3.0393\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5520 - val_loss: 2.8797\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5538 - val_loss: 3.0406\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5522 - val_loss: 2.8799\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5542 - val_loss: 3.0429\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5531 - val_loss: 2.8804\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5555 - val_loss: 3.0468\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5548 - val_loss: 2.8811\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5577 - val_loss: 3.0520\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5576 - val_loss: 2.8817\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5613 - val_loss: 3.0594\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5620 - val_loss: 2.8831\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5726 - val_loss: 3.0852\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5796 - val_loss: 2.8911\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5881 - val_loss: 3.0963\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5873 - val_loss: 2.8930\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5937 - val_loss: 3.1001\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5918 - val_loss: 2.8956\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5984 - val_loss: 3.1044\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5950 - val_loss: 2.8985\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6021 - val_loss: 3.1048\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5955 - val_loss: 2.8978\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5996 - val_loss: 3.0985\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5901 - val_loss: 2.8943\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5887 - val_loss: 3.0842\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5783 - val_loss: 2.8891\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5726 - val_loss: 3.0593\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5589 - val_loss: 2.8811\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5515 - val_loss: 3.0363\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5407 - val_loss: 2.8759\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5347 - val_loss: 3.0170\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5262 - val_loss: 2.8720\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5221 - val_loss: 3.0047\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5174 - val_loss: 2.8706\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5106 - val_loss: 2.9947\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5091 - val_loss: 2.8714\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5067 - val_loss: 2.9933\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5076 - val_loss: 2.8722\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5076 - val_loss: 2.9964\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5094 - val_loss: 2.8723\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5092 - val_loss: 2.9999\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5109 - val_loss: 2.8719\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5104 - val_loss: 3.0044\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5128 - val_loss: 2.8721\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5134 - val_loss: 3.0132\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5175 - val_loss: 2.8731\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5212 - val_loss: 3.0320\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5292 - val_loss: 2.8758\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5344 - val_loss: 3.0505\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5425 - val_loss: 2.8803\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5490 - val_loss: 3.0725\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5571 - val_loss: 2.8888\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5717 - val_loss: 3.1036\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5811 - val_loss: 2.8984\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5909 - val_loss: 3.1121\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5885 - val_loss: 2.9007\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5933 - val_loss: 3.1061\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5850 - val_loss: 2.8991\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5878 - val_loss: 3.0986\n",
      "Epoch 499/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5785 - val_loss: 2.8961\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5797 - val_loss: 3.0913\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5713 - val_loss: 2.8919\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5704 - val_loss: 3.0817\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5627 - val_loss: 2.8865\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5574 - val_loss: 3.0668\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5499 - val_loss: 2.8812\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5416 - val_loss: 3.0444\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5333 - val_loss: 2.8756\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5241 - val_loss: 3.0231\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5172 - val_loss: 2.8711\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5129 - val_loss: 3.0128\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5095 - val_loss: 2.8704\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5065 - val_loss: 3.0084\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5046 - val_loss: 2.8703\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5032 - val_loss: 3.0097\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5043 - val_loss: 2.8711\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5049 - val_loss: 3.0182\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5088 - val_loss: 2.8731\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5105 - val_loss: 3.0274\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5144 - val_loss: 2.8741\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5155 - val_loss: 3.0346\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5190 - val_loss: 2.8753\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5202 - val_loss: 3.0482\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5275 - val_loss: 2.8801\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5354 - val_loss: 3.0743\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5464 - val_loss: 2.8882\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5547 - val_loss: 3.0909\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5591 - val_loss: 2.8923\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5621 - val_loss: 3.0929\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5613 - val_loss: 2.8940\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5639 - val_loss: 3.0928\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5616 - val_loss: 2.8948\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5635 - val_loss: 3.0897\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5590 - val_loss: 2.8935\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5592 - val_loss: 3.0838\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5538 - val_loss: 2.8910\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5520 - val_loss: 3.0743\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5455 - val_loss: 2.8858\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5397 - val_loss: 3.0586\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5329 - val_loss: 2.8802\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5256 - val_loss: 3.0431\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5201 - val_loss: 2.8760\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5129 - val_loss: 3.0291\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5090 - val_loss: 2.8727\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5035 - val_loss: 3.0234\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5030 - val_loss: 2.8725\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.4996 - val_loss: 3.0170\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.4982 - val_loss: 2.8716\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4948 - val_loss: 3.0105\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4927 - val_loss: 2.8709\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4905 - val_loss: 3.0099\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.4913 - val_loss: 2.8715\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4913 - val_loss: 3.0172\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.4951 - val_loss: 2.8735\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.4976 - val_loss: 3.0323\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5044 - val_loss: 2.8766\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5082 - val_loss: 3.0512\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5174 - val_loss: 2.8823\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5242 - val_loss: 3.0723\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5331 - val_loss: 2.8877\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5397 - val_loss: 3.0862\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5449 - val_loss: 2.8922\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5472 - val_loss: 3.0900\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5481 - val_loss: 2.8946\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5495 - val_loss: 3.0897\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5484 - val_loss: 2.8952\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5487 - val_loss: 3.0863\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5455 - val_loss: 2.8941\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5445 - val_loss: 3.0806\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5406 - val_loss: 2.8915\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5381 - val_loss: 3.0732\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5338 - val_loss: 2.8880\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5302 - val_loss: 3.0649\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5263 - val_loss: 2.8835\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5217 - val_loss: 3.0568\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5185 - val_loss: 2.8807\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5155 - val_loss: 3.0547\n",
      "Epoch 577/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5151 - val_loss: 2.8802\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5127 - val_loss: 3.0535\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5130 - val_loss: 2.8804\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5109 - val_loss: 3.0523\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5115 - val_loss: 2.8806\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5098 - val_loss: 3.0512\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5105 - val_loss: 2.8807\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5084 - val_loss: 3.0503\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5093 - val_loss: 2.8810\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5067 - val_loss: 3.0490\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5077 - val_loss: 2.8811\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5052 - val_loss: 3.0488\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5069 - val_loss: 2.8810\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5074 - val_loss: 3.0576\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5125 - val_loss: 2.8848\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5161 - val_loss: 3.0696\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5205 - val_loss: 2.8872\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5203 - val_loss: 3.0695\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5203 - val_loss: 2.8864\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5181 - val_loss: 3.0671\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5183 - val_loss: 2.8864\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5167 - val_loss: 3.0669\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5175 - val_loss: 2.8864\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5157 - val_loss: 3.0661\n",
      "Epoch 1/600\n",
      "1295/1295 [==============================] - 0s 134us/step - loss: 56.1117\n",
      "Epoch 2/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 48.5733\n",
      "Epoch 3/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 35.3455\n",
      "Epoch 4/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 21.1316\n",
      "Epoch 5/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 16.4139\n",
      "Epoch 6/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 14.6191\n",
      "Epoch 7/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 12.4823\n",
      "Epoch 8/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 10.1825\n",
      "Epoch 9/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.9012\n",
      "Epoch 10/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 8.6087\n",
      "Epoch 11/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 8.3808\n",
      "Epoch 12/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 8.0861\n",
      "Epoch 13/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 7.7558\n",
      "Epoch 14/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 7.4354\n",
      "Epoch 15/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 7.0827\n",
      "Epoch 16/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 6.7389\n",
      "Epoch 17/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 6.3984\n",
      "Epoch 18/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 6.1118\n",
      "Epoch 19/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 5.7372\n",
      "Epoch 20/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 5.3998\n",
      "Epoch 21/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 5.0744\n",
      "Epoch 22/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 4.7921\n",
      "Epoch 23/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 4.4923\n",
      "Epoch 24/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 4.2748\n",
      "Epoch 25/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 4.1050\n",
      "Epoch 26/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.9559\n",
      "Epoch 27/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.9874\n",
      "Epoch 28/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.8484\n",
      "Epoch 29/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.7464\n",
      "Epoch 30/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.6192\n",
      "Epoch 31/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.8037\n",
      "Epoch 32/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 3.6042\n",
      "Epoch 33/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.6024\n",
      "Epoch 34/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.5072\n",
      "Epoch 35/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.5788\n",
      "Epoch 36/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.4266\n",
      "Epoch 37/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.3484\n",
      "Epoch 38/600\n",
      "1295/1295 [==============================] - 0s 4us/step - loss: 3.2698\n",
      "Epoch 39/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.2388\n",
      "Epoch 40/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.3116\n",
      "Epoch 41/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.7295\n",
      "Epoch 42/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.2590\n",
      "Epoch 43/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.1884\n",
      "Epoch 44/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.3174\n",
      "Epoch 45/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1978\n",
      "Epoch 46/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.3153\n",
      "Epoch 47/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 3.1877\n",
      "Epoch 48/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1810\n",
      "Epoch 49/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1714\n",
      "Epoch 50/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1033\n",
      "Epoch 51/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0853\n",
      "Epoch 52/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.1891\n",
      "Epoch 53/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0088\n",
      "Epoch 54/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0043\n",
      "Epoch 55/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0480\n",
      "Epoch 56/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0320\n",
      "Epoch 57/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.2190\n",
      "Epoch 58/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 3.5993\n",
      "Epoch 59/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0509\n",
      "Epoch 60/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9341\n",
      "Epoch 61/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0830\n",
      "Epoch 62/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1335\n",
      "Epoch 63/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9448\n",
      "Epoch 64/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9511\n",
      "Epoch 65/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.941 - 0s 8us/step - loss: 2.9461\n",
      "Epoch 66/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.0747\n",
      "Epoch 67/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.2148\n",
      "Epoch 68/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8739\n",
      "Epoch 69/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8533\n",
      "Epoch 70/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9036\n",
      "Epoch 71/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9820\n",
      "Epoch 72/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1011\n",
      "Epoch 73/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8706\n",
      "Epoch 74/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8914\n",
      "Epoch 75/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9251\n",
      "Epoch 76/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.4601\n",
      "Epoch 77/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1619\n",
      "Epoch 78/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.1806\n",
      "Epoch 79/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8055\n",
      "Epoch 80/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.8904\n",
      "Epoch 81/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8714\n",
      "Epoch 82/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8506\n",
      "Epoch 83/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8114\n",
      "Epoch 84/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7863\n",
      "Epoch 85/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8566\n",
      "Epoch 86/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9035\n",
      "Epoch 87/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9022\n",
      "Epoch 88/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7870\n",
      "Epoch 89/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9584\n",
      "Epoch 90/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0326\n",
      "Epoch 91/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7549\n",
      "Epoch 92/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8542\n",
      "Epoch 93/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7726\n",
      "Epoch 94/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8265\n",
      "Epoch 95/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7526\n",
      "Epoch 96/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8054\n",
      "Epoch 97/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7731\n",
      "Epoch 98/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8520\n",
      "Epoch 99/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7419\n",
      "Epoch 100/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7674\n",
      "Epoch 101/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9119\n",
      "Epoch 102/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9028\n",
      "Epoch 103/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.9078\n",
      "Epoch 104/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7532\n",
      "Epoch 105/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8657\n",
      "Epoch 106/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7762\n",
      "Epoch 107/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8749\n",
      "Epoch 108/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8283\n",
      "Epoch 109/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7387\n",
      "Epoch 110/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6945\n",
      "Epoch 111/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7931\n",
      "Epoch 112/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6922\n",
      "Epoch 113/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7435\n",
      "Epoch 114/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0066\n",
      "Epoch 115/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7922\n",
      "Epoch 116/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6867\n",
      "Epoch 117/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7531\n",
      "Epoch 118/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8674\n",
      "Epoch 119/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7566\n",
      "Epoch 120/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7051\n",
      "Epoch 121/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7467\n",
      "Epoch 122/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7925\n",
      "Epoch 123/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7749\n",
      "Epoch 124/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7676\n",
      "Epoch 125/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6869\n",
      "Epoch 126/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7175\n",
      "Epoch 127/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7688\n",
      "Epoch 128/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6845\n",
      "Epoch 129/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8083\n",
      "Epoch 130/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8100\n",
      "Epoch 131/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8856\n",
      "Epoch 132/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7296\n",
      "Epoch 133/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7965\n",
      "Epoch 134/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8063\n",
      "Epoch 135/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7924\n",
      "Epoch 136/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7586\n",
      "Epoch 137/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6980\n",
      "Epoch 138/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6173\n",
      "Epoch 139/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7188\n",
      "Epoch 140/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8709\n",
      "Epoch 141/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8924\n",
      "Epoch 142/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7590\n",
      "Epoch 143/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7101\n",
      "Epoch 144/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6173\n",
      "Epoch 145/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6211\n",
      "Epoch 146/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6716\n",
      "Epoch 147/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6503\n",
      "Epoch 148/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7594\n",
      "Epoch 149/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6811\n",
      "Epoch 150/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6421\n",
      "Epoch 151/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6509\n",
      "Epoch 152/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6676\n",
      "Epoch 153/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6235\n",
      "Epoch 154/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6132\n",
      "Epoch 155/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6160\n",
      "Epoch 156/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6497\n",
      "Epoch 157/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7506\n",
      "Epoch 158/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6975\n",
      "Epoch 159/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6163\n",
      "Epoch 160/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6872\n",
      "Epoch 161/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7932\n",
      "Epoch 162/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5992\n",
      "Epoch 163/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7597\n",
      "Epoch 164/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8276\n",
      "Epoch 165/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6747\n",
      "Epoch 166/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7893\n",
      "Epoch 167/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6727\n",
      "Epoch 168/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7659\n",
      "Epoch 169/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6953\n",
      "Epoch 170/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0911\n",
      "Epoch 171/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5842\n",
      "Epoch 172/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6280\n",
      "Epoch 173/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6790\n",
      "Epoch 174/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7505\n",
      "Epoch 175/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6834\n",
      "Epoch 176/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7859\n",
      "Epoch 177/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7521\n",
      "Epoch 178/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6514\n",
      "Epoch 179/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7130\n",
      "Epoch 180/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7712\n",
      "Epoch 181/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6303\n",
      "Epoch 182/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6015\n",
      "Epoch 183/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9325\n",
      "Epoch 184/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5984\n",
      "Epoch 185/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9572\n",
      "Epoch 186/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6746\n",
      "Epoch 187/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6275\n",
      "Epoch 188/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6796\n",
      "Epoch 189/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8987\n",
      "Epoch 190/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6793\n",
      "Epoch 191/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5983\n",
      "Epoch 192/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7160\n",
      "Epoch 193/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6416\n",
      "Epoch 194/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5760\n",
      "Epoch 195/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5917\n",
      "Epoch 196/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6443\n",
      "Epoch 197/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7364\n",
      "Epoch 198/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6199\n",
      "Epoch 199/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5590\n",
      "Epoch 200/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5482\n",
      "Epoch 201/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6098\n",
      "Epoch 202/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5780\n",
      "Epoch 203/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5857\n",
      "Epoch 204/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7852\n",
      "Epoch 205/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7357\n",
      "Epoch 206/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5766\n",
      "Epoch 207/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7494\n",
      "Epoch 208/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5370\n",
      "Epoch 209/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5351\n",
      "Epoch 210/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5216\n",
      "Epoch 211/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5154\n",
      "Epoch 212/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5464\n",
      "Epoch 213/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5650\n",
      "Epoch 214/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5707\n",
      "Epoch 215/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7246\n",
      "Epoch 216/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6703\n",
      "Epoch 217/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5723\n",
      "Epoch 218/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.691 - 0s 6us/step - loss: 2.6761\n",
      "Epoch 219/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7472\n",
      "Epoch 220/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5474\n",
      "Epoch 221/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6532\n",
      "Epoch 222/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7269\n",
      "Epoch 223/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.5802\n",
      "Epoch 224/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5450\n",
      "Epoch 225/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5441\n",
      "Epoch 226/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7325\n",
      "Epoch 227/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.6979\n",
      "Epoch 228/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6342\n",
      "Epoch 229/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7829\n",
      "Epoch 230/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5208\n",
      "Epoch 231/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5973\n",
      "Epoch 232/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5084\n",
      "Epoch 233/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4955\n",
      "Epoch 234/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5734\n",
      "Epoch 235/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5591\n",
      "Epoch 236/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5976\n",
      "Epoch 237/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6888\n",
      "Epoch 238/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5116\n",
      "Epoch 239/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5215\n",
      "Epoch 240/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5531\n",
      "Epoch 241/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.646 - 0s 8us/step - loss: 2.6349\n",
      "Epoch 242/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6762\n",
      "Epoch 243/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6581\n",
      "Epoch 244/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5046\n",
      "Epoch 245/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6992\n",
      "Epoch 246/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5492\n",
      "Epoch 247/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5374\n",
      "Epoch 248/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5513\n",
      "Epoch 249/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5121\n",
      "Epoch 250/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5282\n",
      "Epoch 251/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7701\n",
      "Epoch 252/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4921\n",
      "Epoch 253/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5150\n",
      "Epoch 254/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5810\n",
      "Epoch 255/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6792\n",
      "Epoch 256/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5372\n",
      "Epoch 257/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5348\n",
      "Epoch 258/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5815\n",
      "Epoch 259/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4859\n",
      "Epoch 260/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7013\n",
      "Epoch 261/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5586\n",
      "Epoch 262/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5890\n",
      "Epoch 263/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5682\n",
      "Epoch 264/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5996\n",
      "Epoch 265/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5891\n",
      "Epoch 266/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8815\n",
      "Epoch 267/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5945\n",
      "Epoch 268/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5489\n",
      "Epoch 269/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4710\n",
      "Epoch 270/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6066\n",
      "Epoch 271/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5873\n",
      "Epoch 272/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4845\n",
      "Epoch 273/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5155\n",
      "Epoch 274/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8145\n",
      "Epoch 275/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8729\n",
      "Epoch 276/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6653\n",
      "Epoch 277/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6324\n",
      "Epoch 278/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5078\n",
      "Epoch 279/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5689\n",
      "Epoch 280/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4877\n",
      "Epoch 281/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5577\n",
      "Epoch 282/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5292\n",
      "Epoch 283/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5747\n",
      "Epoch 284/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5081\n",
      "Epoch 285/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4804\n",
      "Epoch 286/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5861\n",
      "Epoch 287/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4727\n",
      "Epoch 288/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5259\n",
      "Epoch 289/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5592\n",
      "Epoch 290/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5069\n",
      "Epoch 291/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5569\n",
      "Epoch 292/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5038\n",
      "Epoch 293/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4751\n",
      "Epoch 294/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5396\n",
      "Epoch 295/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6448\n",
      "Epoch 296/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4831\n",
      "Epoch 297/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4535\n",
      "Epoch 298/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4772\n",
      "Epoch 299/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5098\n",
      "Epoch 300/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5199\n",
      "Epoch 301/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5391\n",
      "Epoch 302/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5511\n",
      "Epoch 303/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5225\n",
      "Epoch 304/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5079\n",
      "Epoch 305/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4577\n",
      "Epoch 306/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5221\n",
      "Epoch 307/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5196\n",
      "Epoch 308/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5637\n",
      "Epoch 309/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5045\n",
      "Epoch 310/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4801\n",
      "Epoch 311/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4452\n",
      "Epoch 312/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4916\n",
      "Epoch 313/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4706\n",
      "Epoch 314/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4387\n",
      "Epoch 315/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5019\n",
      "Epoch 316/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4853\n",
      "Epoch 317/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5310\n",
      "Epoch 318/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4654\n",
      "Epoch 319/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5226\n",
      "Epoch 320/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5294\n",
      "Epoch 321/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5425\n",
      "Epoch 322/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4511\n",
      "Epoch 323/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5572\n",
      "Epoch 324/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8972\n",
      "Epoch 325/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4435\n",
      "Epoch 326/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5554\n",
      "Epoch 327/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6471\n",
      "Epoch 328/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5812\n",
      "Epoch 329/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5801\n",
      "Epoch 330/600\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.4995\n",
      "Epoch 331/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4407\n",
      "Epoch 332/600\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4432\n",
      "Epoch 333/600\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.5579\n",
      "Epoch 334/600\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4935\n",
      "Epoch 335/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6608\n",
      "Epoch 336/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.6195\n",
      "Epoch 337/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5558\n",
      "Epoch 338/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4587\n",
      "Epoch 339/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4102\n",
      "Epoch 340/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4157\n",
      "Epoch 341/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4712\n",
      "Epoch 342/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5215\n",
      "Epoch 343/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4257\n",
      "Epoch 344/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4142\n",
      "Epoch 345/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4722\n",
      "Epoch 346/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5036\n",
      "Epoch 347/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5784\n",
      "Epoch 348/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5968\n",
      "Epoch 349/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4911\n",
      "Epoch 350/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4237\n",
      "Epoch 351/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6634\n",
      "Epoch 352/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5107\n",
      "Epoch 353/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4118\n",
      "Epoch 354/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4164\n",
      "Epoch 355/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6146\n",
      "Epoch 356/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5208\n",
      "Epoch 357/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4663\n",
      "Epoch 358/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4286\n",
      "Epoch 359/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4145\n",
      "Epoch 360/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5036\n",
      "Epoch 361/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5811\n",
      "Epoch 362/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4838\n",
      "Epoch 363/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4868\n",
      "Epoch 364/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4752\n",
      "Epoch 365/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4898\n",
      "Epoch 366/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4747\n",
      "Epoch 367/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4120\n",
      "Epoch 368/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4508\n",
      "Epoch 369/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5104\n",
      "Epoch 370/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6482\n",
      "Epoch 371/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4256\n",
      "Epoch 372/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3915\n",
      "Epoch 373/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4090\n",
      "Epoch 374/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5030\n",
      "Epoch 375/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5329\n",
      "Epoch 376/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5518\n",
      "Epoch 377/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6640\n",
      "Epoch 378/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4281\n",
      "Epoch 379/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5840\n",
      "Epoch 380/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5253\n",
      "Epoch 381/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4612\n",
      "Epoch 382/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4712\n",
      "Epoch 383/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4341\n",
      "Epoch 384/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6831\n",
      "Epoch 385/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4985\n",
      "Epoch 386/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4026\n",
      "Epoch 387/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4083\n",
      "Epoch 388/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4847\n",
      "Epoch 389/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4931\n",
      "Epoch 390/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3856\n",
      "Epoch 391/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4738\n",
      "Epoch 392/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4948\n",
      "Epoch 393/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7242\n",
      "Epoch 394/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4488\n",
      "Epoch 395/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3932\n",
      "Epoch 396/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4079\n",
      "Epoch 397/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5176\n",
      "Epoch 398/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4331\n",
      "Epoch 399/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3936\n",
      "Epoch 400/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4403\n",
      "Epoch 401/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5295\n",
      "Epoch 402/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3984\n",
      "Epoch 403/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5665\n",
      "Epoch 404/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5053\n",
      "Epoch 405/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5139\n",
      "Epoch 406/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4489\n",
      "Epoch 407/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4588\n",
      "Epoch 408/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4441\n",
      "Epoch 409/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5150\n",
      "Epoch 410/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4685\n",
      "Epoch 411/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4840\n",
      "Epoch 412/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4092\n",
      "Epoch 413/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4988\n",
      "Epoch 414/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5082\n",
      "Epoch 415/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4443\n",
      "Epoch 416/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5430\n",
      "Epoch 417/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4872\n",
      "Epoch 418/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4334\n",
      "Epoch 419/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3640\n",
      "Epoch 420/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4043\n",
      "Epoch 421/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4709\n",
      "Epoch 422/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6699\n",
      "Epoch 423/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7000\n",
      "Epoch 424/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4583\n",
      "Epoch 425/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7053\n",
      "Epoch 426/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3690\n",
      "Epoch 427/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3440\n",
      "Epoch 428/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4784\n",
      "Epoch 429/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4119\n",
      "Epoch 430/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5784\n",
      "Epoch 431/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5204\n",
      "Epoch 432/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3507\n",
      "Epoch 433/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3524\n",
      "Epoch 434/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3385\n",
      "Epoch 435/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3537\n",
      "Epoch 436/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5518\n",
      "Epoch 437/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3949\n",
      "Epoch 438/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3621\n",
      "Epoch 439/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4370\n",
      "Epoch 440/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3653\n",
      "Epoch 441/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3650\n",
      "Epoch 442/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4091\n",
      "Epoch 443/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3749\n",
      "Epoch 444/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3744\n",
      "Epoch 445/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4885\n",
      "Epoch 446/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5598\n",
      "Epoch 447/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3303\n",
      "Epoch 448/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.366 - 0s 13us/step - loss: 2.3750\n",
      "Epoch 449/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.5808\n",
      "Epoch 450/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.5380\n",
      "Epoch 451/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4072\n",
      "Epoch 452/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5402\n",
      "Epoch 453/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4283\n",
      "Epoch 454/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4288\n",
      "Epoch 455/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7483\n",
      "Epoch 456/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4532\n",
      "Epoch 457/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5491\n",
      "Epoch 458/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4143\n",
      "Epoch 459/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4971\n",
      "Epoch 460/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4247\n",
      "Epoch 461/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4623\n",
      "Epoch 462/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3448\n",
      "Epoch 463/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3365\n",
      "Epoch 464/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3959\n",
      "Epoch 465/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4913\n",
      "Epoch 466/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4457\n",
      "Epoch 467/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3815\n",
      "Epoch 468/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3301\n",
      "Epoch 469/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4149\n",
      "Epoch 470/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.3534\n",
      "Epoch 471/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3777\n",
      "Epoch 472/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5138\n",
      "Epoch 473/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5705\n",
      "Epoch 474/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4529\n",
      "Epoch 475/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4183\n",
      "Epoch 476/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5100\n",
      "Epoch 477/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3941\n",
      "Epoch 478/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3684\n",
      "Epoch 479/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5248\n",
      "Epoch 480/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5563\n",
      "Epoch 481/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4615\n",
      "Epoch 482/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5240\n",
      "Epoch 483/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3286\n",
      "Epoch 484/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3517\n",
      "Epoch 485/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4903\n",
      "Epoch 486/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5105\n",
      "Epoch 487/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5078\n",
      "Epoch 488/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3479\n",
      "Epoch 489/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4193\n",
      "Epoch 490/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4210\n",
      "Epoch 491/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7016\n",
      "Epoch 492/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4552\n",
      "Epoch 493/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4081\n",
      "Epoch 494/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4934\n",
      "Epoch 495/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3618\n",
      "Epoch 496/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5259\n",
      "Epoch 497/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5314\n",
      "Epoch 498/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3507\n",
      "Epoch 499/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4287\n",
      "Epoch 500/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3259\n",
      "Epoch 501/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4597\n",
      "Epoch 502/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3812\n",
      "Epoch 503/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4712\n",
      "Epoch 504/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4257\n",
      "Epoch 505/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5728\n",
      "Epoch 506/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3414\n",
      "Epoch 507/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4707\n",
      "Epoch 508/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3105\n",
      "Epoch 509/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4037\n",
      "Epoch 510/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4023\n",
      "Epoch 511/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4102\n",
      "Epoch 512/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3519\n",
      "Epoch 513/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3931\n",
      "Epoch 514/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3469\n",
      "Epoch 515/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.2888\n",
      "Epoch 516/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.2981\n",
      "Epoch 517/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4107\n",
      "Epoch 518/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4556\n",
      "Epoch 519/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3384\n",
      "Epoch 520/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4114\n",
      "Epoch 521/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3864\n",
      "Epoch 522/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4908\n",
      "Epoch 523/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4319\n",
      "Epoch 524/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3095\n",
      "Epoch 525/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3663\n",
      "Epoch 526/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3628\n",
      "Epoch 527/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3809\n",
      "Epoch 528/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3035\n",
      "Epoch 529/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3153\n",
      "Epoch 530/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3114\n",
      "Epoch 531/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4099\n",
      "Epoch 532/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4682\n",
      "Epoch 533/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3746\n",
      "Epoch 534/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4858\n",
      "Epoch 535/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4439\n",
      "Epoch 536/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5880\n",
      "Epoch 537/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4487\n",
      "Epoch 538/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3902\n",
      "Epoch 539/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4523\n",
      "Epoch 540/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3014\n",
      "Epoch 542/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3067\n",
      "Epoch 543/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3379\n",
      "Epoch 544/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4117\n",
      "Epoch 545/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4278\n",
      "Epoch 546/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3262\n",
      "Epoch 547/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3265\n",
      "Epoch 548/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4167\n",
      "Epoch 549/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.3497\n",
      "Epoch 550/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.3513\n",
      "Epoch 551/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3727\n",
      "Epoch 552/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3496\n",
      "Epoch 553/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4252\n",
      "Epoch 554/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5487\n",
      "Epoch 555/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3619\n",
      "Epoch 556/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4630\n",
      "Epoch 557/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3933\n",
      "Epoch 558/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6875\n",
      "Epoch 559/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4718\n",
      "Epoch 560/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3046\n",
      "Epoch 561/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.3057\n",
      "Epoch 562/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5243\n",
      "Epoch 563/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5425\n",
      "Epoch 564/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.2854\n",
      "Epoch 565/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4439\n",
      "Epoch 566/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3144\n",
      "Epoch 567/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3776\n",
      "Epoch 568/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4275\n",
      "Epoch 569/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3689\n",
      "Epoch 570/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3365\n",
      "Epoch 571/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.3230\n",
      "Epoch 572/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5387\n",
      "Epoch 573/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.2922\n",
      "Epoch 574/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3492\n",
      "Epoch 575/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4484\n",
      "Epoch 576/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3805\n",
      "Epoch 577/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4479\n",
      "Epoch 578/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4677\n",
      "Epoch 579/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4705\n",
      "Epoch 580/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4314\n",
      "Epoch 581/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.3944\n",
      "Epoch 582/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4005\n",
      "Epoch 583/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3801\n",
      "Epoch 584/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3989\n",
      "Epoch 585/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4126\n",
      "Epoch 586/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5137\n",
      "Epoch 587/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4324\n",
      "Epoch 588/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4386\n",
      "Epoch 589/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.2834\n",
      "Epoch 590/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.2882\n",
      "Epoch 591/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3990\n",
      "Epoch 592/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3774\n",
      "Epoch 593/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3023\n",
      "Epoch 594/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4757\n",
      "Epoch 595/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5814\n",
      "Epoch 596/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3943\n",
      "Epoch 597/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3381\n",
      "Epoch 598/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3140\n",
      "Epoch 599/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.2691\n",
      "Epoch 600/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [56.111654663085936,\n",
       "  48.57334671020508,\n",
       "  35.34547080993652,\n",
       "  21.131558990478517,\n",
       "  16.4138916015625,\n",
       "  14.619064712524414,\n",
       "  12.482343101501465,\n",
       "  10.182450675964356,\n",
       "  8.901171493530274,\n",
       "  8.608712577819825,\n",
       "  8.380832099914551,\n",
       "  8.086119842529296,\n",
       "  7.755750465393066,\n",
       "  7.435436630249024,\n",
       "  7.0826640129089355,\n",
       "  6.738948631286621,\n",
       "  6.398367500305175,\n",
       "  6.111771297454834,\n",
       "  5.73723030090332,\n",
       "  5.39978666305542,\n",
       "  5.074420166015625,\n",
       "  4.7920557975769045,\n",
       "  4.492268466949463,\n",
       "  4.274750137329102,\n",
       "  4.105009365081787,\n",
       "  3.9558905124664308,\n",
       "  3.9873655796051026,\n",
       "  3.848421907424927,\n",
       "  3.746362495422363,\n",
       "  3.61915979385376,\n",
       "  3.8037203788757323,\n",
       "  3.6042335987091065,\n",
       "  3.602437162399292,\n",
       "  3.50720796585083,\n",
       "  3.5787992000579836,\n",
       "  3.4266061305999758,\n",
       "  3.3483601570129395,\n",
       "  3.26975679397583,\n",
       "  3.2387835025787353,\n",
       "  3.3116362571716307,\n",
       "  3.7294626235961914,\n",
       "  3.258966493606567,\n",
       "  3.188412618637085,\n",
       "  3.3173696041107177,\n",
       "  3.1977760791778564,\n",
       "  3.3152568340301514,\n",
       "  3.1876753330230714,\n",
       "  3.1809621334075926,\n",
       "  3.1714477062225344,\n",
       "  3.1032808303833006,\n",
       "  3.0852939605712892,\n",
       "  3.1891133308410646,\n",
       "  3.00876898765564,\n",
       "  3.004310131072998,\n",
       "  3.047980308532715,\n",
       "  3.0320183753967287,\n",
       "  3.2189799785614013,\n",
       "  3.5993470191955566,\n",
       "  3.0508502960205077,\n",
       "  2.934100866317749,\n",
       "  3.0829764366149903,\n",
       "  3.1334754467010497,\n",
       "  2.944832134246826,\n",
       "  2.9511101722717283,\n",
       "  2.946067190170288,\n",
       "  3.0747056007385254,\n",
       "  3.214759111404419,\n",
       "  2.8739433765411375,\n",
       "  2.8532966136932374,\n",
       "  2.9036081790924073,\n",
       "  2.9819873809814452,\n",
       "  3.1011348247528074,\n",
       "  2.870562028884888,\n",
       "  2.891357898712158,\n",
       "  2.9250936985015867,\n",
       "  3.4601120471954347,\n",
       "  3.1618602752685545,\n",
       "  3.180552673339844,\n",
       "  2.8055295944213867,\n",
       "  2.8903560638427734,\n",
       "  2.87141900062561,\n",
       "  2.850625228881836,\n",
       "  2.8114068031311037,\n",
       "  2.7862637042999268,\n",
       "  2.8565797805786133,\n",
       "  2.9035078048706056,\n",
       "  2.9021790504455565,\n",
       "  2.7869987010955812,\n",
       "  2.9584116458892824,\n",
       "  3.0325506210327147,\n",
       "  2.754944658279419,\n",
       "  2.8541815280914307,\n",
       "  2.772649145126343,\n",
       "  2.826536846160889,\n",
       "  2.752615213394165,\n",
       "  2.805395269393921,\n",
       "  2.7730944633483885,\n",
       "  2.8519759654998778,\n",
       "  2.7419105052947996,\n",
       "  2.7673737049102782,\n",
       "  2.911863851547241,\n",
       "  2.902832555770874,\n",
       "  2.907811975479126,\n",
       "  2.7532399177551268,\n",
       "  2.8656896114349366,\n",
       "  2.7761669635772703,\n",
       "  2.8748772144317627,\n",
       "  2.8282527923583984,\n",
       "  2.7386795043945313,\n",
       "  2.6944681644439696,\n",
       "  2.793074369430542,\n",
       "  2.69217963218689,\n",
       "  2.7435259342193605,\n",
       "  3.0065834522247314,\n",
       "  2.7921839714050294,\n",
       "  2.6866853713989256,\n",
       "  2.7530859470367433,\n",
       "  2.8673859596252442,\n",
       "  2.756597328186035,\n",
       "  2.7050654888153076,\n",
       "  2.746734380722046,\n",
       "  2.7925194263458253,\n",
       "  2.774894952774048,\n",
       "  2.7675772666931153,\n",
       "  2.6869465351104735,\n",
       "  2.717535877227783,\n",
       "  2.768804740905762,\n",
       "  2.684496355056763,\n",
       "  2.808282661437988,\n",
       "  2.8099504470825196,\n",
       "  2.8855607986450194,\n",
       "  2.729564142227173,\n",
       "  2.7965491771698,\n",
       "  2.806319808959961,\n",
       "  2.7924084663391113,\n",
       "  2.758642864227295,\n",
       "  2.698035192489624,\n",
       "  2.6172723293304445,\n",
       "  2.7188199520111085,\n",
       "  2.870945930480957,\n",
       "  2.8924302577972414,\n",
       "  2.758996057510376,\n",
       "  2.7100905895233156,\n",
       "  2.617265558242798,\n",
       "  2.6210604190826414,\n",
       "  2.6716312885284426,\n",
       "  2.650316762924194,\n",
       "  2.7594125747680662,\n",
       "  2.681075096130371,\n",
       "  2.6420944690704347,\n",
       "  2.6508508205413817,\n",
       "  2.6675958156585695,\n",
       "  2.6235371112823485,\n",
       "  2.613209867477417,\n",
       "  2.6160192489624023,\n",
       "  2.649690294265747,\n",
       "  2.7506182193756104,\n",
       "  2.697499084472656,\n",
       "  2.616321563720703,\n",
       "  2.6871548175811766,\n",
       "  2.7931935787200928,\n",
       "  2.599241304397583,\n",
       "  2.759738874435425,\n",
       "  2.82759428024292,\n",
       "  2.674708890914917,\n",
       "  2.789291334152222,\n",
       "  2.6726543426513674,\n",
       "  2.7658920288085938,\n",
       "  2.6952868461608888,\n",
       "  3.0910887718200684,\n",
       "  2.584178256988525,\n",
       "  2.6279616355895996,\n",
       "  2.6790284156799316,\n",
       "  2.7504875659942627,\n",
       "  2.6833627223968506,\n",
       "  2.7858745098114013,\n",
       "  2.752068376541138,\n",
       "  2.651380443572998,\n",
       "  2.7130456924438477,\n",
       "  2.771194648742676,\n",
       "  2.630314588546753,\n",
       "  2.601526165008545,\n",
       "  2.9325438499450684,\n",
       "  2.598353052139282,\n",
       "  2.957174205780029,\n",
       "  2.6745794773101808,\n",
       "  2.627533435821533,\n",
       "  2.679569053649902,\n",
       "  2.8987165451049806,\n",
       "  2.679324817657471,\n",
       "  2.5982971668243406,\n",
       "  2.716030550003052,\n",
       "  2.641604709625244,\n",
       "  2.5759684562683107,\n",
       "  2.59165678024292,\n",
       "  2.644250917434692,\n",
       "  2.7364002227783204,\n",
       "  2.6198853969573976,\n",
       "  2.5590352058410644,\n",
       "  2.5481899261474608,\n",
       "  2.6097976207733153,\n",
       "  2.5780264377593993,\n",
       "  2.5857380390167237,\n",
       "  2.7852134704589844,\n",
       "  2.735720634460449,\n",
       "  2.5765838623046875,\n",
       "  2.749365711212158,\n",
       "  2.5370389461517333,\n",
       "  2.5350834846496584,\n",
       "  2.521647024154663,\n",
       "  2.515448474884033,\n",
       "  2.546356439590454,\n",
       "  2.5649829864501954,\n",
       "  2.570688533782959,\n",
       "  2.7246221542358398,\n",
       "  2.670340585708618,\n",
       "  2.572349691390991,\n",
       "  2.676116704940796,\n",
       "  2.747196340560913,\n",
       "  2.5474018096923827,\n",
       "  2.653184413909912,\n",
       "  2.726920413970947,\n",
       "  2.5801743030548097,\n",
       "  2.5449668407440185,\n",
       "  2.5440511226654055,\n",
       "  2.7325249671936036,\n",
       "  2.6978618621826174,\n",
       "  2.6341511726379396,\n",
       "  2.782894468307495,\n",
       "  2.5207844734191895,\n",
       "  2.597335386276245,\n",
       "  2.5083737850189207,\n",
       "  2.4955475330352783,\n",
       "  2.5734062671661375,\n",
       "  2.559065055847168,\n",
       "  2.597638559341431,\n",
       "  2.6888436794281008,\n",
       "  2.5116458892822267,\n",
       "  2.521485614776611,\n",
       "  2.553108644485474,\n",
       "  2.6348587036132813,\n",
       "  2.676185369491577,\n",
       "  2.658118724822998,\n",
       "  2.504580593109131,\n",
       "  2.699201965332031,\n",
       "  2.549223613739014,\n",
       "  2.5374190330505373,\n",
       "  2.5513226985931396,\n",
       "  2.5121185779571533,\n",
       "  2.528154230117798,\n",
       "  2.7700551986694335,\n",
       "  2.492139387130737,\n",
       "  2.5149552822113037,\n",
       "  2.5810027599334715,\n",
       "  2.6792272567749023,\n",
       "  2.537234306335449,\n",
       "  2.5347962856292723,\n",
       "  2.5814780712127687,\n",
       "  2.4858995914459228,\n",
       "  2.7013011932373048,\n",
       "  2.558592653274536,\n",
       "  2.588954305648804,\n",
       "  2.568187189102173,\n",
       "  2.5995650768280028,\n",
       "  2.589065408706665,\n",
       "  2.881539058685303,\n",
       "  2.5945334434509277,\n",
       "  2.54888858795166,\n",
       "  2.471033239364624,\n",
       "  2.6065834045410154,\n",
       "  2.5873482704162596,\n",
       "  2.484507846832275,\n",
       "  2.5155307769775392,\n",
       "  2.814456510543823,\n",
       "  2.872935438156128,\n",
       "  2.6652788639068605,\n",
       "  2.6323956489562987,\n",
       "  2.5077961444854737,\n",
       "  2.568891143798828,\n",
       "  2.4876812934875487,\n",
       "  2.5577298164367677,\n",
       "  2.5291772365570067,\n",
       "  2.574669694900513,\n",
       "  2.508117198944092,\n",
       "  2.480362558364868,\n",
       "  2.586149501800537,\n",
       "  2.472712278366089,\n",
       "  2.525936651229858,\n",
       "  2.559228467941284,\n",
       "  2.5068581104278564,\n",
       "  2.5569265842437745,\n",
       "  2.503788137435913,\n",
       "  2.47505578994751,\n",
       "  2.539642810821533,\n",
       "  2.6448342323303224,\n",
       "  2.483127784729004,\n",
       "  2.4534996509552003,\n",
       "  2.4771714210510254,\n",
       "  2.5098477363586427,\n",
       "  2.5198625564575194,\n",
       "  2.5390913486480713,\n",
       "  2.5511306285858155,\n",
       "  2.522531032562256,\n",
       "  2.507946252822876,\n",
       "  2.4576941967010497,\n",
       "  2.5220545291900636,\n",
       "  2.5195803165435793,\n",
       "  2.563655233383179,\n",
       "  2.504466009140015,\n",
       "  2.4800558567047117,\n",
       "  2.445207405090332,\n",
       "  2.491571044921875,\n",
       "  2.4705668926239013,\n",
       "  2.438731050491333,\n",
       "  2.501927614212036,\n",
       "  2.4852859020233153,\n",
       "  2.531015968322754,\n",
       "  2.465428352355957,\n",
       "  2.522648334503174,\n",
       "  2.529360628128052,\n",
       "  2.5424917221069334,\n",
       "  2.4511404991149903,\n",
       "  2.5571922302246093,\n",
       "  2.8971831798553467,\n",
       "  2.443492031097412,\n",
       "  2.5554373264312744,\n",
       "  2.64711332321167,\n",
       "  2.581247901916504,\n",
       "  2.5800827026367186,\n",
       "  2.499482250213623,\n",
       "  2.44065465927124,\n",
       "  2.4432492733001707,\n",
       "  2.5578588962554933,\n",
       "  2.4934850215911863,\n",
       "  2.660824203491211,\n",
       "  2.6195404529571533,\n",
       "  2.555763101577759,\n",
       "  2.4587488174438477,\n",
       "  2.410166120529175,\n",
       "  2.415665054321289,\n",
       "  2.4712265491485597,\n",
       "  2.5215132236480713,\n",
       "  2.4256640434265138,\n",
       "  2.4142338275909423,\n",
       "  2.472202777862549,\n",
       "  2.5035507678985596,\n",
       "  2.5784244537353516,\n",
       "  2.596765327453613,\n",
       "  2.4910959720611574,\n",
       "  2.423662614822388,\n",
       "  2.6634223461151123,\n",
       "  2.5107328414916994,\n",
       "  2.4118322849273683,\n",
       "  2.416409158706665,\n",
       "  2.6146316528320312,\n",
       "  2.5208273410797117,\n",
       "  2.4662569999694823,\n",
       "  2.42862548828125,\n",
       "  2.414524459838867,\n",
       "  2.503551959991455,\n",
       "  2.5810564041137694,\n",
       "  2.483795499801636,\n",
       "  2.4868409633636475,\n",
       "  2.4751914501190186,\n",
       "  2.4898447036743163,\n",
       "  2.4746970653533937,\n",
       "  2.4119823932647706,\n",
       "  2.4507894039154055,\n",
       "  2.510388231277466,\n",
       "  2.648239755630493,\n",
       "  2.425645112991333,\n",
       "  2.3915051460266112,\n",
       "  2.4089982509613037,\n",
       "  2.5030354499816894,\n",
       "  2.532863140106201,\n",
       "  2.551781702041626,\n",
       "  2.664033222198486,\n",
       "  2.4280906677246095,\n",
       "  2.5839656352996827,\n",
       "  2.5252688407897947,\n",
       "  2.4611942291259767,\n",
       "  2.471232032775879,\n",
       "  2.4341111183166504,\n",
       "  2.683069372177124,\n",
       "  2.498497486114502,\n",
       "  2.4026256561279298,\n",
       "  2.408282232284546,\n",
       "  2.484701156616211,\n",
       "  2.4930521488189696,\n",
       "  2.3856116771697997,\n",
       "  2.473843240737915,\n",
       "  2.494823932647705,\n",
       "  2.724239158630371,\n",
       "  2.4487924575805664,\n",
       "  2.393205261230469,\n",
       "  2.4078672409057615,\n",
       "  2.5176294326782225,\n",
       "  2.433116054534912,\n",
       "  2.3935725688934326,\n",
       "  2.4402538776397704,\n",
       "  2.5294604301452637,\n",
       "  2.398391056060791,\n",
       "  2.5664868354797363,\n",
       "  2.5053308963775636,\n",
       "  2.513910245895386,\n",
       "  2.4489034175872804,\n",
       "  2.4588210582733154,\n",
       "  2.4440779209136965,\n",
       "  2.514966678619385,\n",
       "  2.468474292755127,\n",
       "  2.4840402126312258,\n",
       "  2.409223127365112,\n",
       "  2.4987868309020995,\n",
       "  2.5082207679748536,\n",
       "  2.444303512573242,\n",
       "  2.5429941177368165,\n",
       "  2.4871817588806153,\n",
       "  2.4333516120910645,\n",
       "  2.3640151977539063,\n",
       "  2.4042535781860352,\n",
       "  2.4708866119384765,\n",
       "  2.6698653221130373,\n",
       "  2.699976682662964,\n",
       "  2.458275079727173,\n",
       "  2.7052683353424074,\n",
       "  2.3689676761627196,\n",
       "  2.3439733505249025,\n",
       "  2.478426790237427,\n",
       "  2.4119295120239257,\n",
       "  2.578383684158325,\n",
       "  2.5204183578491213,\n",
       "  2.3507336616516112,\n",
       "  2.3523809909820557,\n",
       "  2.338548278808594,\n",
       "  2.3537190437316893,\n",
       "  2.5517748832702636,\n",
       "  2.3949103355407715,\n",
       "  2.3621041774749756,\n",
       "  2.436956691741943,\n",
       "  2.365316867828369,\n",
       "  2.3650091648101808,\n",
       "  2.4091391563415527,\n",
       "  2.3749014377593993,\n",
       "  2.3743780136108397,\n",
       "  2.4885396003723144,\n",
       "  2.559834289550781,\n",
       "  2.3302828788757326,\n",
       "  2.3749768257141115,\n",
       "  2.5808390617370605,\n",
       "  2.5379507064819338,\n",
       "  2.4072474002838136,\n",
       "  2.5401959896087645,\n",
       "  2.4282626152038573,\n",
       "  2.42882981300354,\n",
       "  2.748262071609497,\n",
       "  2.4532297611236573,\n",
       "  2.549126482009888,\n",
       "  2.4142503261566164,\n",
       "  2.497098922729492,\n",
       "  2.424721527099609,\n",
       "  2.462321949005127,\n",
       "  2.3447573661804197,\n",
       "  2.3364540576934814,\n",
       "  2.3958837032318114,\n",
       "  2.4913230419158934,\n",
       "  2.445748138427734,\n",
       "  2.3815062999725343,\n",
       "  2.330077886581421,\n",
       "  2.4148956775665282,\n",
       "  2.353369045257568,\n",
       "  2.3777488231658936,\n",
       "  2.5138444900512695,\n",
       "  2.5705137729644774,\n",
       "  2.4529241561889648,\n",
       "  2.418271780014038,\n",
       "  2.51004581451416,\n",
       "  2.394096279144287,\n",
       "  2.3684439659118652,\n",
       "  2.5248092651367187,\n",
       "  2.556274890899658,\n",
       "  2.461484098434448,\n",
       "  2.524029779434204,\n",
       "  2.328586196899414,\n",
       "  2.351661968231201,\n",
       "  2.49034161567688,\n",
       "  2.510493803024292,\n",
       "  2.507792568206787,\n",
       "  2.3479409217834473,\n",
       "  2.4192757606506348,\n",
       "  2.4209537506103516,\n",
       "  2.701622819900513,\n",
       "  2.455224847793579,\n",
       "  2.4081146717071533,\n",
       "  2.4933820724487306,\n",
       "  2.361834192276001,\n",
       "  2.5259405612945556,\n",
       "  2.531378078460693,\n",
       "  2.3507394790649414,\n",
       "  2.4287246227264405,\n",
       "  2.3259394645690916,\n",
       "  2.4596810340881348,\n",
       "  2.3811628341674806,\n",
       "  2.471228551864624,\n",
       "  2.425688123703003,\n",
       "  2.5728321075439453,\n",
       "  2.3414124488830566,\n",
       "  2.470691108703613,\n",
       "  2.310529327392578,\n",
       "  2.4037079334259035,\n",
       "  2.4022932052612305,\n",
       "  2.4102407455444337,\n",
       "  2.3518834590911863,\n",
       "  2.3930612564086915,\n",
       "  2.346933650970459,\n",
       "  2.288822078704834,\n",
       "  2.2981497764587404,\n",
       "  2.4106765270233153,\n",
       "  2.4556362628936768,\n",
       "  2.3384085655212403,\n",
       "  2.411354970932007,\n",
       "  2.3863742351531982,\n",
       "  2.490816926956177,\n",
       "  2.4318942070007323,\n",
       "  2.3094514846801757,\n",
       "  2.3662542343139648,\n",
       "  2.362803840637207,\n",
       "  2.3808953762054443,\n",
       "  2.3035488605499266,\n",
       "  2.315264272689819,\n",
       "  2.3113728046417235,\n",
       "  2.4099111557006836,\n",
       "  2.4682270526885985,\n",
       "  2.3745937824249266,\n",
       "  2.4858452320098876,\n",
       "  2.443857955932617,\n",
       "  2.5880151271820067,\n",
       "  2.4486663341522217,\n",
       "  2.3902413845062256,\n",
       "  2.452300500869751,\n",
       "  2.538400936126709,\n",
       "  2.3014174938201903,\n",
       "  2.3067086696624757,\n",
       "  2.3378714084625245,\n",
       "  2.4117417335510254,\n",
       "  2.4277597427368165,\n",
       "  2.3262451171875,\n",
       "  2.3265340328216553,\n",
       "  2.4166939735412596,\n",
       "  2.3497385501861574,\n",
       "  2.3513018131256103,\n",
       "  2.3726786613464355,\n",
       "  2.3495833396911623,\n",
       "  2.4252166748046875,\n",
       "  2.548655796051025,\n",
       "  2.361916017532349,\n",
       "  2.462974691390991,\n",
       "  2.3933248996734617,\n",
       "  2.6874627590179445,\n",
       "  2.4717685699462892,\n",
       "  2.304613494873047,\n",
       "  2.305669832229614,\n",
       "  2.5242614269256594,\n",
       "  2.5425384521484373,\n",
       "  2.2854065895080566,\n",
       "  2.4438570022583006,\n",
       "  2.314432716369629,\n",
       "  2.3775755882263185,\n",
       "  2.4275065422058106,\n",
       "  2.3689057350158693,\n",
       "  2.336532735824585,\n",
       "  2.322990560531616,\n",
       "  2.5387124538421633,\n",
       "  2.292183256149292,\n",
       "  2.349170112609863,\n",
       "  2.4483636379241944,\n",
       "  2.3805391788482666,\n",
       "  2.447931003570557,\n",
       "  2.467680263519287,\n",
       "  2.4704853534698485,\n",
       "  2.4313822269439695,\n",
       "  2.394376611709595,\n",
       "  2.4004656791687013,\n",
       "  2.3800582885742188,\n",
       "  2.398896646499634,\n",
       "  2.4125503063201905,\n",
       "  2.5137304306030273,\n",
       "  2.4323801040649413,\n",
       "  2.4385870933532714,\n",
       "  2.283364772796631,\n",
       "  2.2881753921508787,\n",
       "  2.399025821685791,\n",
       "  2.3773900985717775,\n",
       "  2.3023187160491942,\n",
       "  2.475664663314819,\n",
       "  2.5813969135284425,\n",
       "  2.394339895248413,\n",
       "  2.3381452560424805,\n",
       "  2.314003896713257,\n",
       "  2.269122314453125,\n",
       "  2.3306007862091063]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyVal = []\n",
    "historyTr = []\n",
    "\n",
    "mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_modelLC()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=1036).history\n",
    "    historyVal.append(history['val_loss'])\n",
    "    historyTr.append(history['loss'])\n",
    "model=create_modelLC()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=600, \n",
    "                      batch_size=1036, callbacks=[mc]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_mean=np.mean(historyVal, axis=0)\n",
    "historyTr_mean=np.mean(historyTr, axis=0)\n",
    "\n",
    "historyVal_sd=np.std(historyVal, axis=0)\n",
    "historyTr_sd=np.std(historyTr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_mean)),historyTr_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_mean)), historyTr_mean - historyTr_sd, historyTr_mean + historyTr_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_mean)), historyVal_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_mean)), historyVal_mean - historyVal_sd, \n",
    "                 historyVal_mean + historyVal_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.title('Learning curves with best parameters', fontsize = 18)\n",
    "plt.legend()\n",
    "plt.ylim(5,20)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 21, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model result:\n",
      "MEE on the validation 2.8851062297821044 with standard deviation 0.14524664445320093\n",
      "MEE on the training 2.5144044876098635 with standard deviation 0.03642023272310837\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model result:\")\n",
    "print(\"MEE on the validation\",historyVal_mean[-1],\"with standard deviation\",historyVal_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_mean[-1],\"with standard deviation\",historyTr_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_NoReg():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    ))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    ))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=mlpr.best_params_['lr'], \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 325us/step - loss: 56.4381 - val_loss: 54.2541\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 54.0474 - val_loss: 50.3602\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 50.1549 - val_loss: 44.6226\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 44.4228 - val_loss: 36.5713\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 36.3975 - val_loss: 27.2096\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 27.1435 - val_loss: 20.3356\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 20.3264 - val_loss: 17.5408\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 17.6071 - val_loss: 16.5672\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 16.6555 - val_loss: 16.3364\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 16.4224 - val_loss: 16.2202\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 16.3054 - val_loss: 15.9416\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.0291 - val_loss: 15.4637\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 15.5546 - val_loss: 14.7964\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.8890 - val_loss: 13.9061\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.9952 - val_loss: 12.7818\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.8533 - val_loss: 11.5921\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.5903 - val_loss: 10.4427\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 10.3899 - val_loss: 9.4450\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 9.4107 - val_loss: 8.9086\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.8718 - val_loss: 8.7415\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.6980 - val_loss: 8.6834\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 8.6284 - val_loss: 8.6320\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.5794 - val_loss: 8.6007\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.5543 - val_loss: 8.5668\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.5239 - val_loss: 8.5255\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.4888 - val_loss: 8.4809\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.4548 - val_loss: 8.4419\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 8.4227 - val_loss: 8.4058\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.3905 - val_loss: 8.3714\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.3586 - val_loss: 8.3363\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.3256 - val_loss: 8.3005\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2907 - val_loss: 8.2632\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.2535 - val_loss: 8.2234\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2134 - val_loss: 8.1803\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1699 - val_loss: 8.1334\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1221 - val_loss: 8.0817\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0692 - val_loss: 8.0244\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0105 - val_loss: 7.9608\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.9451 - val_loss: 7.8903\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8724 - val_loss: 7.8126\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.7917 - val_loss: 7.7274\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.7029 - val_loss: 7.6346\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.6060 - val_loss: 7.5347\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.5011 - val_loss: 7.4288\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.3886 - val_loss: 7.3181\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.2690 - val_loss: 7.2022\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.1432 - val_loss: 7.0811\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.0122 - val_loss: 6.9545\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.8767 - val_loss: 6.8230\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.7375 - val_loss: 6.6870\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.5951 - val_loss: 6.5468\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.4496 - val_loss: 6.4027\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.3010 - val_loss: 6.2542\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.1489 - val_loss: 6.1018\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 5.9927 - val_loss: 5.9440\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.8322 - val_loss: 5.7823\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.6672 - val_loss: 5.6148\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.4985 - val_loss: 5.4483\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.3265 - val_loss: 5.2705\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.1525 - val_loss: 5.1149\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.9796 - val_loss: 4.9360\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.8172 - val_loss: 4.8605\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.6923 - val_loss: 4.7879\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.6951 - val_loss: 5.2265\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.0339 - val_loss: 5.4263\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.3827 - val_loss: 5.5909\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.3932 - val_loss: 4.6390\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.4877 - val_loss: 4.3430\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.1075 - val_loss: 4.1967\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9870 - val_loss: 4.1612\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9183 - val_loss: 4.0885\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.8926 - val_loss: 4.2305\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.9752 - val_loss: 4.3881\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.2604 - val_loss: 4.8564\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.6238 - val_loss: 4.5778\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.4529 - val_loss: 4.5108\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.2718 - val_loss: 4.1909\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0314 - val_loss: 4.2185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9668 - val_loss: 4.1076\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9448 - val_loss: 4.2290\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9950 - val_loss: 4.1316\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9813 - val_loss: 4.2448\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0215 - val_loss: 4.1014\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.9410 - val_loss: 4.1363\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.9171 - val_loss: 3.9970\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8241 - val_loss: 4.0468\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8324 - val_loss: 3.9619\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7838 - val_loss: 4.0107\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8046 - val_loss: 3.9322\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7480 - val_loss: 3.9531\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7528 - val_loss: 3.8857\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6929 - val_loss: 3.8907\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6956 - val_loss: 3.8525\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6531 - val_loss: 3.8456\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6569 - val_loss: 3.8229\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6169 - val_loss: 3.7967\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.6129 - val_loss: 3.7858\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5727 - val_loss: 3.7451\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5642 - val_loss: 3.7495\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5289 - val_loss: 3.6990\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5194 - val_loss: 3.7152\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4876 - val_loss: 3.6531\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4732 - val_loss: 3.6828\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4486 - val_loss: 3.6105\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4299 - val_loss: 3.6567\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4168 - val_loss: 3.5723\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3890 - val_loss: 3.6241\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3777 - val_loss: 3.5403\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3522 - val_loss: 3.6080\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3566 - val_loss: 3.5269\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3390 - val_loss: 3.6013\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3463 - val_loss: 3.5127\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3253 - val_loss: 3.5869\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3284 - val_loss: 3.4917\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3025 - val_loss: 3.5666\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3047 - val_loss: 3.4641\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2700 - val_loss: 3.5424\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2768 - val_loss: 3.4451\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2482 - val_loss: 3.5267\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2575 - val_loss: 3.4314\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2336 - val_loss: 3.5157\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2433 - val_loss: 3.4193\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2212 - val_loss: 3.5029\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2273 - val_loss: 3.4055\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2057 - val_loss: 3.4888\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2101 - val_loss: 3.3924\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1908 - val_loss: 3.4776\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1962 - val_loss: 3.3768\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1723 - val_loss: 3.4628\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1777 - val_loss: 3.3639\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1567 - val_loss: 3.4522\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1654 - val_loss: 3.3558\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1484 - val_loss: 3.4472\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1592 - val_loss: 3.3493\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1425 - val_loss: 3.4425\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1531 - val_loss: 3.3421\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1353 - val_loss: 3.4362\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1456 - val_loss: 3.3343\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1269 - val_loss: 3.4293\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1376 - val_loss: 3.3269\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1190 - val_loss: 3.4236\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1308 - val_loss: 3.3204\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.1127 - val_loss: 3.4190\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1253 - val_loss: 3.3145\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1072 - val_loss: 3.4144\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1199 - val_loss: 3.3086\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1015 - val_loss: 3.4094\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1141 - val_loss: 3.3024\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0954 - val_loss: 3.4039\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1078 - val_loss: 3.2961\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0891 - val_loss: 3.3979\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1010 - val_loss: 3.2896\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0824 - val_loss: 3.3914\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0937 - val_loss: 3.2831\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0757 - val_loss: 3.3852\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0868 - val_loss: 3.2770\n",
      "Epoch 157/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0697 - val_loss: 3.3799\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0809 - val_loss: 3.2717\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0648 - val_loss: 3.3749\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0756 - val_loss: 3.2664\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0597 - val_loss: 3.3693\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0695 - val_loss: 3.2605\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0533 - val_loss: 3.3631\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0625 - val_loss: 3.2546\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0470 - val_loss: 3.3577\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0563 - val_loss: 3.2488\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0408 - val_loss: 3.3517\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0498 - val_loss: 3.2430\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0347 - val_loss: 3.3463\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0439 - val_loss: 3.2379\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0298 - val_loss: 3.3420\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0393 - val_loss: 3.2329\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0250 - val_loss: 3.3370\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0340 - val_loss: 3.2273\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0193 - val_loss: 3.3316\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0282 - val_loss: 3.2218\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0137 - val_loss: 3.3263\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0227 - val_loss: 3.2164\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0083 - val_loss: 3.3207\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0170 - val_loss: 3.2108\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0026 - val_loss: 3.3149\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.0110 - val_loss: 3.2052\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.9968 - val_loss: 3.3093\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0052 - val_loss: 3.1998\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9913 - val_loss: 3.3038\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9997 - val_loss: 3.1945\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9860 - val_loss: 3.2984\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9943 - val_loss: 3.1893\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9808 - val_loss: 3.2931\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9890 - val_loss: 3.1841\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9756 - val_loss: 3.2877\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9839 - val_loss: 3.1790\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9706 - val_loss: 3.2825\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9788 - val_loss: 3.1740\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9657 - val_loss: 3.2774\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9739 - val_loss: 3.1692\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9610 - val_loss: 3.2725\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9692 - val_loss: 3.1645\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9565 - val_loss: 3.2677\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9647 - val_loss: 3.1601\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9523 - val_loss: 3.2631\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.9604 - val_loss: 3.1561\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9481 - val_loss: 3.2587\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9562 - val_loss: 3.1521\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9441 - val_loss: 3.2543\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9522 - val_loss: 3.1484\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.9403 - val_loss: 3.2501\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9483 - val_loss: 3.1434\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9304 - val_loss: 3.2424\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9389 - val_loss: 3.1381\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9223 - val_loss: 3.2365\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9329 - val_loss: 3.1337\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9174 - val_loss: 3.2323\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9292 - val_loss: 3.1301\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9140 - val_loss: 3.2286\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9260 - val_loss: 3.1266\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9105 - val_loss: 3.2249\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9227 - val_loss: 3.1229\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9066 - val_loss: 3.2205\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9186 - val_loss: 3.1190\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9020 - val_loss: 3.2156\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9136 - val_loss: 3.1150\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8968 - val_loss: 3.2103\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9080 - val_loss: 3.1111\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8917 - val_loss: 3.2049\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9023 - val_loss: 3.1070\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8868 - val_loss: 3.1983\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8954 - val_loss: 3.1022\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8810 - val_loss: 3.1904\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8869 - val_loss: 3.0972\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8744 - val_loss: 3.1835\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8794 - val_loss: 3.0931\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8694 - val_loss: 3.1791\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8751 - val_loss: 3.0901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8670 - val_loss: 3.1769\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8734 - val_loss: 3.0877\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8659 - val_loss: 3.1752\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8727 - val_loss: 3.0853\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8649 - val_loss: 3.1731\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8714 - val_loss: 3.0827\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8631 - val_loss: 3.1703\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8692 - val_loss: 3.0798\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8606 - val_loss: 3.1670\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8662 - val_loss: 3.0769\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8575 - val_loss: 3.1633\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8625 - val_loss: 3.0739\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.8542 - val_loss: 3.1593\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.8586 - val_loss: 3.0709\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8507 - val_loss: 3.1553\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.8546 - val_loss: 3.0678\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8472 - val_loss: 3.1512\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8506 - val_loss: 3.0647\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8438 - val_loss: 3.1473\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8467 - val_loss: 3.0618\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8407 - val_loss: 3.1438\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8434 - val_loss: 3.0591\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8381 - val_loss: 3.1409\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8408 - val_loss: 3.0567\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8360 - val_loss: 3.1384\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8387 - val_loss: 3.0544\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8341 - val_loss: 3.1360\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8368 - val_loss: 3.0521\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8322 - val_loss: 3.1334\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8346 - val_loss: 3.0495\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8298 - val_loss: 3.1304\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8318 - val_loss: 3.0468\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8271 - val_loss: 3.1270\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8287 - val_loss: 3.0441\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8242 - val_loss: 3.1237\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8253 - val_loss: 3.0414\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8213 - val_loss: 3.1204\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.8221 - val_loss: 3.0388\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.8185 - val_loss: 3.1173\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8190 - val_loss: 3.0364\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.8159 - val_loss: 3.1157\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8174 - val_loss: 3.0354\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8150 - val_loss: 3.1123\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8142 - val_loss: 3.0315\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8105 - val_loss: 3.1073\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8085 - val_loss: 3.0292\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8075 - val_loss: 3.1041\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8054 - val_loss: 3.0273\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8057 - val_loss: 3.1045\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8060 - val_loss: 3.0271\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 2.8065 - val_loss: 3.1071\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8095 - val_loss: 3.0267\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8072 - val_loss: 3.1065\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 2.8089 - val_loss: 3.0242\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8038 - val_loss: 3.1002\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8013 - val_loss: 3.0195\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7954 - val_loss: 3.0891\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7874 - val_loss: 3.0116\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7802 - val_loss: 3.0780\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7737 - val_loss: 3.0050\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7664 - val_loss: 3.0686\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7626 - val_loss: 3.0004\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7555 - val_loss: 3.0593\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7526 - val_loss: 2.9974\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7440 - val_loss: 3.0487\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7424 - val_loss: 3.0004\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.7336 - val_loss: 3.0395\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7384 - val_loss: 3.0092\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7320 - val_loss: 3.0355\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7460 - val_loss: 3.0238\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7403 - val_loss: 3.0346\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7605 - val_loss: 3.0393\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7538 - val_loss: 3.0384\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7747 - val_loss: 3.0508\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7648 - val_loss: 3.0368\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7792 - val_loss: 3.0530\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7663 - val_loss: 3.0269\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7712 - val_loss: 3.0416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7515 - val_loss: 3.0085\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7486 - val_loss: 3.0246\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7292 - val_loss: 2.9963\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7260 - val_loss: 3.0043\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7047 - val_loss: 2.9926\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7080 - val_loss: 2.9932\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6962 - val_loss: 3.0038\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.7093 - val_loss: 2.9941\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7049 - val_loss: 3.0184\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7171 - val_loss: 2.9947\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7122 - val_loss: 3.0254\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7206 - val_loss: 2.9897\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7123 - val_loss: 3.0297\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7204 - val_loss: 2.9877\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7182 - val_loss: 3.0410\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7290 - val_loss: 2.9891\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7329 - val_loss: 3.0603\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7503 - val_loss: 2.9977\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7627 - val_loss: 3.0916\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7902 - val_loss: 3.0184\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8073 - val_loss: 3.1249\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8348 - val_loss: 3.0262\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8283 - val_loss: 3.1272\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.8435 - val_loss: 3.0235\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8284 - val_loss: 3.1183\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8370 - val_loss: 3.0133\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8117 - val_loss: 3.0973\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8127 - val_loss: 3.0000\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7899 - val_loss: 3.0808\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7918 - val_loss: 2.9924\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7761 - val_loss: 3.0666\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7726 - val_loss: 2.9866\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7564 - val_loss: 3.0578\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7592 - val_loss: 2.9824\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7446 - val_loss: 3.0460\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7444 - val_loss: 2.9743\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7302 - val_loss: 3.0381\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7344 - val_loss: 2.9719\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7263 - val_loss: 3.0383\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7343 - val_loss: 2.9706\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7265 - val_loss: 3.0383\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7341 - val_loss: 2.9702\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7288 - val_loss: 3.0415\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7380 - val_loss: 2.9719\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7345 - val_loss: 3.0463\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7441 - val_loss: 2.9737\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7396 - val_loss: 3.0490\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7478 - val_loss: 2.9736\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7414 - val_loss: 3.0484\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7479 - val_loss: 2.9721\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7403 - val_loss: 3.0467\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7462 - val_loss: 2.9704\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7383 - val_loss: 3.0446\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7440 - val_loss: 2.9687\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7356 - val_loss: 3.0420\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7409 - val_loss: 2.9665\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.7319 - val_loss: 3.0388\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7371 - val_loss: 2.9642\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7278 - val_loss: 3.0356\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7332 - val_loss: 2.9621\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7239 - val_loss: 3.0328\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7298 - val_loss: 2.9602\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7206 - val_loss: 3.0305\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7269 - val_loss: 2.9586\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7178 - val_loss: 3.0287\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 2.7246 - val_loss: 2.9573\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.7157 - val_loss: 3.0273\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7229 - val_loss: 2.9562\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7142 - val_loss: 3.0264\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7218 - val_loss: 2.9553\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7133 - val_loss: 3.0257\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7210 - val_loss: 2.9545\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7126 - val_loss: 3.0252\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7205 - val_loss: 2.9538\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7121 - val_loss: 3.0248\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7200 - val_loss: 2.9532\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7117 - val_loss: 3.0243\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7195 - val_loss: 2.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7113 - val_loss: 3.0245\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7201 - val_loss: 2.9523\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7124 - val_loss: 3.0263\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7225 - val_loss: 2.9529\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7150 - val_loss: 3.0273\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7240 - val_loss: 2.9534\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7167 - val_loss: 3.0254\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7223 - val_loss: 2.9520\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7154 - val_loss: 3.0241\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7212 - val_loss: 2.9514\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7152 - val_loss: 3.0246\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7232 - val_loss: 2.9549\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7214 - val_loss: 3.0311\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7328 - val_loss: 2.9636\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7339 - val_loss: 3.0420\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7488 - val_loss: 2.9814\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7538 - val_loss: 3.0426\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7514 - val_loss: 2.9780\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7377 - val_loss: 3.0200\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7273 - val_loss: 2.9816\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7198 - val_loss: 2.9828\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.6883 - val_loss: 2.9591\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6685 - val_loss: 2.9424\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6444 - val_loss: 2.9631\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6520 - val_loss: 2.9473\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6513 - val_loss: 2.9797\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6652 - val_loss: 2.9485\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6573 - val_loss: 2.9781\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6640 - val_loss: 2.9439\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6526 - val_loss: 2.9741\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6600 - val_loss: 2.9432\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6501 - val_loss: 2.9729\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6590 - val_loss: 2.9441\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6497 - val_loss: 2.9726\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6589 - val_loss: 2.9444\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6487 - val_loss: 2.9714\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6578 - val_loss: 2.9443\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.6472 - val_loss: 2.9700\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6569 - val_loss: 2.9444\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6458 - val_loss: 2.9675\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6555 - val_loss: 2.9447\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6440 - val_loss: 2.9640\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6529 - val_loss: 2.9448\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6419 - val_loss: 2.9612\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6511 - val_loss: 2.9454\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6408 - val_loss: 2.9595\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6505 - val_loss: 2.9459\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6398 - val_loss: 2.9577\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6489 - val_loss: 2.9454\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6379 - val_loss: 2.9564\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6471 - val_loss: 2.9446\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6363 - val_loss: 2.9560\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6459 - val_loss: 2.9436\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6352 - val_loss: 2.9561\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6449 - val_loss: 2.9420\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6339 - val_loss: 2.9556\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6433 - val_loss: 2.9401\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6324 - val_loss: 2.9545\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6408 - val_loss: 2.9377\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6306 - val_loss: 2.9561\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6413 - val_loss: 2.9367\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6311 - val_loss: 2.9574\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6415 - val_loss: 2.9346\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6308 - val_loss: 2.9581\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.6403 - val_loss: 2.9312\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6307 - val_loss: 2.9598\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6399 - val_loss: 2.9293\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6346 - val_loss: 2.9649\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6446 - val_loss: 2.9278\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6402 - val_loss: 2.9692\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6492 - val_loss: 2.9260\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6485 - val_loss: 2.9793\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6602 - val_loss: 2.9270\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6597 - val_loss: 2.9938\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6760 - val_loss: 2.9342\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6763 - val_loss: 3.0154\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7001 - val_loss: 2.9442\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.6949 - val_loss: 3.0339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7210 - val_loss: 2.9562\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7162 - val_loss: 3.0517\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7420 - val_loss: 2.9730\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7440 - val_loss: 3.0742\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7716 - val_loss: 2.9910\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7678 - val_loss: 3.0703\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7682 - val_loss: 2.9794\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7487 - val_loss: 3.0413\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7317 - val_loss: 2.9526\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7076 - val_loss: 3.0089\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6893 - val_loss: 2.9234\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6582 - val_loss: 2.9790\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6569 - val_loss: 2.9269\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6497 - val_loss: 2.9729\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6566 - val_loss: 2.9241\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6331 - val_loss: 2.9469\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6271 - val_loss: 2.9180\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6141 - val_loss: 2.9437\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6224 - val_loss: 2.9255\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6185 - val_loss: 2.9492\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6297 - val_loss: 2.9283\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6200 - val_loss: 2.9465\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6269 - val_loss: 2.9267\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6157 - val_loss: 2.9442\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6239 - val_loss: 2.9274\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6146 - val_loss: 2.9448\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6247 - val_loss: 2.9283\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6142 - val_loss: 2.9435\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6237 - val_loss: 2.9281\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6123 - val_loss: 2.9410\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6215 - val_loss: 2.9283\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6105 - val_loss: 2.9393\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6200 - val_loss: 2.9288\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6092 - val_loss: 2.9379\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6187 - val_loss: 2.9288\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6078 - val_loss: 2.9367\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6171 - val_loss: 2.9283\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6064 - val_loss: 2.9360\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6156 - val_loss: 2.9275\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6052 - val_loss: 2.9356\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6143 - val_loss: 2.9264\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6041 - val_loss: 2.9355\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6131 - val_loss: 2.9250\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6032 - val_loss: 2.9357\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6121 - val_loss: 2.9235\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6025 - val_loss: 2.9362\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6115 - val_loss: 2.9219\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6021 - val_loss: 2.9375\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6111 - val_loss: 2.9197\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6021 - val_loss: 2.9390\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6103 - val_loss: 2.9162\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6028 - val_loss: 2.9430\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6121 - val_loss: 2.9143\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6088 - val_loss: 2.9522\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6202 - val_loss: 2.9135\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6209 - val_loss: 2.9665\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6348 - val_loss: 2.9175\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6385 - val_loss: 2.9863\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.6562 - val_loss: 2.9277\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6595 - val_loss: 3.0109\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6831 - val_loss: 2.9408\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6822 - val_loss: 3.0270\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7003 - val_loss: 2.9550\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7056 - val_loss: 3.0518\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7299 - val_loss: 2.9771\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7367 - val_loss: 3.0725\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7562 - val_loss: 2.9906\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7564 - val_loss: 3.0627\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7437 - val_loss: 2.9695\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7232 - val_loss: 3.0294\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7021 - val_loss: 2.9290\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6610 - val_loss: 2.9699\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6283 - val_loss: 2.8988\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6017 - val_loss: 2.9502\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6148 - val_loss: 2.9203\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6172 - val_loss: 2.9488\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6204 - val_loss: 2.9095\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5969 - val_loss: 2.9303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5945 - val_loss: 2.9061\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5912 - val_loss: 2.9384\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6034 - val_loss: 2.9122\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6007 - val_loss: 2.9424\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6078 - val_loss: 2.9087\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5982 - val_loss: 2.9395\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6027 - val_loss: 2.9064\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5968 - val_loss: 2.9421\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6043 - val_loss: 2.9071\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5988 - val_loss: 2.9439\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6055 - val_loss: 2.9065\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5984 - val_loss: 2.9435\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6042 - val_loss: 2.9058\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5973 - val_loss: 2.9436\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6035 - val_loss: 2.9058\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5967 - val_loss: 2.9436\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6028 - val_loss: 2.9057\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5957 - val_loss: 2.9433\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6017 - val_loss: 2.9055\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5946 - val_loss: 2.9429\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6006 - val_loss: 2.9054\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5935 - val_loss: 2.9425\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5995 - val_loss: 2.9053\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5924 - val_loss: 2.9420\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5983 - val_loss: 2.9052\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5912 - val_loss: 2.9414\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5970 - val_loss: 2.9052\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5898 - val_loss: 2.9406\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5957 - val_loss: 2.9053\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5884 - val_loss: 2.9395\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5941 - val_loss: 2.9055\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5866 - val_loss: 2.9380\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5923 - val_loss: 2.9062\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5845 - val_loss: 2.9355\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5899 - val_loss: 2.9076\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5814 - val_loss: 2.9317\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5875 - val_loss: 2.9118\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5785 - val_loss: 2.9285\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5896 - val_loss: 2.9240\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5822 - val_loss: 2.9304\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6032 - val_loss: 2.9453\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5967 - val_loss: 2.9447\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6364 - val_loss: 3.0144\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6718 - val_loss: 3.0248\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7644 - val_loss: 3.0803\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7511 - val_loss: 2.9863\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7162 - val_loss: 2.9982\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6503 - val_loss: 2.9069\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6043 - val_loss: 2.9351\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5700 - val_loss: 2.8812\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5513 - val_loss: 2.8995\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5311 - val_loss: 2.8803\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5330 - val_loss: 2.8940\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5356 - val_loss: 2.9002\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 424us/step - loss: 56.3789 - val_loss: 51.8816\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 53.8285 - val_loss: 47.4921\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 49.4389 - val_loss: 40.8497\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 42.8016 - val_loss: 32.1055\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 34.0436 - val_loss: 23.1627\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 24.9888 - val_loss: 18.1191\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 19.7121 - val_loss: 16.4059\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 17.5359 - val_loss: 16.0231\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 16.7454 - val_loss: 16.0653\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 16.5564 - val_loss: 16.0250\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 16.4411 - val_loss: 15.7051\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.1596 - val_loss: 15.1164\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.6765 - val_loss: 14.2587\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 14.9591 - val_loss: 13.0747\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.9290 - val_loss: 11.6433\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.6271 - val_loss: 10.3499\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.3385 - val_loss: 9.3324\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.1694 - val_loss: 8.5511\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.2977 - val_loss: 8.2292\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.9479 - val_loss: 8.1718\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8390 - val_loss: 8.1356\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7841 - val_loss: 8.0978\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7488 - val_loss: 8.0660\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7269 - val_loss: 8.0211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.6921 - val_loss: 7.9745\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6588 - val_loss: 7.9328\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.6302 - val_loss: 7.8946\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6026 - val_loss: 7.8592\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5756 - val_loss: 7.8256\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5486 - val_loss: 7.7921\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5201 - val_loss: 7.7583\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4898 - val_loss: 7.7238\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4574 - val_loss: 7.6870\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4222 - val_loss: 7.6475\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3834 - val_loss: 7.6042\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3401 - val_loss: 7.5562\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2914 - val_loss: 7.5029\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2364 - val_loss: 7.4436\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1743 - val_loss: 7.3778\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1044 - val_loss: 7.3047\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0259 - val_loss: 7.2240\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9389 - val_loss: 7.1355\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8432 - val_loss: 7.0409\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.7392 - val_loss: 6.9400\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.6282 - val_loss: 6.8356\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.5115 - val_loss: 6.7265\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 7.3904 - val_loss: 6.6186\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.2661 - val_loss: 6.5065\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 7.1394 - val_loss: 6.3999\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.0112 - val_loss: 6.2870\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 6.8820 - val_loss: 6.1833\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 6.7520 - val_loss: 6.0702\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.6210 - val_loss: 5.9672\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 6.4886 - val_loss: 5.8534\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 6.3541 - val_loss: 5.7466\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.2166 - val_loss: 5.6274\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 6.0754 - val_loss: 5.5133\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.9297 - val_loss: 5.3852\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.7793 - val_loss: 5.2615\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.6239 - val_loss: 5.1212\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.4643 - val_loss: 4.9920\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.3010 - val_loss: 4.8376\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 5.1364 - val_loss: 4.7225\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.9730 - val_loss: 4.5569\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.8170 - val_loss: 4.4868\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.6747 - val_loss: 4.3249\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.5541 - val_loss: 4.3543\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.4704 - val_loss: 4.2217\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.4506 - val_loss: 4.5034\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.5446 - val_loss: 4.4818\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.7386 - val_loss: 4.9299\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.8985 - val_loss: 4.3974\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.6188 - val_loss: 4.3835\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.3079 - val_loss: 3.9929\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.0843 - val_loss: 4.0120\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.9662 - val_loss: 3.8628\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8914 - val_loss: 3.8997\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8475 - val_loss: 3.8113\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8516 - val_loss: 4.0649\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.9769 - val_loss: 4.1161\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.2115 - val_loss: 4.7959\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.6700 - val_loss: 4.3515\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.4686 - val_loss: 4.4176\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.2676 - val_loss: 3.9511\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9795 - val_loss: 4.0258\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8601 - val_loss: 3.7573\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.7522 - val_loss: 3.8931\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7472 - val_loss: 3.7744\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7841 - val_loss: 4.0979\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.9460 - val_loss: 4.0002\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0515 - val_loss: 4.2635\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.0940 - val_loss: 3.9200\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9462 - val_loss: 4.0387\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8464 - val_loss: 3.7567\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7496 - val_loss: 3.9177\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.7229 - val_loss: 3.7200\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7151 - val_loss: 3.9359\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7409 - val_loss: 3.7493\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7520 - val_loss: 3.9513\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7445 - val_loss: 3.7279\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7272 - val_loss: 3.8982\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6807 - val_loss: 3.6540\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6349 - val_loss: 3.8213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5954 - val_loss: 3.6074\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5809 - val_loss: 3.7953\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5636 - val_loss: 3.5980\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5684 - val_loss: 3.7874\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5493 - val_loss: 3.5886\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5536 - val_loss: 3.7672\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5213 - val_loss: 3.5662\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5226 - val_loss: 3.7396\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4872 - val_loss: 3.5456\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4923 - val_loss: 3.7186\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4597 - val_loss: 3.5276\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4656 - val_loss: 3.7001\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4341 - val_loss: 3.5039\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4310 - val_loss: 3.6775\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4047 - val_loss: 3.4855\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4030 - val_loss: 3.6598\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3815 - val_loss: 3.4691\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3788 - val_loss: 3.6411\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.3577 - val_loss: 3.4507\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3500 - val_loss: 3.6209\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3321 - val_loss: 3.4353\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3241 - val_loss: 3.6051\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3113 - val_loss: 3.4256\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3050 - val_loss: 3.5908\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2928 - val_loss: 3.4147\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2867 - val_loss: 3.5731\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2710 - val_loss: 3.3995\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2628 - val_loss: 3.5512\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2457 - val_loss: 3.3845\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2371 - val_loss: 3.5277\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2186 - val_loss: 3.3684\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.2116 - val_loss: 3.4960\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1838 - val_loss: 3.3431\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1749 - val_loss: 3.4700\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1552 - val_loss: 3.3299\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1525 - val_loss: 3.4625\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1443 - val_loss: 3.3302\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1494 - val_loss: 3.4644\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1429 - val_loss: 3.3325\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1503 - val_loss: 3.4649\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1402 - val_loss: 3.3323\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1479 - val_loss: 3.4632\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1352 - val_loss: 3.3300\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1434 - val_loss: 3.4610\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1297 - val_loss: 3.3273\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1359 - val_loss: 3.4541\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1203 - val_loss: 3.3217\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1255 - val_loss: 3.4470\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1101 - val_loss: 3.3150\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1145 - val_loss: 3.4403\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1001 - val_loss: 3.3098\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1046 - val_loss: 3.4344\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0915 - val_loss: 3.3062\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0969 - val_loss: 3.4302\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0846 - val_loss: 3.3039\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0912 - val_loss: 3.4270\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0789 - val_loss: 3.3021\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0860 - val_loss: 3.4311\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0806 - val_loss: 3.3124\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0944 - val_loss: 3.4370\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0874 - val_loss: 3.3165\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0976 - val_loss: 3.4325\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0806 - val_loss: 3.3072\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0832 - val_loss: 3.4214\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0639 - val_loss: 3.2944\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0640 - val_loss: 3.4098\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0459 - val_loss: 3.2815\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0438 - val_loss: 3.3924\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0240 - val_loss: 3.2620\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0155 - val_loss: 3.3754\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0014 - val_loss: 3.2500\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9957 - val_loss: 3.3663\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9885 - val_loss: 3.2450\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9875 - val_loss: 3.3661\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9847 - val_loss: 3.2465\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9880 - val_loss: 3.3710\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9871 - val_loss: 3.2585\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0002 - val_loss: 3.3902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0085 - val_loss: 3.2907\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0384 - val_loss: 3.4245\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0480 - val_loss: 3.3164\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0702 - val_loss: 3.4293\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0492 - val_loss: 3.2995\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0473 - val_loss: 3.4002\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0121 - val_loss: 3.2682\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0029 - val_loss: 3.3729\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9771 - val_loss: 3.2441\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9657 - val_loss: 3.3464\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9399 - val_loss: 3.2147\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9263 - val_loss: 3.3270\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9060 - val_loss: 3.1944\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8972 - val_loss: 3.3127\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8837 - val_loss: 3.1874\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8826 - val_loss: 3.3144\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8770 - val_loss: 3.1914\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8811 - val_loss: 3.3215\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8798 - val_loss: 3.1958\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8863 - val_loss: 3.3299\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8880 - val_loss: 3.2053\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8987 - val_loss: 3.3486\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9069 - val_loss: 3.2266\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9281 - val_loss: 3.3792\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9441 - val_loss: 3.2744\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9902 - val_loss: 3.4361\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0245 - val_loss: 3.3660\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0962 - val_loss: 3.4756\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0859 - val_loss: 3.3875\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1170 - val_loss: 3.4251\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0259 - val_loss: 3.2718\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9745 - val_loss: 3.3461\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9016 - val_loss: 3.2007\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8798 - val_loss: 3.3218\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8530 - val_loss: 3.1744\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8426 - val_loss: 3.3026\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8272 - val_loss: 3.1698\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8188 - val_loss: 3.2881\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8091 - val_loss: 3.1681\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8078 - val_loss: 3.2874\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8053 - val_loss: 3.1675\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8086 - val_loss: 3.2965\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8088 - val_loss: 3.1692\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8144 - val_loss: 3.3050\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8131 - val_loss: 3.1720\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8208 - val_loss: 3.3146\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8200 - val_loss: 3.1782\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8318 - val_loss: 3.3305\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8363 - val_loss: 3.1927\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8517 - val_loss: 3.3524\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8610 - val_loss: 3.2300\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9013 - val_loss: 3.4020\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9313 - val_loss: 3.3116\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0016 - val_loss: 3.4572\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0263 - val_loss: 3.4156\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1194 - val_loss: 3.4466\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0370 - val_loss: 3.3189\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9933 - val_loss: 3.3417\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8879 - val_loss: 3.2148\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8560 - val_loss: 3.3202\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8157 - val_loss: 3.1721\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8059 - val_loss: 3.3111\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7907 - val_loss: 3.1593\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7857 - val_loss: 3.2890\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7664 - val_loss: 3.1589\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7638 - val_loss: 3.2822\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7563 - val_loss: 3.1572\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7597 - val_loss: 3.2857\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7570 - val_loss: 3.1582\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7626 - val_loss: 3.2937\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7606 - val_loss: 3.1600\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7696 - val_loss: 3.3031\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7683 - val_loss: 3.1619\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7731 - val_loss: 3.3071\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7715 - val_loss: 3.1652\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7744 - val_loss: 3.3119\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7745 - val_loss: 3.1690\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7787 - val_loss: 3.3194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7800 - val_loss: 3.1752\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7874 - val_loss: 3.3300\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7904 - val_loss: 3.1888\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8053 - val_loss: 3.3458\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8113 - val_loss: 3.2254\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8538 - val_loss: 3.3901\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8872 - val_loss: 3.3223\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9735 - val_loss: 3.4714\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0106 - val_loss: 3.3898\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0474 - val_loss: 3.4116\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9655 - val_loss: 3.2963\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9194 - val_loss: 3.3261\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8441 - val_loss: 3.2254\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8117 - val_loss: 3.3002\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7659 - val_loss: 3.1662\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7519 - val_loss: 3.3063\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7465 - val_loss: 3.1507\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7501 - val_loss: 3.2890\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7260 - val_loss: 3.1467\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7148 - val_loss: 3.2659\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7029 - val_loss: 3.1451\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7030 - val_loss: 3.2713\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7049 - val_loss: 3.1462\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7143 - val_loss: 3.2845\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7141 - val_loss: 3.1465\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7204 - val_loss: 3.2896\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7173 - val_loss: 3.1480\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7189 - val_loss: 3.2916\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7177 - val_loss: 3.1494\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7189 - val_loss: 3.2960\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7206 - val_loss: 3.1508\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7207 - val_loss: 3.3002\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7232 - val_loss: 3.1524\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7220 - val_loss: 3.3038\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7260 - val_loss: 3.1551\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7250 - val_loss: 3.3096\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7314 - val_loss: 3.1611\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7338 - val_loss: 3.3208\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7445 - val_loss: 3.1780\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7585 - val_loss: 3.3454\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7816 - val_loss: 3.2280\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8272 - val_loss: 3.3967\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8726 - val_loss: 3.3220\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9436 - val_loss: 3.4318\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9440 - val_loss: 3.3438\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9545 - val_loss: 3.3489\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8646 - val_loss: 3.2560\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8148 - val_loss: 3.2901\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7708 - val_loss: 3.2183\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7518 - val_loss: 3.2602\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7272 - val_loss: 3.1791\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7010 - val_loss: 3.2612\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6981 - val_loss: 3.1592\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6931 - val_loss: 3.2935\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7095 - val_loss: 3.1425\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7105 - val_loss: 3.2950\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7002 - val_loss: 3.1303\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6862 - val_loss: 3.2692\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6749 - val_loss: 3.1268\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6688 - val_loss: 3.2669\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6699 - val_loss: 3.1283\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6721 - val_loss: 3.2763\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6760 - val_loss: 3.1302\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6765 - val_loss: 3.2818\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6795 - val_loss: 3.1330\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6780 - val_loss: 3.2858\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6823 - val_loss: 3.1357\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6804 - val_loss: 3.2901\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6858 - val_loss: 3.1384\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6832 - val_loss: 3.2938\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6893 - val_loss: 3.1416\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6866 - val_loss: 3.2974\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6937 - val_loss: 3.1463\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6921 - val_loss: 3.3017\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7008 - val_loss: 3.1551\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7037 - val_loss: 3.3090\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7161 - val_loss: 3.1757\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7317 - val_loss: 3.3284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7586 - val_loss: 3.2259\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7971 - val_loss: 3.3696\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8284 - val_loss: 3.2765\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8560 - val_loss: 3.3668\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8357 - val_loss: 3.2551\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8232 - val_loss: 3.3252\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7849 - val_loss: 3.2135\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7643 - val_loss: 3.3030\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7366 - val_loss: 3.1770\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7244 - val_loss: 3.3012\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7150 - val_loss: 3.1545\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6997 - val_loss: 3.2914\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6889 - val_loss: 3.1357\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6698 - val_loss: 3.2790\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6628 - val_loss: 3.1201\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6528 - val_loss: 3.2699\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6498 - val_loss: 3.1150\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6427 - val_loss: 3.2627\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6412 - val_loss: 3.1146\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6386 - val_loss: 3.2640\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6408 - val_loss: 3.1156\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6404 - val_loss: 3.2682\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6434 - val_loss: 3.1176\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6427 - val_loss: 3.2713\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6457 - val_loss: 3.1197\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6449 - val_loss: 3.2742\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6485 - val_loss: 3.1221\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6478 - val_loss: 3.2773\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6521 - val_loss: 3.1252\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6518 - val_loss: 3.2807\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6571 - val_loss: 3.1298\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6576 - val_loss: 3.2855\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6651 - val_loss: 3.1378\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6674 - val_loss: 3.2937\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6798 - val_loss: 3.1541\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6885 - val_loss: 3.3109\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7136 - val_loss: 3.1950\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7410 - val_loss: 3.3346\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7672 - val_loss: 3.2248\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7722 - val_loss: 3.3208\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7628 - val_loss: 3.2082\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7442 - val_loss: 3.2953\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7274 - val_loss: 3.1865\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7142 - val_loss: 3.2926\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7107 - val_loss: 3.1783\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7100 - val_loss: 3.3025\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7109 - val_loss: 3.1702\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7039 - val_loss: 3.2975\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6999 - val_loss: 3.1608\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6901 - val_loss: 3.2909\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6890 - val_loss: 3.1557\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6829 - val_loss: 3.2902\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6842 - val_loss: 3.1520\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6781 - val_loss: 3.2883\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6779 - val_loss: 3.1452\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6681 - val_loss: 3.2832\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6662 - val_loss: 3.1350\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6543 - val_loss: 3.2770\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6516 - val_loss: 3.1215\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6391 - val_loss: 3.2671\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6327 - val_loss: 3.1073\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6220 - val_loss: 3.2561\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6147 - val_loss: 3.1003\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6090 - val_loss: 3.2497\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6051 - val_loss: 3.0987\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6036 - val_loss: 3.2478\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6010 - val_loss: 3.0989\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6011 - val_loss: 3.2483\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5995 - val_loss: 3.0992\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6006 - val_loss: 3.2503\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5997 - val_loss: 3.0997\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6015 - val_loss: 3.2529\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6009 - val_loss: 3.1008\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6030 - val_loss: 3.2555\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6029 - val_loss: 3.1026\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6054 - val_loss: 3.2581\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6057 - val_loss: 3.1053\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6088 - val_loss: 3.2603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6095 - val_loss: 3.1094\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6135 - val_loss: 3.2676\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6190 - val_loss: 3.1177\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6250 - val_loss: 3.2797\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6360 - val_loss: 3.1312\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6425 - val_loss: 3.2926\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6599 - val_loss: 3.1551\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6711 - val_loss: 3.3095\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7018 - val_loss: 3.1917\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7123 - val_loss: 3.3070\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7263 - val_loss: 3.1921\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7016 - val_loss: 3.2758\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6927 - val_loss: 3.1719\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6729 - val_loss: 3.2734\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6784 - val_loss: 3.1696\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6741 - val_loss: 3.2892\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6860 - val_loss: 3.1692\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6771 - val_loss: 3.2892\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6805 - val_loss: 3.1620\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6678 - val_loss: 3.2848\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6715 - val_loss: 3.1569\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6615 - val_loss: 3.2848\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6673 - val_loss: 3.1537\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6578 - val_loss: 3.2840\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6629 - val_loss: 3.1496\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6530 - val_loss: 3.2823\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6573 - val_loss: 3.1438\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6449 - val_loss: 3.2798\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6482 - val_loss: 3.1343\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6342 - val_loss: 3.2755\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6354 - val_loss: 3.1213\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6207 - val_loss: 3.2679\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6153 - val_loss: 3.1042\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6015 - val_loss: 3.2555\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5920 - val_loss: 3.0912\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5846 - val_loss: 3.2448\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5756 - val_loss: 3.0872\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5734 - val_loss: 3.2371\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5666 - val_loss: 3.0872\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5674 - val_loss: 3.2347\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5635 - val_loss: 3.0888\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5650 - val_loss: 3.2335\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5616 - val_loss: 3.0909\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5627 - val_loss: 3.2320\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5602 - val_loss: 3.0931\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5610 - val_loss: 3.2304\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5596 - val_loss: 3.0962\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5604 - val_loss: 3.2293\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5608 - val_loss: 3.1008\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5621 - val_loss: 3.2247\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5620 - val_loss: 3.1073\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5657 - val_loss: 3.2268\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5687 - val_loss: 3.1172\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5770 - val_loss: 3.2326\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5759 - val_loss: 3.1270\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5869 - val_loss: 3.2384\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5848 - val_loss: 3.1378\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5980 - val_loss: 3.2439\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5962 - val_loss: 3.1504\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6131 - val_loss: 3.2492\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6099 - val_loss: 3.1668\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6349 - val_loss: 3.2563\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6292 - val_loss: 3.1909\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6626 - val_loss: 3.2631\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6507 - val_loss: 3.2113\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6791 - val_loss: 3.2469\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6455 - val_loss: 3.2044\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6634 - val_loss: 3.2401\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6371 - val_loss: 3.1974\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6571 - val_loss: 3.2431\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6323 - val_loss: 3.1884\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6506 - val_loss: 3.2432\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6232 - val_loss: 3.1750\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6347 - val_loss: 3.2417\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6072 - val_loss: 3.1552\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6150 - val_loss: 3.2390\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5903 - val_loss: 3.1316\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5899 - val_loss: 3.2353\n",
      "Epoch 494/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5692 - val_loss: 3.1066\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5600 - val_loss: 3.2313\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5504 - val_loss: 3.0908\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5446 - val_loss: 3.2326\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5431 - val_loss: 3.0850\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5443 - val_loss: 3.2419\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5449 - val_loss: 3.0841\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5491 - val_loss: 3.2498\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5492 - val_loss: 3.0840\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5547 - val_loss: 3.2568\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5543 - val_loss: 3.0864\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5588 - val_loss: 3.2591\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5566 - val_loss: 3.0882\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5594 - val_loss: 3.2615\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5591 - val_loss: 3.0905\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5618 - val_loss: 3.2648\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5623 - val_loss: 3.0923\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5643 - val_loss: 3.2672\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5648 - val_loss: 3.0938\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5663 - val_loss: 3.2694\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5669 - val_loss: 3.0952\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5682 - val_loss: 3.2715\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5688 - val_loss: 3.0964\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5697 - val_loss: 3.2734\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5704 - val_loss: 3.0975\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5709 - val_loss: 3.2749\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5717 - val_loss: 3.0990\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5730 - val_loss: 3.2774\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5742 - val_loss: 3.1011\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5768 - val_loss: 3.2812\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5783 - val_loss: 3.1034\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5790 - val_loss: 3.2823\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5806 - val_loss: 3.1049\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5782 - val_loss: 3.2776\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5790 - val_loss: 3.1066\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5764 - val_loss: 3.2812\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5842 - val_loss: 3.1111\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5852 - val_loss: 3.2890\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5936 - val_loss: 3.1142\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5909 - val_loss: 3.2884\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5954 - val_loss: 3.1144\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5886 - val_loss: 3.2855\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5934 - val_loss: 3.1142\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5867 - val_loss: 3.2851\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5922 - val_loss: 3.1130\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5855 - val_loss: 3.2848\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5896 - val_loss: 3.1105\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5826 - val_loss: 3.2835\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5850 - val_loss: 3.1072\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5781 - val_loss: 3.2819\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5794 - val_loss: 3.1036\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5733 - val_loss: 3.2800\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5734 - val_loss: 3.1000\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5681 - val_loss: 3.2765\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5665 - val_loss: 3.0959\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5618 - val_loss: 3.2710\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5582 - val_loss: 3.0913\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5536 - val_loss: 3.2641\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5489 - val_loss: 3.0866\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5447 - val_loss: 3.2588\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5415 - val_loss: 3.0836\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5384 - val_loss: 3.2525\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5337 - val_loss: 3.0806\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5316 - val_loss: 3.2462\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5269 - val_loss: 3.0786\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5279 - val_loss: 3.2473\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5256 - val_loss: 3.0788\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5282 - val_loss: 3.2476\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5244 - val_loss: 3.0782\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5263 - val_loss: 3.2461\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5223 - val_loss: 3.0779\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.5246 - val_loss: 3.2463\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5213 - val_loss: 3.0780\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5241 - val_loss: 3.2475\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5211 - val_loss: 3.0782\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5237 - val_loss: 3.2487\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5210 - val_loss: 3.0784\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5233 - val_loss: 3.2491\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5204 - val_loss: 3.0784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5225 - val_loss: 3.2493\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5197 - val_loss: 3.0785\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5219 - val_loss: 3.2498\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5192 - val_loss: 3.0787\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5215 - val_loss: 3.2506\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5190 - val_loss: 3.0791\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5214 - val_loss: 3.2547\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5214 - val_loss: 3.0806\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5242 - val_loss: 3.2581\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5237 - val_loss: 3.0821\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5258 - val_loss: 3.2608\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5264 - val_loss: 3.0848\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5297 - val_loss: 3.2678\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5349 - val_loss: 3.0946\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5460 - val_loss: 3.2831\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5557 - val_loss: 3.1088\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5677 - val_loss: 3.2931\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5789 - val_loss: 3.1215\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5838 - val_loss: 3.2906\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5920 - val_loss: 3.1284\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5903 - val_loss: 3.2865\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5943 - val_loss: 3.1295\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5870 - val_loss: 3.2812\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5875 - val_loss: 3.1248\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5804 - val_loss: 3.2818\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5819 - val_loss: 3.1186\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5755 - val_loss: 3.2849\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5761 - val_loss: 3.1125\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 339us/step - loss: 55.5221 - val_loss: 54.8771\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 52.7793 - val_loss: 50.2242\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 48.1292 - val_loss: 43.3767\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 41.3041 - val_loss: 34.3733\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 32.3912 - val_loss: 25.0342\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 23.3720 - val_loss: 19.6839\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.6143 - val_loss: 17.4724\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 16.9176 - val_loss: 16.5984\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.4191 - val_loss: 16.2918\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.2755 - val_loss: 16.0539\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.0311 - val_loss: 15.6899\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.5598 - val_loss: 15.1714\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.8945 - val_loss: 14.4277\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.0195 - val_loss: 13.3665\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 12.8806 - val_loss: 12.0718\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.5850 - val_loss: 10.7137\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.3121 - val_loss: 9.5850\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.2442 - val_loss: 9.0195\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.7079 - val_loss: 8.8674\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5938 - val_loss: 8.7713\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5337 - val_loss: 8.6931\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4800 - val_loss: 8.6652\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4507 - val_loss: 8.6524\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4091 - val_loss: 8.6440\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3638 - val_loss: 8.6374\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3245 - val_loss: 8.6244\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2862 - val_loss: 8.6046\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2478 - val_loss: 8.5793\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2080 - val_loss: 8.5480\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1651 - val_loss: 8.5108\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1186 - val_loss: 8.4678\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.0686 - val_loss: 8.4195\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0150 - val_loss: 8.3663\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9577 - val_loss: 8.3085\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8967 - val_loss: 8.2459\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8318 - val_loss: 8.1784\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.7629 - val_loss: 8.1057\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.6897 - val_loss: 8.0275\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6119 - val_loss: 7.9437\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.5290 - val_loss: 7.8540\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 7.4404 - val_loss: 7.7575\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.3457 - val_loss: 7.6533\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.2441 - val_loss: 7.5405\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.1351 - val_loss: 7.4186\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.0188 - val_loss: 7.2880\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.8959 - val_loss: 7.1527\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.7686 - val_loss: 7.0123\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.6391 - val_loss: 6.8719\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 6.5086 - val_loss: 6.7269\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.3770 - val_loss: 6.5844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.2432 - val_loss: 6.4364\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.1067 - val_loss: 6.2889\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.9666 - val_loss: 6.1361\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.8223 - val_loss: 5.9803\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.6734 - val_loss: 5.8180\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.5206 - val_loss: 5.6576\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.3654 - val_loss: 5.4901\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.2107 - val_loss: 5.3383\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 5.0613 - val_loss: 5.1856\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.9222 - val_loss: 5.0549\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.7963 - val_loss: 4.9248\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.6831 - val_loss: 4.8221\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.5842 - val_loss: 4.7121\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.4987 - val_loss: 4.6530\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.4294 - val_loss: 4.5625\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.3805 - val_loss: 4.6142\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.3791 - val_loss: 4.5722\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.4338 - val_loss: 4.8959\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.6053 - val_loss: 4.8497\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.7453 - val_loss: 5.0511\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.7481 - val_loss: 4.6537\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.5529 - val_loss: 4.6052\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.3598 - val_loss: 4.2839\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.1707 - val_loss: 4.2823\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0745 - val_loss: 4.1317\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0009 - val_loss: 4.1729\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.9668 - val_loss: 4.0767\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.9505 - val_loss: 4.1884\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.9649 - val_loss: 4.1242\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0136 - val_loss: 4.3659\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.1088 - val_loss: 4.3546\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.2636 - val_loss: 4.5255\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2593 - val_loss: 4.3838\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.2935 - val_loss: 4.3164\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0710 - val_loss: 4.0400\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9464 - val_loss: 4.0349\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8157 - val_loss: 3.9105\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8151 - val_loss: 3.9973\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.7800 - val_loss: 3.9559\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8647 - val_loss: 4.0728\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.8575 - val_loss: 4.0918\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0090 - val_loss: 4.1031\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8939 - val_loss: 3.9931\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.9144 - val_loss: 3.9371\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7460 - val_loss: 3.8406\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.7689 - val_loss: 3.8624\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6837 - val_loss: 3.8184\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7524 - val_loss: 3.8525\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.6821 - val_loss: 3.7960\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7347 - val_loss: 3.8027\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6436 - val_loss: 3.7298\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6736 - val_loss: 3.7477\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5988 - val_loss: 3.6891\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6363 - val_loss: 3.7171\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5761 - val_loss: 3.6545\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6045 - val_loss: 3.6706\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5378 - val_loss: 3.6025\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5536 - val_loss: 3.6303\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5035 - val_loss: 3.5553\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5057 - val_loss: 3.5883\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4677 - val_loss: 3.5002\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4500 - val_loss: 3.5365\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4234 - val_loss: 3.4514\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4024 - val_loss: 3.5000\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3928 - val_loss: 3.4284\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3813 - val_loss: 3.4825\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3795 - val_loss: 3.4089\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3634 - val_loss: 3.4477\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3498 - val_loss: 3.3704\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3260 - val_loss: 3.4084\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3160 - val_loss: 3.3423\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2991 - val_loss: 3.3797\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2916 - val_loss: 3.3173\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2747 - val_loss: 3.3515\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.2673 - val_loss: 3.2749\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2333 - val_loss: 3.3042\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2255 - val_loss: 3.2401\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1993 - val_loss: 3.2765\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2010 - val_loss: 3.2265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1863 - val_loss: 3.2679\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1936 - val_loss: 3.2217\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1817 - val_loss: 3.2687\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1943 - val_loss: 3.2248\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1848 - val_loss: 3.2764\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2013 - val_loss: 3.2288\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1884 - val_loss: 3.2820\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2063 - val_loss: 3.2388\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1983 - val_loss: 3.2948\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2183 - val_loss: 3.2560\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2151 - val_loss: 3.3020\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2253 - val_loss: 3.2423\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2015 - val_loss: 3.2722\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1963 - val_loss: 3.1969\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1568 - val_loss: 3.2178\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1435 - val_loss: 3.1401\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1025 - val_loss: 3.1492\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0776 - val_loss: 3.0773\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0422 - val_loss: 3.0891\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0230 - val_loss: 3.0382\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0022 - val_loss: 3.0560\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9934 - val_loss: 3.0235\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9858 - val_loss: 3.0471\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9847 - val_loss: 3.0211\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9824 - val_loss: 3.0503\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9866 - val_loss: 3.0259\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9872 - val_loss: 3.0622\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9963 - val_loss: 3.0405\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0017 - val_loss: 3.0840\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0156 - val_loss: 3.0657\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0274 - val_loss: 3.1256\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0526 - val_loss: 3.1321\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0905 - val_loss: 3.2343\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1573 - val_loss: 3.2630\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2123 - val_loss: 3.3797\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2941 - val_loss: 3.2926\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2388 - val_loss: 3.2813\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.2032 - val_loss: 3.1565\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1167 - val_loss: 3.1375\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0628 - val_loss: 3.0499\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0123 - val_loss: 3.0567\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9834 - val_loss: 2.9954\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9528 - val_loss: 2.9942\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9293 - val_loss: 2.9489\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9078 - val_loss: 2.9521\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8992 - val_loss: 2.9307\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8901 - val_loss: 2.9323\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8856 - val_loss: 2.9220\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8817 - val_loss: 2.9234\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8792 - val_loss: 2.9161\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8766 - val_loss: 2.9205\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8755 - val_loss: 2.9135\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8742 - val_loss: 2.9257\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8760 - val_loss: 2.9150\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8746 - val_loss: 2.9320\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8774 - val_loss: 2.9216\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8778 - val_loss: 2.9452\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.8845 - val_loss: 2.9354\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.8886 - val_loss: 2.9705\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9031 - val_loss: 2.9671\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9188 - val_loss: 3.0299\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.9566 - val_loss: 3.0640\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0119 - val_loss: 3.2214\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1403 - val_loss: 3.3465\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2774 - val_loss: 3.4504\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3434 - val_loss: 3.2627\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2038 - val_loss: 3.1788\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1016 - val_loss: 3.0368\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9965 - val_loss: 3.0311\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9565 - val_loss: 2.9691\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9198 - val_loss: 2.9827\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9062 - val_loss: 2.9350\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8845 - val_loss: 2.9395\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8702 - val_loss: 2.8987\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8530 - val_loss: 2.9081\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8479 - val_loss: 2.8816\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8381 - val_loss: 2.8945\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8363 - val_loss: 2.8768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8318 - val_loss: 2.8917\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8327 - val_loss: 2.8756\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8300 - val_loss: 2.8938\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8327 - val_loss: 2.8759\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8293 - val_loss: 2.8957\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.8324 - val_loss: 2.8758\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.8276 - val_loss: 2.8948\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8302 - val_loss: 2.8764\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8264 - val_loss: 2.8975\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8308 - val_loss: 2.8800\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8283 - val_loss: 2.9076\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8375 - val_loss: 2.8931\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8385 - val_loss: 2.9349\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8588 - val_loss: 2.9332\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8759 - val_loss: 3.0078\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9248 - val_loss: 3.0586\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9943 - val_loss: 3.2753\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1766 - val_loss: 3.3576\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2769 - val_loss: 3.3470\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2306 - val_loss: 3.1208\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0630 - val_loss: 3.0600\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9814 - val_loss: 2.9641\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9148 - val_loss: 2.9750\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.8915 - val_loss: 2.9249\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 2.8668 - val_loss: 2.9523\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8661 - val_loss: 2.9120\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.8527 - val_loss: 2.9395\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8530 - val_loss: 2.8993\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.8388 - val_loss: 2.9308\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8425 - val_loss: 2.8950\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8317 - val_loss: 2.9260\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8372 - val_loss: 2.8920\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8281 - val_loss: 2.9288\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8385 - val_loss: 2.9001\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8348 - val_loss: 2.9432\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8493 - val_loss: 2.9201\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8531 - val_loss: 2.9742\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8764 - val_loss: 2.9619\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8927 - val_loss: 3.0450\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9443 - val_loss: 3.0847\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0123 - val_loss: 3.2364\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1235 - val_loss: 3.1733\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1043 - val_loss: 3.1536\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0458 - val_loss: 3.0070\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9446 - val_loss: 2.9838\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8905 - val_loss: 2.9039\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.8412 - val_loss: 2.9188\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8218 - val_loss: 2.8608\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7944 - val_loss: 2.8729\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7822 - val_loss: 2.8263\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7639 - val_loss: 2.8355\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7560 - val_loss: 2.8195\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7500 - val_loss: 2.8250\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7463 - val_loss: 2.8196\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7440 - val_loss: 2.8217\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7421 - val_loss: 2.8183\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7402 - val_loss: 2.8188\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7384 - val_loss: 2.8163\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7366 - val_loss: 2.8171\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7349 - val_loss: 2.8150\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7332 - val_loss: 2.8159\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7315 - val_loss: 2.8139\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7298 - val_loss: 2.8152\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7282 - val_loss: 2.8129\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7266 - val_loss: 2.8157\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7253 - val_loss: 2.8121\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7241 - val_loss: 2.8186\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7238 - val_loss: 2.8127\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7242 - val_loss: 2.8271\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7266 - val_loss: 2.8181\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7297 - val_loss: 2.8476\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7389 - val_loss: 2.8297\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7419 - val_loss: 2.8729\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7555 - val_loss: 2.8529\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7641 - val_loss: 2.9238\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7967 - val_loss: 2.9505\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8606 - val_loss: 3.1422\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0045 - val_loss: 3.3068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1947 - val_loss: 3.4007\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2126 - val_loss: 3.1092\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0288 - val_loss: 3.0430\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9141 - val_loss: 2.9110\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8402 - val_loss: 2.9492\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8272 - val_loss: 2.8914\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8115 - val_loss: 2.9478\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8151 - val_loss: 2.9059\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8183 - val_loss: 2.9789\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8416 - val_loss: 2.9616\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8717 - val_loss: 3.0518\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9104 - val_loss: 3.0358\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9444 - val_loss: 3.1273\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9777 - val_loss: 3.0456\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9571 - val_loss: 3.0610\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9193 - val_loss: 2.9421\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8607 - val_loss: 2.9479\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8145 - val_loss: 2.8607\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7735 - val_loss: 2.8861\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7516 - val_loss: 2.8252\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7271 - val_loss: 2.8427\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7120 - val_loss: 2.8055\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6982 - val_loss: 2.8192\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6933 - val_loss: 2.8003\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6885 - val_loss: 2.8104\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6864 - val_loss: 2.7980\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6840 - val_loss: 2.8086\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6827 - val_loss: 2.7976\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6811 - val_loss: 2.8090\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6803 - val_loss: 2.7977\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6791 - val_loss: 2.8112\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6791 - val_loss: 2.7986\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6782 - val_loss: 2.8180\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6809 - val_loss: 2.8028\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6826 - val_loss: 2.8361\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6913 - val_loss: 2.8144\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.6961 - val_loss: 2.8620\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.7101 - val_loss: 2.8408\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7257 - val_loss: 2.9230\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.7590 - val_loss: 2.9235\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.8170 - val_loss: 3.0817\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9052 - val_loss: 3.1294\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0135 - val_loss: 3.2436\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0428 - val_loss: 3.0648\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9592 - val_loss: 3.0435\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8704 - val_loss: 2.9125\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.8160 - val_loss: 2.9402\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7822 - val_loss: 2.8605\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7560 - val_loss: 2.9098\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7434 - val_loss: 2.8540\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7401 - val_loss: 2.9134\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 44us/step - loss: 2.7419 - val_loss: 2.8657\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.7518 - val_loss: 2.9438\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7669 - val_loss: 2.9046\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7933 - val_loss: 2.9922\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8124 - val_loss: 2.9474\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8385 - val_loss: 3.0266\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8437 - val_loss: 2.9499\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8407 - val_loss: 3.0062\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8249 - val_loss: 2.9202\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8113 - val_loss: 2.9656\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7864 - val_loss: 2.8871\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7755 - val_loss: 2.9398\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7599 - val_loss: 2.8646\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7500 - val_loss: 2.9153\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7334 - val_loss: 2.8424\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7205 - val_loss: 2.8969\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7129 - val_loss: 2.8304\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7040 - val_loss: 2.8804\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6975 - val_loss: 2.8166\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6857 - val_loss: 2.8620\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6801 - val_loss: 2.8069\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6706 - val_loss: 2.8510\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6691 - val_loss: 2.8039\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6641 - val_loss: 2.8495\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6657 - val_loss: 2.8051\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6638 - val_loss: 2.8570\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6688 - val_loss: 2.8111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6699 - val_loss: 2.8704\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.6770 - val_loss: 2.8218\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6819 - val_loss: 2.8901\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6911 - val_loss: 2.8449\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7098 - val_loss: 2.9543\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7438 - val_loss: 2.9354\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8075 - val_loss: 3.0681\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8493 - val_loss: 3.0108\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8798 - val_loss: 3.1183\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8900 - val_loss: 2.9892\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8619 - val_loss: 3.0013\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7924 - val_loss: 2.8819\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7583 - val_loss: 2.9351\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7318 - val_loss: 2.8557\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7268 - val_loss: 2.9188\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7115 - val_loss: 2.8438\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7078 - val_loss: 2.9167\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7041 - val_loss: 2.8480\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7116 - val_loss: 2.9336\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7165 - val_loss: 2.8716\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7385 - val_loss: 2.9646\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7440 - val_loss: 2.8978\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7650 - val_loss: 2.9778\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7560 - val_loss: 2.8993\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7652 - val_loss: 2.9683\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7467 - val_loss: 2.8861\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7509 - val_loss: 2.9524\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7300 - val_loss: 2.8688\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7318 - val_loss: 2.9384\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7147 - val_loss: 2.8577\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7187 - val_loss: 2.9344\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7082 - val_loss: 2.8558\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7156 - val_loss: 2.9370\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7079 - val_loss: 2.8580\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7167 - val_loss: 2.9405\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.7089 - val_loss: 2.8600\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7177 - val_loss: 2.9425\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7086 - val_loss: 2.8607\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.7172 - val_loss: 2.9430\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.7069 - val_loss: 2.8601\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7152 - val_loss: 2.9425\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7044 - val_loss: 2.8586\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7122 - val_loss: 2.9415\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.7014 - val_loss: 2.8568\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7088 - val_loss: 2.9402\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.6981 - val_loss: 2.8548\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7052 - val_loss: 2.9388\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6948 - val_loss: 2.8529\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7014 - val_loss: 2.9372\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6912 - val_loss: 2.8509\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6974 - val_loss: 2.9350\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6872 - val_loss: 2.8486\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6926 - val_loss: 2.9315\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6822 - val_loss: 2.8432\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6829 - val_loss: 2.9184\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6691 - val_loss: 2.8310\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6633 - val_loss: 2.8932\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6464 - val_loss: 2.8168\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6377 - val_loss: 2.8670\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6237 - val_loss: 2.8073\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6169 - val_loss: 2.8546\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6124 - val_loss: 2.8069\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6091 - val_loss: 2.8519\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6091 - val_loss: 2.8085\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6077 - val_loss: 2.8499\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6076 - val_loss: 2.8091\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6054 - val_loss: 2.8467\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6048 - val_loss: 2.8092\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6028 - val_loss: 2.8461\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6025 - val_loss: 2.8102\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6014 - val_loss: 2.8480\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6017 - val_loss: 2.8118\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6017 - val_loss: 2.8522\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6024 - val_loss: 2.8139\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6037 - val_loss: 2.8593\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6048 - val_loss: 2.8176\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6093 - val_loss: 2.8759\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6132 - val_loss: 2.8298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6275 - val_loss: 2.9141\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6399 - val_loss: 2.8638\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6753 - val_loss: 2.9918\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7048 - val_loss: 2.9602\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7830 - val_loss: 3.0982\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8045 - val_loss: 2.9860\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8081 - val_loss: 3.0516\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7637 - val_loss: 2.9218\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7457 - val_loss: 2.9893\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7101 - val_loss: 2.8908\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7147 - val_loss: 2.9761\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6952 - val_loss: 2.8870\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7097 - val_loss: 2.9773\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6914 - val_loss: 2.8823\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7025 - val_loss: 2.9666\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6806 - val_loss: 2.8709\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6881 - val_loss: 2.9524\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6664 - val_loss: 2.8588\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6718 - val_loss: 2.9391\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6537 - val_loss: 2.8444\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6503 - val_loss: 2.9101\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6288 - val_loss: 2.8314\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6254 - val_loss: 2.8919\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6111 - val_loss: 2.8261\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6112 - val_loss: 2.8859\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6027 - val_loss: 2.8265\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6074 - val_loss: 2.8900\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6031 - val_loss: 2.8304\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6112 - val_loss: 2.8998\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6088 - val_loss: 2.8380\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6208 - val_loss: 2.9156\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6194 - val_loss: 2.8475\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6341 - val_loss: 2.9314\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6309 - val_loss: 2.8559\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6457 - val_loss: 2.9467\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6417 - val_loss: 2.8657\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6596 - val_loss: 2.9619\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6543 - val_loss: 2.8731\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6701 - val_loss: 2.9675\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6592 - val_loss: 2.8742\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6711 - val_loss: 2.9663\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6575 - val_loss: 2.8712\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6666 - val_loss: 2.9583\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6492 - val_loss: 2.8626\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6533 - val_loss: 2.9446\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6360 - val_loss: 2.8515\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6354 - val_loss: 2.9233\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6171 - val_loss: 2.8425\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6174 - val_loss: 2.9128\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6058 - val_loss: 2.8408\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6100 - val_loss: 2.9130\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6023 - val_loss: 2.8432\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6105 - val_loss: 2.9203\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6058 - val_loss: 2.8486\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6172 - val_loss: 2.9307\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6131 - val_loss: 2.8544\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6249 - val_loss: 2.9378\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6180 - val_loss: 2.8574\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6284 - val_loss: 2.9407\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6194 - val_loss: 2.8581\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6287 - val_loss: 2.9414\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6186 - val_loss: 2.8580\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6271 - val_loss: 2.9405\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6166 - val_loss: 2.8572\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6243 - val_loss: 2.9391\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6139 - val_loss: 2.8566\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6214 - val_loss: 2.9384\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6117 - val_loss: 2.8566\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6194 - val_loss: 2.9389\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6105 - val_loss: 2.8574\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6186 - val_loss: 2.9403\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6102 - val_loss: 2.8586\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6188 - val_loss: 2.9426\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6107 - val_loss: 2.8602\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6198 - val_loss: 2.9453\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6118 - val_loss: 2.8617\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6210 - val_loss: 2.9474\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6124 - val_loss: 2.8624\n",
      "Epoch 520/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6210 - val_loss: 2.9475\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6113 - val_loss: 2.8618\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6187 - val_loss: 2.9453\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6081 - val_loss: 2.8602\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6144 - val_loss: 2.9421\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6039 - val_loss: 2.8587\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6097 - val_loss: 2.9401\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6005 - val_loss: 2.8584\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6065 - val_loss: 2.9400\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5987 - val_loss: 2.8592\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6053 - val_loss: 2.9417\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5983 - val_loss: 2.8608\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6058 - val_loss: 2.9444\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5990 - val_loss: 2.8628\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6078 - val_loss: 2.9488\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6015 - val_loss: 2.8662\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6124 - val_loss: 2.9553\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6062 - val_loss: 2.8701\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6181 - val_loss: 2.9604\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6099 - val_loss: 2.8716\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6201 - val_loss: 2.9607\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6094 - val_loss: 2.8701\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6172 - val_loss: 2.9558\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6045 - val_loss: 2.8657\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6085 - val_loss: 2.9458\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5945 - val_loss: 2.8594\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5951 - val_loss: 2.9365\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5846 - val_loss: 2.8574\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5865 - val_loss: 2.9343\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5807 - val_loss: 2.8596\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5853 - val_loss: 2.9373\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5810 - val_loss: 2.8625\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5873 - val_loss: 2.9438\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5844 - val_loss: 2.8659\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5917 - val_loss: 2.9500\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5879 - val_loss: 2.8681\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5948 - val_loss: 2.9524\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5884 - val_loss: 2.8686\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5951 - val_loss: 2.9541\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5884 - val_loss: 2.8710\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5991 - val_loss: 2.9609\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5935 - val_loss: 2.8754\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6060 - val_loss: 2.9677\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5989 - val_loss: 2.8778\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6096 - val_loss: 2.9686\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5992 - val_loss: 2.8774\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6082 - val_loss: 2.9658\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5960 - val_loss: 2.8736\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6007 - val_loss: 2.9565\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5870 - val_loss: 2.8672\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5862 - val_loss: 2.9441\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5746 - val_loss: 2.8643\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5741 - val_loss: 2.9346\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5664 - val_loss: 2.8636\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5673 - val_loss: 2.9311\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5620 - val_loss: 2.8639\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5623 - val_loss: 2.9289\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5577 - val_loss: 2.8631\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5571 - val_loss: 2.9262\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5535 - val_loss: 2.8637\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5533 - val_loss: 2.9247\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5508 - val_loss: 2.8655\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5509 - val_loss: 2.9242\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5492 - val_loss: 2.8680\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5487 - val_loss: 2.9216\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5458 - val_loss: 2.8686\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5442 - val_loss: 2.9173\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5403 - val_loss: 2.8692\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5390 - val_loss: 2.9128\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5353 - val_loss: 2.8708\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5333 - val_loss: 2.9062\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5294 - val_loss: 2.8733\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5249 - val_loss: 2.8952\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5212 - val_loss: 2.8770\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5167 - val_loss: 2.8874\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5185 - val_loss: 2.8879\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5176 - val_loss: 2.8879\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5248 - val_loss: 2.9077\n",
      "Epoch 598/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5284 - val_loss: 2.8972\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5477 - val_loss: 2.9346\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5462 - val_loss: 2.9085\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 347us/step - loss: 56.3517 - val_loss: 54.3667\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 53.6881 - val_loss: 49.9534\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 49.2671 - val_loss: 43.5292\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 42.8281 - val_loss: 34.9467\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 34.2068 - val_loss: 25.5873\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 24.8223 - val_loss: 19.7969\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.2152 - val_loss: 17.5578\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 17.1333 - val_loss: 16.8056\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 16.4705 - val_loss: 16.6237\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 16.3088 - val_loss: 16.4433\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 16.1249 - val_loss: 16.0593\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 15.7219 - val_loss: 15.4735\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 15.1016 - val_loss: 14.6797\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 14.2547 - val_loss: 13.6356\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 13.1393 - val_loss: 12.4040\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 11.8493 - val_loss: 11.1696\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 10.5954 - val_loss: 10.1167\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 9.4805 - val_loss: 9.4798\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 8.7808 - val_loss: 9.2563\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.5784 - val_loss: 9.1360\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.4942 - val_loss: 9.0666\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.4591 - val_loss: 9.0163\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.4285 - val_loss: 8.9927\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.4027 - val_loss: 8.9719\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.3666 - val_loss: 8.9566\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.3362 - val_loss: 8.9411\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.3092 - val_loss: 8.9211\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2837 - val_loss: 8.8980\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2587 - val_loss: 8.8724\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2337 - val_loss: 8.8443\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2076 - val_loss: 8.8144\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.1802 - val_loss: 8.7826\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.1514 - val_loss: 8.7485\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1206 - val_loss: 8.7120\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.0874 - val_loss: 8.6728\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.0513 - val_loss: 8.6300\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.0115 - val_loss: 8.5827\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.9675 - val_loss: 8.5298\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 7.9183 - val_loss: 8.4701\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 7.8630 - val_loss: 8.4026\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.8006 - val_loss: 8.3259\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 7.7301 - val_loss: 8.2391\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.6508 - val_loss: 8.1410\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 7.5618 - val_loss: 8.0307\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.4624 - val_loss: 7.9073\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 7.3520 - val_loss: 7.7705\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 7.2300 - val_loss: 7.6202\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.0961 - val_loss: 7.4564\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 6.9506 - val_loss: 7.2815\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 6.7945 - val_loss: 7.0948\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 6.6284 - val_loss: 6.8996\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 6.4534 - val_loss: 6.6955\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 6.2712 - val_loss: 6.4855\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 6.0839 - val_loss: 6.2719\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 5.8939 - val_loss: 6.0580\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.7043 - val_loss: 5.8472\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 5.5182 - val_loss: 5.6442\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.3391 - val_loss: 5.4539\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.1707 - val_loss: 5.2794\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.0150 - val_loss: 5.1227\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.8757 - val_loss: 4.9826\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.7531 - val_loss: 4.8568\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.6457 - val_loss: 4.7491\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.5526 - val_loss: 4.6483\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 4.4717 - val_loss: 4.5624\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.4033 - val_loss: 4.4834\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.3460 - val_loss: 4.4258\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.3003 - val_loss: 4.3703\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.2699 - val_loss: 4.4248\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 4.3126 - val_loss: 4.4948\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.4481 - val_loss: 4.8560\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.7546 - val_loss: 4.6414\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.6297 - val_loss: 4.6387\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.5518 - val_loss: 4.2586\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2586 - val_loss: 4.1870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.1263 - val_loss: 4.0322\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.0381 - val_loss: 4.0543\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0082 - val_loss: 3.9647\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.9929 - val_loss: 4.0764\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0211 - val_loss: 4.0019\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0509 - val_loss: 4.2065\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.1398 - val_loss: 4.0603\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.1226 - val_loss: 4.2442\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.1803 - val_loss: 3.9897\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.0610 - val_loss: 4.0961\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.0443 - val_loss: 3.8740\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.9494 - val_loss: 4.0017\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.9582 - val_loss: 3.8392\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.9120 - val_loss: 4.0058\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.9623 - val_loss: 3.8370\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.9077 - val_loss: 3.9997\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.9597 - val_loss: 3.8190\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.8884 - val_loss: 3.9593\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.9267 - val_loss: 3.7724\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.8420 - val_loss: 3.8835\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.8607 - val_loss: 3.7108\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.7795 - val_loss: 3.8035\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.7908 - val_loss: 3.6593\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.7277 - val_loss: 3.7476\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.7433 - val_loss: 3.6239\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.6931 - val_loss: 3.7141\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.7178 - val_loss: 3.6030\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.6755 - val_loss: 3.6922\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.7037 - val_loss: 3.5800\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.6564 - val_loss: 3.6523\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6731 - val_loss: 3.5388\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.6180 - val_loss: 3.5957\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6267 - val_loss: 3.4894\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5718 - val_loss: 3.5362\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5775 - val_loss: 3.4545\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.5409 - val_loss: 3.5026\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5515 - val_loss: 3.4373\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.5271 - val_loss: 3.4830\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5375 - val_loss: 3.4236\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5171 - val_loss: 3.4575\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5178 - val_loss: 3.3976\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4956 - val_loss: 3.4135\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4818 - val_loss: 3.3465\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4520 - val_loss: 3.3395\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4206 - val_loss: 3.2806\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3937 - val_loss: 3.2861\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3771 - val_loss: 3.2499\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3670 - val_loss: 3.2680\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3624 - val_loss: 3.2463\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3667 - val_loss: 3.2703\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3648 - val_loss: 3.2533\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3759 - val_loss: 3.2720\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3669 - val_loss: 3.2496\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3746 - val_loss: 3.2599\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.3570 - val_loss: 3.2348\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3629 - val_loss: 3.2421\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3426 - val_loss: 3.2185\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3495 - val_loss: 3.2252\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3287 - val_loss: 3.2027\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3365 - val_loss: 3.2071\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3133 - val_loss: 3.1801\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3179 - val_loss: 3.1799\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2900 - val_loss: 3.1444\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2886 - val_loss: 3.1450\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2618 - val_loss: 3.1088\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2593 - val_loss: 3.1166\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2394 - val_loss: 3.0808\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2349 - val_loss: 3.0905\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2169 - val_loss: 3.0627\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2199 - val_loss: 3.0813\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2085 - val_loss: 3.0637\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2206 - val_loss: 3.0888\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2152 - val_loss: 3.0810\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2356 - val_loss: 3.1093\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2337 - val_loss: 3.1135\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2652 - val_loss: 3.1484\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2691 - val_loss: 3.1602\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3027 - val_loss: 3.1728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2916 - val_loss: 3.1666\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3073 - val_loss: 3.1478\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2687 - val_loss: 3.0896\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2469 - val_loss: 3.0685\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1990 - val_loss: 3.0185\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1858 - val_loss: 3.0190\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1577 - val_loss: 2.9729\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1472 - val_loss: 2.9817\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1271 - val_loss: 2.9350\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1176 - val_loss: 2.9627\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1109 - val_loss: 2.9226\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1067 - val_loss: 2.9594\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1082 - val_loss: 2.9371\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1166 - val_loss: 2.9786\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1233 - val_loss: 2.9673\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1395 - val_loss: 3.0134\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1521 - val_loss: 3.0333\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1925 - val_loss: 3.0925\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2252 - val_loss: 3.1677\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3007 - val_loss: 3.1832\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3103 - val_loss: 3.1659\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3000 - val_loss: 3.0939\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2251 - val_loss: 3.0196\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1856 - val_loss: 2.9902\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1352 - val_loss: 2.9106\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0999 - val_loss: 2.9162\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0744 - val_loss: 2.8577\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0539 - val_loss: 2.8741\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0421 - val_loss: 2.8309\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0257 - val_loss: 2.8428\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0189 - val_loss: 2.8206\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0120 - val_loss: 2.8336\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0113 - val_loss: 2.8136\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0057 - val_loss: 2.8288\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0062 - val_loss: 2.8090\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0014 - val_loss: 2.8277\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0035 - val_loss: 2.8053\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9989 - val_loss: 2.8284\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0019 - val_loss: 2.8051\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9989 - val_loss: 2.8418\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0092 - val_loss: 2.8213\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0146 - val_loss: 2.8754\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0325 - val_loss: 2.8698\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0556 - val_loss: 2.9471\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 3.0903 - val_loss: 3.0207\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1737 - val_loss: 3.1903\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3217 - val_loss: 3.4531\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5363 - val_loss: 3.3392\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.4639 - val_loss: 3.1977\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3115 - val_loss: 2.9670\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1147 - val_loss: 2.8252\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0342 - val_loss: 2.8187\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9974 - val_loss: 2.7646\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9737 - val_loss: 2.7736\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9643 - val_loss: 2.7571\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9601 - val_loss: 2.7713\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9584 - val_loss: 2.7530\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9539 - val_loss: 2.7733\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9558 - val_loss: 2.7539\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9551 - val_loss: 2.7859\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9616 - val_loss: 2.7641\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9641 - val_loss: 2.7996\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9690 - val_loss: 2.7730\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9697 - val_loss: 2.8047\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9708 - val_loss: 2.7773\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9706 - val_loss: 2.8074\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9710 - val_loss: 2.7806\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9712 - val_loss: 2.8115\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9725 - val_loss: 2.7950\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9810 - val_loss: 2.8367\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9914 - val_loss: 2.8491\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0226 - val_loss: 2.9078\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0521 - val_loss: 2.9812\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1267 - val_loss: 3.0975\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2316 - val_loss: 3.2468\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3507 - val_loss: 3.1897\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3213 - val_loss: 3.1220\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2388 - val_loss: 2.9138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0612 - val_loss: 2.7836\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9793 - val_loss: 2.7681\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9416 - val_loss: 2.7183\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9267 - val_loss: 2.7303\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9109 - val_loss: 2.6984\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9012 - val_loss: 2.7136\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8953 - val_loss: 2.6924\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8920 - val_loss: 2.7107\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8900 - val_loss: 2.6880\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8893 - val_loss: 2.7125\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8888 - val_loss: 2.6918\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8911 - val_loss: 2.7199\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8911 - val_loss: 2.7010\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8962 - val_loss: 2.7296\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8957 - val_loss: 2.7102\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9004 - val_loss: 2.7409\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9020 - val_loss: 2.7305\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9134 - val_loss: 2.7683\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9223 - val_loss: 2.7873\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9553 - val_loss: 2.8524\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9975 - val_loss: 2.9294\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0722 - val_loss: 2.9998\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1357 - val_loss: 3.0890\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2076 - val_loss: 3.1020\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2309 - val_loss: 3.0922\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2062 - val_loss: 2.9512\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0890 - val_loss: 2.8867\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0287 - val_loss: 2.8076\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9615 - val_loss: 2.7644\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9393 - val_loss: 2.7669\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9216 - val_loss: 2.7438\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9193 - val_loss: 2.7532\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9052 - val_loss: 2.7269\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9007 - val_loss: 2.7349\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8867 - val_loss: 2.7104\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8845 - val_loss: 2.7157\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8694 - val_loss: 2.6863\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8655 - val_loss: 2.6961\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8527 - val_loss: 2.6671\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8485 - val_loss: 2.6803\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8394 - val_loss: 2.6531\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8344 - val_loss: 2.6670\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8287 - val_loss: 2.6451\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8250 - val_loss: 2.6612\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8226 - val_loss: 2.6431\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8217 - val_loss: 2.6624\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8210 - val_loss: 2.6447\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8217 - val_loss: 2.6678\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8222 - val_loss: 2.6508\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8257 - val_loss: 2.6763\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8259 - val_loss: 2.6592\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8312 - val_loss: 2.6884\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8326 - val_loss: 2.6757\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8427 - val_loss: 2.7051\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8442 - val_loss: 2.7153\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8703 - val_loss: 2.7768\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9090 - val_loss: 2.8630\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0033 - val_loss: 2.9913\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1155 - val_loss: 3.1333\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2568 - val_loss: 3.1868\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3094 - val_loss: 3.1517\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2654 - val_loss: 2.9336\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0616 - val_loss: 2.7803\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9253 - val_loss: 2.7092\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8564 - val_loss: 2.6800\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8463 - val_loss: 2.6913\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8342 - val_loss: 2.6688\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8326 - val_loss: 2.6793\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8175 - val_loss: 2.6519\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8128 - val_loss: 2.6647\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8023 - val_loss: 2.6418\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8007 - val_loss: 2.6576\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7953 - val_loss: 2.6382\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7959 - val_loss: 2.6580\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7937 - val_loss: 2.6408\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7967 - val_loss: 2.6648\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7971 - val_loss: 2.6495\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8034 - val_loss: 2.6751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8038 - val_loss: 2.6646\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8151 - val_loss: 2.6941\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8185 - val_loss: 2.6976\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8430 - val_loss: 2.7476\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8664 - val_loss: 2.7786\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9203 - val_loss: 2.8662\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9837 - val_loss: 2.9574\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0933 - val_loss: 3.0539\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1681 - val_loss: 3.0254\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1523 - val_loss: 2.9304\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0483 - val_loss: 2.8040\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9415 - val_loss: 2.7405\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8705 - val_loss: 2.6995\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8444 - val_loss: 2.7084\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8322 - val_loss: 2.6875\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8340 - val_loss: 2.7123\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8281 - val_loss: 2.6830\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8278 - val_loss: 2.7026\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8167 - val_loss: 2.6734\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8162 - val_loss: 2.6931\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8074 - val_loss: 2.6686\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8104 - val_loss: 2.6921\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8049 - val_loss: 2.6696\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8096 - val_loss: 2.6950\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8055 - val_loss: 2.6745\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8133 - val_loss: 2.7037\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8123 - val_loss: 2.6847\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8224 - val_loss: 2.7204\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8269 - val_loss: 2.7049\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8420 - val_loss: 2.7559\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8605 - val_loss: 2.7580\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8965 - val_loss: 2.8517\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9567 - val_loss: 2.8756\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0141 - val_loss: 2.9207\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0299 - val_loss: 2.8638\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9966 - val_loss: 2.8206\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9314 - val_loss: 2.7298\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8648 - val_loss: 2.7163\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8311 - val_loss: 2.6808\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8169 - val_loss: 2.6954\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8013 - val_loss: 2.6579\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7949 - val_loss: 2.6825\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7851 - val_loss: 2.6422\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7776 - val_loss: 2.6589\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7634 - val_loss: 2.6228\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7551 - val_loss: 2.6370\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7450 - val_loss: 2.6090\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7383 - val_loss: 2.6215\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7324 - val_loss: 2.6011\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7293 - val_loss: 2.6154\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7276 - val_loss: 2.5998\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7271 - val_loss: 2.6157\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7296 - val_loss: 2.6035\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7297 - val_loss: 2.6179\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7316 - val_loss: 2.6076\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7331 - val_loss: 2.6221\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7353 - val_loss: 2.6125\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7371 - val_loss: 2.6246\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7383 - val_loss: 2.6175\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7416 - val_loss: 2.6283\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7437 - val_loss: 2.6268\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7500 - val_loss: 2.6384\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7576 - val_loss: 2.6441\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7652 - val_loss: 2.6633\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7856 - val_loss: 2.6869\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8077 - val_loss: 2.7271\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8565 - val_loss: 2.8061\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9210 - val_loss: 2.7792\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9168 - val_loss: 2.8176\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9224 - val_loss: 2.7538\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8935 - val_loss: 2.8032\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9011 - val_loss: 2.7543\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8941 - val_loss: 2.8131\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9016 - val_loss: 2.7382\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8754 - val_loss: 2.7747\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8601 - val_loss: 2.7081\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8427 - val_loss: 2.7644\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8484 - val_loss: 2.7134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8486 - val_loss: 2.7758\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8589 - val_loss: 2.7193\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8550 - val_loss: 2.7730\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8564 - val_loss: 2.7100\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8444 - val_loss: 2.7605\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8434 - val_loss: 2.6987\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8316 - val_loss: 2.7519\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8336 - val_loss: 2.6916\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8238 - val_loss: 2.7474\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8274 - val_loss: 2.6859\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8175 - val_loss: 2.7408\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8192 - val_loss: 2.6770\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8074 - val_loss: 2.7292\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8066 - val_loss: 2.6619\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7904 - val_loss: 2.7029\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7811 - val_loss: 2.6384\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7642 - val_loss: 2.6642\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7477 - val_loss: 2.6118\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7329 - val_loss: 2.6289\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7193 - val_loss: 2.5959\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7118 - val_loss: 2.6112\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7053 - val_loss: 2.5883\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7023 - val_loss: 2.6011\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6974 - val_loss: 2.5838\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6967 - val_loss: 2.6008\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6971 - val_loss: 2.5888\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7006 - val_loss: 2.6031\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7006 - val_loss: 2.5933\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7037 - val_loss: 2.5997\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7006 - val_loss: 2.5936\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7010 - val_loss: 2.5921\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6967 - val_loss: 2.5929\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6989 - val_loss: 2.5914\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6983 - val_loss: 2.5987\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7022 - val_loss: 2.5947\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7051 - val_loss: 2.6100\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7107 - val_loss: 2.6033\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7165 - val_loss: 2.6225\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7211 - val_loss: 2.6158\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7309 - val_loss: 2.6417\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7387 - val_loss: 2.6422\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7604 - val_loss: 2.6819\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7764 - val_loss: 2.6725\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7965 - val_loss: 2.7277\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8192 - val_loss: 2.6763\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.8044 - val_loss: 2.7109\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7988 - val_loss: 2.6524\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7787 - val_loss: 2.6914\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7770 - val_loss: 2.6393\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7646 - val_loss: 2.6894\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7716 - val_loss: 2.6377\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7633 - val_loss: 2.6989\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7770 - val_loss: 2.6458\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7737 - val_loss: 2.7185\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7909 - val_loss: 2.6603\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7900 - val_loss: 2.7454\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8101 - val_loss: 2.6865\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8162 - val_loss: 2.7621\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8216 - val_loss: 2.6678\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7932 - val_loss: 2.7085\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7690 - val_loss: 2.6268\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7439 - val_loss: 2.6609\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7267 - val_loss: 2.6049\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7153 - val_loss: 2.6361\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7082 - val_loss: 2.5972\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7040 - val_loss: 2.6240\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6994 - val_loss: 2.5940\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6988 - val_loss: 2.6212\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6965 - val_loss: 2.5935\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6983 - val_loss: 2.6250\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6980 - val_loss: 2.5958\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7011 - val_loss: 2.6310\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7010 - val_loss: 2.5984\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7045 - val_loss: 2.6372\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7042 - val_loss: 2.6012\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7080 - val_loss: 2.6431\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7076 - val_loss: 2.6042\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7115 - val_loss: 2.6496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7117 - val_loss: 2.6082\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7163 - val_loss: 2.6568\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7168 - val_loss: 2.6122\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7215 - val_loss: 2.6637\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7216 - val_loss: 2.6162\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7261 - val_loss: 2.6730\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7283 - val_loss: 2.6242\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7367 - val_loss: 2.6902\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7431 - val_loss: 2.6364\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7514 - val_loss: 2.7086\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7599 - val_loss: 2.6499\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7670 - val_loss: 2.7264\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7768 - val_loss: 2.6628\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7819 - val_loss: 2.7515\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8013 - val_loss: 2.6823\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8041 - val_loss: 2.7624\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8127 - val_loss: 2.6851\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8066 - val_loss: 2.7552\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8059 - val_loss: 2.6722\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7918 - val_loss: 2.7374\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7875 - val_loss: 2.6535\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7705 - val_loss: 2.7142\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7630 - val_loss: 2.6346\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7477 - val_loss: 2.6845\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7348 - val_loss: 2.6128\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7197 - val_loss: 2.6451\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7013 - val_loss: 2.5909\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6889 - val_loss: 2.6185\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6825 - val_loss: 2.5859\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6791 - val_loss: 2.6069\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6747 - val_loss: 2.5850\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6756 - val_loss: 2.6035\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6719 - val_loss: 2.5846\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6742 - val_loss: 2.6043\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6717 - val_loss: 2.5857\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6751 - val_loss: 2.6076\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6731 - val_loss: 2.5876\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6770 - val_loss: 2.6108\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6740 - val_loss: 2.5879\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6773 - val_loss: 2.6147\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6752 - val_loss: 2.5887\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6787 - val_loss: 2.6205\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6780 - val_loss: 2.5903\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6816 - val_loss: 2.6284\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6826 - val_loss: 2.5937\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6868 - val_loss: 2.6394\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6898 - val_loss: 2.5980\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6935 - val_loss: 2.6509\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6978 - val_loss: 2.6029\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7006 - val_loss: 2.6616\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7056 - val_loss: 2.6087\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7085 - val_loss: 2.6719\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7139 - val_loss: 2.6154\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7168 - val_loss: 2.6813\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7218 - val_loss: 2.6209\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7234 - val_loss: 2.6872\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7269 - val_loss: 2.6234\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7259 - val_loss: 2.6881\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7275 - val_loss: 2.6226\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7244 - val_loss: 2.6846\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7240 - val_loss: 2.6182\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7184 - val_loss: 2.6758\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7156 - val_loss: 2.6107\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7085 - val_loss: 2.6636\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7042 - val_loss: 2.6015\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6960 - val_loss: 2.6493\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6917 - val_loss: 2.5936\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6844 - val_loss: 2.6396\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6832 - val_loss: 2.5906\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6797 - val_loss: 2.6371\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6813 - val_loss: 2.5902\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6783 - val_loss: 2.6355\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6796 - val_loss: 2.5893\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6764 - val_loss: 2.6348\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6784 - val_loss: 2.5891\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6758 - val_loss: 2.6359\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6786 - val_loss: 2.5896\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6762 - val_loss: 2.6379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6795 - val_loss: 2.5904\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6771 - val_loss: 2.6405\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6809 - val_loss: 2.5916\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6785 - val_loss: 2.6439\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6829 - val_loss: 2.5933\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6807 - val_loss: 2.6478\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6855 - val_loss: 2.5954\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6834 - val_loss: 2.6521\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6885 - val_loss: 2.5979\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6864 - val_loss: 2.6563\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6917 - val_loss: 2.6007\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6899 - val_loss: 2.6605\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6949 - val_loss: 2.6035\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6934 - val_loss: 2.6637\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6973 - val_loss: 2.6051\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6949 - val_loss: 2.6648\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6977 - val_loss: 2.6053\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6945 - val_loss: 2.6644\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6968 - val_loss: 2.6021\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6886 - val_loss: 2.6535\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6862 - val_loss: 2.5934\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6779 - val_loss: 2.6473\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6805 - val_loss: 2.5928\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6755 - val_loss: 2.6463\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6797 - val_loss: 2.5930\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6750 - val_loss: 2.6442\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6781 - val_loss: 2.5923\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6734 - val_loss: 2.6409\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6750 - val_loss: 2.5893\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6681 - val_loss: 2.6360\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6700 - val_loss: 2.5866\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6637 - val_loss: 2.6337\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6675 - val_loss: 2.5862\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6623 - val_loss: 2.6335\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6669 - val_loss: 2.5869\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6625 - val_loss: 2.6343\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6671 - val_loss: 2.5879\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6631 - val_loss: 2.6357\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6677 - val_loss: 2.5890\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6641 - val_loss: 2.6381\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6691 - val_loss: 2.5914\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6670 - val_loss: 2.6431\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6728 - val_loss: 2.5945\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6707 - val_loss: 2.6490\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6770 - val_loss: 2.5975\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6749 - val_loss: 2.6556\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6822 - val_loss: 2.6013\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6799 - val_loss: 2.6611\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6867 - val_loss: 2.6042\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6834 - val_loss: 2.6640\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6887 - val_loss: 2.6050\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6838 - val_loss: 2.6630\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6871 - val_loss: 2.6025\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6797 - val_loss: 2.6573\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6812 - val_loss: 2.5975\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6723 - val_loss: 2.6502\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6746 - val_loss: 2.5937\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 0s 372us/step - loss: 56.5391 - val_loss: 52.9482\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 53.9457 - val_loss: 48.7728\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 49.7656 - val_loss: 42.8729\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 43.8531 - val_loss: 35.0614\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 36.0032 - val_loss: 26.3826\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 27.1514 - val_loss: 20.1810\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.4540 - val_loss: 17.7202\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.6772 - val_loss: 16.8101\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.6352 - val_loss: 16.5719\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 16.3390 - val_loss: 16.4681\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.2235 - val_loss: 16.2323\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.9968 - val_loss: 15.7762\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.5626 - val_loss: 15.0458\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.8672 - val_loss: 13.9554\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 13.8237 - val_loss: 12.5609\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.4896 - val_loss: 11.2539\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 11.2964 - val_loss: 10.0729\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.2148 - val_loss: 9.0832\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.2814 - val_loss: 8.6050\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.8173 - val_loss: 8.4892\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6630 - val_loss: 8.4612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.6033 - val_loss: 8.4618\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5515 - val_loss: 8.4715\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5284 - val_loss: 8.4359\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5006 - val_loss: 8.3863\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4675 - val_loss: 8.3438\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4351 - val_loss: 8.3013\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4037 - val_loss: 8.2583\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3714 - val_loss: 8.2188\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3374 - val_loss: 8.1779\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2995 - val_loss: 8.1317\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2562 - val_loss: 8.0806\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2067 - val_loss: 8.0237\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1498 - val_loss: 7.9584\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.0841 - val_loss: 7.8847\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0088 - val_loss: 7.8011\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9230 - val_loss: 7.7069\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8261 - val_loss: 7.6023\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.7183 - val_loss: 7.4877\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6004 - val_loss: 7.3660\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.4738 - val_loss: 7.2343\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.3401 - val_loss: 7.0983\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.2003 - val_loss: 6.9561\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.0557 - val_loss: 6.8117\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.9075 - val_loss: 6.6633\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.7563 - val_loss: 6.5111\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.6030 - val_loss: 6.3542\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.4476 - val_loss: 6.1928\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 6.2902 - val_loss: 6.0264\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.1302 - val_loss: 5.8566\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.9675 - val_loss: 5.6835\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 5.8020 - val_loss: 5.5089\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.6346 - val_loss: 5.3407\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.4669 - val_loss: 5.1752\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.3024 - val_loss: 5.0206\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.1461 - val_loss: 4.8748\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.0009 - val_loss: 4.7498\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.8682 - val_loss: 4.6295\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.7501 - val_loss: 4.5386\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.6474 - val_loss: 4.4355\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.5593 - val_loss: 4.3943\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.4903 - val_loss: 4.3154\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.4575 - val_loss: 4.4372\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.4977 - val_loss: 4.5593\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.7328 - val_loss: 4.8838\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.9069 - val_loss: 4.9619\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 5.1522 - val_loss: 4.4731\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.5055 - val_loss: 4.0520\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.2111 - val_loss: 4.0687\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.1186 - val_loss: 3.9441\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.0982 - val_loss: 4.1270\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.1368 - val_loss: 4.1070\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 4.2965 - val_loss: 4.3993\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.3787 - val_loss: 4.4842\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.6779 - val_loss: 4.2652\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.2335 - val_loss: 3.9307\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.1240 - val_loss: 4.0142\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0056 - val_loss: 3.9761\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 4.1508 - val_loss: 4.1331\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0910 - val_loss: 4.0404\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2171 - val_loss: 4.0381\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.9970 - val_loss: 3.8825\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0416 - val_loss: 3.9246\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8896 - val_loss: 3.8109\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.9600 - val_loss: 3.9018\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.8571 - val_loss: 3.8029\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.9368 - val_loss: 3.8583\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.8068 - val_loss: 3.7291\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.8512 - val_loss: 3.7973\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7429 - val_loss: 3.6777\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7838 - val_loss: 3.7600\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6968 - val_loss: 3.6418\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.7338 - val_loss: 3.7190\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6481 - val_loss: 3.5946\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6733 - val_loss: 3.6731\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.5949 - val_loss: 3.5523\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.6177 - val_loss: 3.6436\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.5582 - val_loss: 3.5297\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5832 - val_loss: 3.6218\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5283 - val_loss: 3.5033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5453 - val_loss: 3.5902\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4892 - val_loss: 3.4635\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4943 - val_loss: 3.5578\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4514 - val_loss: 3.4251\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4446 - val_loss: 3.5171\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4053 - val_loss: 3.3894\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3969 - val_loss: 3.4802\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3649 - val_loss: 3.3661\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3651 - val_loss: 3.4622\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3406 - val_loss: 3.3607\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3504 - val_loss: 3.4609\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3300 - val_loss: 3.3595\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3409 - val_loss: 3.4528\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3145 - val_loss: 3.3477\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3209 - val_loss: 3.4318\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2884 - val_loss: 3.3303\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2955 - val_loss: 3.4136\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2658 - val_loss: 3.3177\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2757 - val_loss: 3.4003\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2479 - val_loss: 3.3011\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2532 - val_loss: 3.3811\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2255 - val_loss: 3.2763\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2233 - val_loss: 3.3603\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2028 - val_loss: 3.2653\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2054 - val_loss: 3.3529\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1908 - val_loss: 3.2636\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1973 - val_loss: 3.3536\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1857 - val_loss: 3.2661\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1938 - val_loss: 3.3531\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1801 - val_loss: 3.2651\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1870 - val_loss: 3.3474\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1703 - val_loss: 3.2597\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1757 - val_loss: 3.3388\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1584 - val_loss: 3.2536\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1641 - val_loss: 3.3331\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1493 - val_loss: 3.2528\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1575 - val_loss: 3.3328\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1451 - val_loss: 3.2567\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1555 - val_loss: 3.3354\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1441 - val_loss: 3.2625\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1550 - val_loss: 3.3380\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1436 - val_loss: 3.2671\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1534 - val_loss: 3.3365\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1401 - val_loss: 3.2658\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1467 - val_loss: 3.3289\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1311 - val_loss: 3.2572\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1338 - val_loss: 3.3142\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1157 - val_loss: 3.2425\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1156 - val_loss: 3.2942\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0968 - val_loss: 3.2275\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0968 - val_loss: 3.2794\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0822 - val_loss: 3.2190\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0827 - val_loss: 3.2713\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0730 - val_loss: 3.2101\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0712 - val_loss: 3.2629\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0638 - val_loss: 3.2062\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0640 - val_loss: 3.2568\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0559 - val_loss: 3.2004\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0557 - val_loss: 3.2496\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0482 - val_loss: 3.1971\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0488 - val_loss: 3.2468\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0439 - val_loss: 3.1985\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0464 - val_loss: 3.2490\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0445 - val_loss: 3.2051\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0488 - val_loss: 3.2563\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0500 - val_loss: 3.2199\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0587 - val_loss: 3.2754\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0689 - val_loss: 3.2689\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0973 - val_loss: 3.3150\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1102 - val_loss: 3.3055\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1226 - val_loss: 3.3143\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1120 - val_loss: 3.2713\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.0866 - val_loss: 3.2407\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0350 - val_loss: 3.1709\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9987 - val_loss: 3.1679\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9674 - val_loss: 3.0922\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9471 - val_loss: 3.1209\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9246 - val_loss: 3.0490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.9054 - val_loss: 3.0808\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8966 - val_loss: 3.0381\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8854 - val_loss: 3.0606\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8789 - val_loss: 3.0329\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8715 - val_loss: 3.0419\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8665 - val_loss: 3.0325\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8625 - val_loss: 3.0355\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8591 - val_loss: 3.0307\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8560 - val_loss: 3.0326\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8531 - val_loss: 3.0279\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8502 - val_loss: 3.0313\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8476 - val_loss: 3.0253\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8452 - val_loss: 3.0312\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8434 - val_loss: 3.0259\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8425 - val_loss: 3.0419\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8495 - val_loss: 3.0549\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8584 - val_loss: 3.0738\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8728 - val_loss: 3.1532\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9290 - val_loss: 3.2178\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0336 - val_loss: 3.4903\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2598 - val_loss: 3.6191\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5178 - val_loss: 3.5218\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2466 - val_loss: 3.2323\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0341 - val_loss: 3.1210\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8922 - val_loss: 3.0601\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8516 - val_loss: 3.0273\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8399 - val_loss: 3.0502\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8335 - val_loss: 3.0115\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8258 - val_loss: 3.0270\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.8204 - val_loss: 2.9992\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8181 - val_loss: 3.0218\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8159 - val_loss: 3.0068\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8203 - val_loss: 3.0245\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8170 - val_loss: 3.0118\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8190 - val_loss: 3.0236\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8159 - val_loss: 3.0122\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8147 - val_loss: 3.0193\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8126 - val_loss: 3.0156\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8140 - val_loss: 3.0232\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8142 - val_loss: 3.0231\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8192 - val_loss: 3.0486\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8325 - val_loss: 3.0545\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8513 - val_loss: 3.1476\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9111 - val_loss: 3.2091\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0415 - val_loss: 3.4509\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2058 - val_loss: 3.4176\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3036 - val_loss: 3.4065\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 3.1212 - val_loss: 3.1603\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9875 - val_loss: 3.1107\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8588 - val_loss: 3.0201\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8079 - val_loss: 3.0006\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7878 - val_loss: 3.0017\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7812 - val_loss: 2.9809\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7758 - val_loss: 2.9878\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7746 - val_loss: 2.9758\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7732 - val_loss: 2.9826\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7749 - val_loss: 2.9793\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7709 - val_loss: 2.9778\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7716 - val_loss: 2.9789\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7666 - val_loss: 2.9725\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7663 - val_loss: 2.9778\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7631 - val_loss: 2.9703\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7634 - val_loss: 2.9795\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7614 - val_loss: 2.9693\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7626 - val_loss: 2.9835\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7617 - val_loss: 2.9701\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7646 - val_loss: 2.9906\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7641 - val_loss: 2.9733\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7704 - val_loss: 3.0173\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7822 - val_loss: 2.9998\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8124 - val_loss: 3.1623\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9039 - val_loss: 3.2133\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1099 - val_loss: 3.5053\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2302 - val_loss: 3.2603\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1452 - val_loss: 3.1756\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9047 - val_loss: 3.0021\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7815 - val_loss: 2.9751\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7540 - val_loss: 2.9815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7463 - val_loss: 2.9440\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7453 - val_loss: 2.9783\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7425 - val_loss: 2.9362\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7392 - val_loss: 2.9632\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7356 - val_loss: 2.9317\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7323 - val_loss: 2.9581\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7303 - val_loss: 2.9336\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7280 - val_loss: 2.9566\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7275 - val_loss: 2.9324\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7259 - val_loss: 2.9569\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7254 - val_loss: 2.9316\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7239 - val_loss: 2.9563\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7234 - val_loss: 2.9306\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7220 - val_loss: 2.9551\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7217 - val_loss: 2.9298\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7204 - val_loss: 2.9553\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7210 - val_loss: 2.9303\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7205 - val_loss: 2.9577\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7228 - val_loss: 2.9364\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7244 - val_loss: 2.9612\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7291 - val_loss: 2.9611\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7413 - val_loss: 2.9780\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7623 - val_loss: 3.0339\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8035 - val_loss: 3.0556\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8810 - val_loss: 3.2749\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0272 - val_loss: 3.2618\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1457 - val_loss: 3.3851\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1033 - val_loss: 3.0945\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9566 - val_loss: 3.0822\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8009 - val_loss: 2.9307\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7299 - val_loss: 2.9657\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7115 - val_loss: 2.9069\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7042 - val_loss: 2.9425\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6996 - val_loss: 2.8956\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6952 - val_loss: 2.9258\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6932 - val_loss: 2.8953\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6937 - val_loss: 2.9305\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6936 - val_loss: 2.9045\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6969 - val_loss: 2.9375\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6989 - val_loss: 2.9131\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.7019 - val_loss: 2.9398\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7045 - val_loss: 2.9255\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7078 - val_loss: 2.9403\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7104 - val_loss: 2.9475\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7220 - val_loss: 2.9496\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7380 - val_loss: 2.9878\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7571 - val_loss: 2.9874\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7981 - val_loss: 3.0971\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8520 - val_loss: 3.0551\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8962 - val_loss: 3.2293\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9678 - val_loss: 3.0926\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9530 - val_loss: 3.1579\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8796 - val_loss: 2.9546\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7840 - val_loss: 2.9896\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7267 - val_loss: 2.8922\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6950 - val_loss: 2.9285\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6784 - val_loss: 2.8724\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6746 - val_loss: 2.9204\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6725 - val_loss: 2.8679\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6709 - val_loss: 2.9161\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6696 - val_loss: 2.8675\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6662 - val_loss: 2.9137\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6655 - val_loss: 2.8665\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6652 - val_loss: 2.9148\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6659 - val_loss: 2.8675\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6658 - val_loss: 2.9144\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6654 - val_loss: 2.8703\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6652 - val_loss: 2.9100\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6644 - val_loss: 2.8757\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6653 - val_loss: 2.9037\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6648 - val_loss: 2.8899\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6695 - val_loss: 2.9068\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6762 - val_loss: 2.9179\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6870 - val_loss: 2.9205\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7044 - val_loss: 2.9524\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7172 - val_loss: 2.9486\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7532 - val_loss: 3.0161\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7791 - val_loss: 2.9821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7980 - val_loss: 3.0901\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8439 - val_loss: 3.0136\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8531 - val_loss: 3.1189\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8594 - val_loss: 2.9886\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8342 - val_loss: 3.0605\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7919 - val_loss: 2.9054\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7242 - val_loss: 2.9376\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6789 - val_loss: 2.8544\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6516 - val_loss: 2.8874\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6374 - val_loss: 2.8430\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6345 - val_loss: 2.8851\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6344 - val_loss: 2.8410\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6349 - val_loss: 2.8914\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6362 - val_loss: 2.8421\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6413 - val_loss: 2.9051\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6447 - val_loss: 2.8452\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6505 - val_loss: 2.9196\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6538 - val_loss: 2.8449\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6543 - val_loss: 2.9216\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6522 - val_loss: 2.8436\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6521 - val_loss: 2.9202\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6491 - val_loss: 2.8419\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6512 - val_loss: 2.9203\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6484 - val_loss: 2.8418\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6515 - val_loss: 2.9222\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6487 - val_loss: 2.8413\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6514 - val_loss: 2.9233\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6486 - val_loss: 2.8407\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6510 - val_loss: 2.9239\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6481 - val_loss: 2.8405\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6511 - val_loss: 2.9245\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6481 - val_loss: 2.8418\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6529 - val_loss: 2.9294\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6515 - val_loss: 2.8463\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6591 - val_loss: 2.9383\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6596 - val_loss: 2.8540\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6685 - val_loss: 2.9457\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6683 - val_loss: 2.8619\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6804 - val_loss: 2.9534\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6778 - val_loss: 2.8729\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6959 - val_loss: 2.9794\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7035 - val_loss: 2.8850\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7140 - val_loss: 2.9941\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7180 - val_loss: 2.8857\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7155 - val_loss: 2.9733\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7010 - val_loss: 2.8725\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6932 - val_loss: 2.9364\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6694 - val_loss: 2.8501\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6537 - val_loss: 2.8724\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6215 - val_loss: 2.8301\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6091 - val_loss: 2.8414\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6039 - val_loss: 2.8522\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6087 - val_loss: 2.8466\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6216 - val_loss: 2.8813\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6390 - val_loss: 2.8571\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6347 - val_loss: 2.8758\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6410 - val_loss: 2.8544\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6285 - val_loss: 2.8615\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6346 - val_loss: 2.8565\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6278 - val_loss: 2.8631\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6389 - val_loss: 2.8630\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6328 - val_loss: 2.8657\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6447 - val_loss: 2.8714\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6395 - val_loss: 2.8670\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6506 - val_loss: 2.8789\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6459 - val_loss: 2.8688\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.6553 - val_loss: 2.8809\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6482 - val_loss: 2.8689\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6558 - val_loss: 2.8794\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6467 - val_loss: 2.8670\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6534 - val_loss: 2.8762\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6434 - val_loss: 2.8638\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6496 - val_loss: 2.8725\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6395 - val_loss: 2.8606\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6456 - val_loss: 2.8688\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6356 - val_loss: 2.8578\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6419 - val_loss: 2.8648\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6316 - val_loss: 2.8551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6381 - val_loss: 2.8599\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6268 - val_loss: 2.8524\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6332 - val_loss: 2.8544\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6216 - val_loss: 2.8495\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6277 - val_loss: 2.8500\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6165 - val_loss: 2.8459\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6226 - val_loss: 2.8476\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6130 - val_loss: 2.8434\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6197 - val_loss: 2.8472\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6117 - val_loss: 2.8425\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6196 - val_loss: 2.8483\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6122 - val_loss: 2.8427\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6212 - val_loss: 2.8507\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6140 - val_loss: 2.8437\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6238 - val_loss: 2.8533\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6165 - val_loss: 2.8448\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6261 - val_loss: 2.8559\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6188 - val_loss: 2.8455\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6291 - val_loss: 2.8622\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6242 - val_loss: 2.8499\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6378 - val_loss: 2.8732\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6353 - val_loss: 2.8573\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6503 - val_loss: 2.8849\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6466 - val_loss: 2.8611\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6590 - val_loss: 2.8904\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6520 - val_loss: 2.8606\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6595 - val_loss: 2.8867\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6491 - val_loss: 2.8567\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6534 - val_loss: 2.8752\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6376 - val_loss: 2.8473\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6395 - val_loss: 2.8626\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6246 - val_loss: 2.8411\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6266 - val_loss: 2.8432\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6088 - val_loss: 2.8327\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6067 - val_loss: 2.8222\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5911 - val_loss: 2.8281\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5912 - val_loss: 2.8123\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5810 - val_loss: 2.8316\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5856 - val_loss: 2.8107\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5811 - val_loss: 2.8360\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5874 - val_loss: 2.8111\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5822 - val_loss: 2.8351\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5876 - val_loss: 2.8120\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5815 - val_loss: 2.8317\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5879 - val_loss: 2.8146\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5819 - val_loss: 2.8296\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5906 - val_loss: 2.8186\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5840 - val_loss: 2.8290\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5947 - val_loss: 2.8235\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5879 - val_loss: 2.8284\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6002 - val_loss: 2.8323\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5952 - val_loss: 2.8304\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6080 - val_loss: 2.8444\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6050 - val_loss: 2.8342\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6174 - val_loss: 2.8590\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6173 - val_loss: 2.8443\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6359 - val_loss: 2.8790\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6348 - val_loss: 2.8522\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6513 - val_loss: 2.8883\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6457 - val_loss: 2.8595\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6598 - val_loss: 2.8874\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6480 - val_loss: 2.8584\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6551 - val_loss: 2.8755\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.6364 - val_loss: 2.8483\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6375 - val_loss: 2.8572\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6179 - val_loss: 2.8370\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6198 - val_loss: 2.8384\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5990 - val_loss: 2.8206\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5968 - val_loss: 2.8139\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5772 - val_loss: 2.8123\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5786 - val_loss: 2.8038\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5670 - val_loss: 2.8148\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5696 - val_loss: 2.7955\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5626 - val_loss: 2.8212\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5671 - val_loss: 2.7921\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5616 - val_loss: 2.8248\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5647 - val_loss: 2.7875\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5603 - val_loss: 2.8244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5628 - val_loss: 2.7861\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5593 - val_loss: 2.8235\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5620 - val_loss: 2.7870\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5604 - val_loss: 2.8237\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5655 - val_loss: 2.7907\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5634 - val_loss: 2.8237\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5714 - val_loss: 2.7980\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5673 - val_loss: 2.8229\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5789 - val_loss: 2.8093\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5747 - val_loss: 2.8234\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5900 - val_loss: 2.8251\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5855 - val_loss: 2.8246\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6006 - val_loss: 2.8423\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5978 - val_loss: 2.8319\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6183 - val_loss: 2.8651\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6177 - val_loss: 2.8442\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6404 - val_loss: 2.8831\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6350 - val_loss: 2.8502\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6496 - val_loss: 2.8811\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6345 - val_loss: 2.8456\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6417 - val_loss: 2.8648\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6188 - val_loss: 2.8336\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6218 - val_loss: 2.8455\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5998 - val_loss: 2.8229\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6027 - val_loss: 2.8270\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5831 - val_loss: 2.8139\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5858 - val_loss: 2.8108\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5691 - val_loss: 2.8078\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5724 - val_loss: 2.7997\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5599 - val_loss: 2.8080\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5658 - val_loss: 2.7935\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5571 - val_loss: 2.8128\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5651 - val_loss: 2.7911\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5566 - val_loss: 2.8151\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5649 - val_loss: 2.7904\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5557 - val_loss: 2.8137\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5636 - val_loss: 2.7905\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5545 - val_loss: 2.8118\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5631 - val_loss: 2.7917\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5543 - val_loss: 2.8111\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5641 - val_loss: 2.7940\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 2.5552 - val_loss: 2.8110\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5658 - val_loss: 2.7966\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5563 - val_loss: 2.8106\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5673 - val_loss: 2.7998\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 2.5574 - val_loss: 2.8090\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5694 - val_loss: 2.8067\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5616 - val_loss: 2.8137\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5803 - val_loss: 2.8253\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5756 - val_loss: 2.8193\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5963 - val_loss: 2.8411\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5882 - val_loss: 2.8226\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6062 - val_loss: 2.8532\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5976 - val_loss: 2.8247\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6150 - val_loss: 2.8576\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6025 - val_loss: 2.8265\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6158 - val_loss: 2.8515\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5971 - val_loss: 2.8207\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6051 - val_loss: 2.8381\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5852 - val_loss: 2.8147\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5922 - val_loss: 2.8225\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5709 - val_loss: 2.8059\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5745 - val_loss: 2.8060\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5564 - val_loss: 2.8022\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5619 - val_loss: 2.7962\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5484 - val_loss: 2.7988\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5544 - val_loss: 2.7871\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5423 - val_loss: 2.8031\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5509 - val_loss: 2.7826\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5400 - val_loss: 2.8033\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5475 - val_loss: 2.7768\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5365 - val_loss: 2.8012\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5424 - val_loss: 2.7755\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5339 - val_loss: 2.7973\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5414 - val_loss: 2.7771\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5333 - val_loss: 2.7950\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5410 - val_loss: 2.7795\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5332 - val_loss: 2.7948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5426 - val_loss: 2.7803\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5340 - val_loss: 2.7965\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5451 - val_loss: 2.7853\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5369 - val_loss: 2.8007\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5512 - val_loss: 2.7918\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5429 - val_loss: 2.8067\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5599 - val_loss: 2.8029\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5506 - val_loss: 2.8052\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5665 - val_loss: 2.8144\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5576 - val_loss: 2.8061\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5776 - val_loss: 2.8283\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5680 - val_loss: 2.8090\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5866 - val_loss: 2.8401\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5781 - val_loss: 2.8131\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5946 - val_loss: 2.8433\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5813 - val_loss: 2.8114\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5934 - val_loss: 2.8371\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5759 - val_loss: 2.8094\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5867 - val_loss: 2.8268\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5671 - val_loss: 2.8066\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5761 - val_loss: 2.8123\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5538 - val_loss: 2.7978\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5587 - val_loss: 2.7964\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5396 - val_loss: 2.7943\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5469 - val_loss: 2.7873\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5332 - val_loss: 2.7962\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5433 - val_loss: 2.7821\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5291 - val_loss: 2.7935\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5372 - val_loss: 2.7751\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5235 - val_loss: 2.7866\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5289 - val_loss: 2.7700\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5185 - val_loss: 2.7803\n",
      "Epoch 1/600\n",
      "1295/1295 [==============================] - 0s 228us/step - loss: 56.7366\n",
      "Epoch 2/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 49.0682\n",
      "Epoch 3/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 33.9580\n",
      "Epoch 4/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 19.3417\n",
      "Epoch 5/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 16.6214\n",
      "Epoch 6/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 16.2791\n",
      "Epoch 7/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 15.3398\n",
      "Epoch 8/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 13.4858\n",
      "Epoch 9/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 10.8902\n",
      "Epoch 10/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 9.0367\n",
      "Epoch 11/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 8.7159\n",
      "Epoch 12/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 8.6544\n",
      "Epoch 13/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.5363\n",
      "Epoch 14/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.4836\n",
      "Epoch 15/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.3664\n",
      "Epoch 16/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.2393\n",
      "Epoch 17/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.1135\n",
      "Epoch 18/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 7.9438\n",
      "Epoch 19/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 7.7699\n",
      "Epoch 20/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 7.5536\n",
      "Epoch 21/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 7.3470\n",
      "Epoch 22/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 7.1274\n",
      "Epoch 23/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 6.9110\n",
      "Epoch 24/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 6.7955\n",
      "Epoch 25/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 6.4730\n",
      "Epoch 26/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 6.3198\n",
      "Epoch 27/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 6.0959\n",
      "Epoch 28/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 5.8650\n",
      "Epoch 29/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 5.7963\n",
      "Epoch 30/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 5.5279\n",
      "Epoch 31/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 5.3019\n",
      "Epoch 32/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 5.1586\n",
      "Epoch 33/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 4.7780\n",
      "Epoch 34/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 4.4933\n",
      "Epoch 35/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 4.3741\n",
      "Epoch 36/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 4.1411\n",
      "Epoch 37/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 4.0732\n",
      "Epoch 38/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 4.4241\n",
      "Epoch 39/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 4.6383\n",
      "Epoch 40/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.7616\n",
      "Epoch 41/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 4.2496\n",
      "Epoch 42/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 4.1027\n",
      "Epoch 43/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.9236\n",
      "Epoch 44/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.7270\n",
      "Epoch 45/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.6257\n",
      "Epoch 46/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.6546\n",
      "Epoch 47/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.5061\n",
      "Epoch 48/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.4908\n",
      "Epoch 49/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 4.0093\n",
      "Epoch 50/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.6842\n",
      "Epoch 51/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2601\n",
      "Epoch 52/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.3753\n",
      "Epoch 53/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.6030\n",
      "Epoch 54/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.3275\n",
      "Epoch 55/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2757\n",
      "Epoch 56/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.4542\n",
      "Epoch 57/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2538\n",
      "Epoch 58/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.5163\n",
      "Epoch 59/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1479\n",
      "Epoch 60/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1166\n",
      "Epoch 61/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1131\n",
      "Epoch 62/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.1082\n",
      "Epoch 63/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.1747\n",
      "Epoch 64/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.2306\n",
      "Epoch 65/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.3874\n",
      "Epoch 66/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.3552\n",
      "Epoch 67/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0487\n",
      "Epoch 68/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0477\n",
      "Epoch 69/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0445\n",
      "Epoch 70/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0343\n",
      "Epoch 71/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9994\n",
      "Epoch 72/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0140\n",
      "Epoch 73/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0722\n",
      "Epoch 74/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1991\n",
      "Epoch 75/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.2328\n",
      "Epoch 76/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0303\n",
      "Epoch 77/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.4423\n",
      "Epoch 78/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.4488\n",
      "Epoch 79/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0176\n",
      "Epoch 80/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.1525\n",
      "Epoch 81/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0976\n",
      "Epoch 82/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9443\n",
      "Epoch 83/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0205\n",
      "Epoch 84/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9396\n",
      "Epoch 85/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0988\n",
      "Epoch 86/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0996\n",
      "Epoch 87/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2611\n",
      "Epoch 88/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9514\n",
      "Epoch 89/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8879\n",
      "Epoch 90/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9572\n",
      "Epoch 91/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9025\n",
      "Epoch 92/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9408\n",
      "Epoch 93/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.4139\n",
      "Epoch 94/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.0705\n",
      "Epoch 95/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.1174\n",
      "Epoch 96/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.8909\n",
      "Epoch 97/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9965\n",
      "Epoch 98/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9746\n",
      "Epoch 99/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.8768\n",
      "Epoch 100/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.9583\n",
      "Epoch 101/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 3.0071\n",
      "Epoch 102/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 3.1581\n",
      "Epoch 103/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8999\n",
      "Epoch 104/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9102\n",
      "Epoch 105/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.1690\n",
      "Epoch 106/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9678\n",
      "Epoch 107/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9243\n",
      "Epoch 108/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9765\n",
      "Epoch 109/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8931\n",
      "Epoch 110/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8254\n",
      "Epoch 111/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9479\n",
      "Epoch 112/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9634\n",
      "Epoch 113/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8461\n",
      "Epoch 114/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8031\n",
      "Epoch 115/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8309\n",
      "Epoch 116/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.2222\n",
      "Epoch 117/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0839\n",
      "Epoch 118/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8808\n",
      "Epoch 119/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8660\n",
      "Epoch 120/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8842\n",
      "Epoch 121/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7973\n",
      "Epoch 122/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8065\n",
      "Epoch 123/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9593\n",
      "Epoch 124/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8343\n",
      "Epoch 125/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9684\n",
      "Epoch 126/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8716\n",
      "Epoch 127/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7789\n",
      "Epoch 128/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8151\n",
      "Epoch 129/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8982\n",
      "Epoch 130/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0111\n",
      "Epoch 131/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.8325\n",
      "Epoch 132/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7801\n",
      "Epoch 133/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.8190\n",
      "Epoch 134/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9439\n",
      "Epoch 135/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7790\n",
      "Epoch 136/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8917\n",
      "Epoch 137/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8461\n",
      "Epoch 138/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7847\n",
      "Epoch 139/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7608\n",
      "Epoch 140/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7570\n",
      "Epoch 141/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7417\n",
      "Epoch 142/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7888\n",
      "Epoch 143/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.9995\n",
      "Epoch 144/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8639\n",
      "Epoch 145/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7685\n",
      "Epoch 146/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8846\n",
      "Epoch 147/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7513\n",
      "Epoch 148/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7507\n",
      "Epoch 149/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7230\n",
      "Epoch 150/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7932\n",
      "Epoch 151/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7950\n",
      "Epoch 152/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9596\n",
      "Epoch 153/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8233\n",
      "Epoch 154/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7834\n",
      "Epoch 155/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8750\n",
      "Epoch 156/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8296\n",
      "Epoch 157/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8421\n",
      "Epoch 158/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.8397\n",
      "Epoch 159/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8895\n",
      "Epoch 160/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0543\n",
      "Epoch 161/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7135\n",
      "Epoch 162/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7371\n",
      "Epoch 163/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7718\n",
      "Epoch 164/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8079\n",
      "Epoch 165/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8069\n",
      "Epoch 166/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8850\n",
      "Epoch 167/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7337\n",
      "Epoch 168/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7115\n",
      "Epoch 169/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6939\n",
      "Epoch 170/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7242\n",
      "Epoch 171/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7861\n",
      "Epoch 172/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0609\n",
      "Epoch 173/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9151\n",
      "Epoch 174/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8388\n",
      "Epoch 175/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7126\n",
      "Epoch 176/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7934\n",
      "Epoch 177/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8639\n",
      "Epoch 178/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7123\n",
      "Epoch 179/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7121\n",
      "Epoch 180/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6927\n",
      "Epoch 181/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7682\n",
      "Epoch 182/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7208\n",
      "Epoch 183/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7137\n",
      "Epoch 184/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6606\n",
      "Epoch 185/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6942\n",
      "Epoch 186/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8274\n",
      "Epoch 187/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6869\n",
      "Epoch 188/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7344\n",
      "Epoch 189/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6608\n",
      "Epoch 190/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6456\n",
      "Epoch 191/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6418\n",
      "Epoch 192/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8088\n",
      "Epoch 193/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9055\n",
      "Epoch 194/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7267\n",
      "Epoch 195/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7400\n",
      "Epoch 196/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7775\n",
      "Epoch 197/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7439\n",
      "Epoch 198/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8990\n",
      "Epoch 199/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.0006\n",
      "Epoch 200/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7313\n",
      "Epoch 201/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7880\n",
      "Epoch 202/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.8121\n",
      "Epoch 203/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6650\n",
      "Epoch 204/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8489\n",
      "Epoch 205/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8158\n",
      "Epoch 206/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8204\n",
      "Epoch 207/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7133\n",
      "Epoch 208/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7503\n",
      "Epoch 209/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.8934\n",
      "Epoch 210/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8052\n",
      "Epoch 211/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6529\n",
      "Epoch 212/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8013\n",
      "Epoch 213/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8830\n",
      "Epoch 214/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6757\n",
      "Epoch 215/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6489\n",
      "Epoch 216/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7810\n",
      "Epoch 217/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8060\n",
      "Epoch 218/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6577\n",
      "Epoch 219/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6462\n",
      "Epoch 220/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6102\n",
      "Epoch 221/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6033\n",
      "Epoch 222/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6418\n",
      "Epoch 223/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6368\n",
      "Epoch 224/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6711\n",
      "Epoch 225/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6108\n",
      "Epoch 226/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6828\n",
      "Epoch 227/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6217\n",
      "Epoch 228/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8080\n",
      "Epoch 229/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7935\n",
      "Epoch 230/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6254\n",
      "Epoch 231/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6054\n",
      "Epoch 232/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9281\n",
      "Epoch 233/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8590\n",
      "Epoch 234/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6518\n",
      "Epoch 235/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6335\n",
      "Epoch 236/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6325\n",
      "Epoch 237/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8324\n",
      "Epoch 238/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6336\n",
      "Epoch 239/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6290\n",
      "Epoch 240/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7450\n",
      "Epoch 241/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8130\n",
      "Epoch 242/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6236\n",
      "Epoch 243/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7627\n",
      "Epoch 244/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6508\n",
      "Epoch 245/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5746\n",
      "Epoch 246/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6633\n",
      "Epoch 247/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6406\n",
      "Epoch 248/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5885\n",
      "Epoch 249/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6000\n",
      "Epoch 250/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5934\n",
      "Epoch 251/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6215\n",
      "Epoch 252/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7000\n",
      "Epoch 253/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6053\n",
      "Epoch 254/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6610\n",
      "Epoch 255/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7693\n",
      "Epoch 256/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6198\n",
      "Epoch 257/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5960\n",
      "Epoch 258/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6967\n",
      "Epoch 259/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6740\n",
      "Epoch 260/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7103\n",
      "Epoch 261/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5957\n",
      "Epoch 262/600\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.5983\n",
      "Epoch 263/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5818\n",
      "Epoch 264/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6026\n",
      "Epoch 265/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6642\n",
      "Epoch 266/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7408\n",
      "Epoch 267/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6176\n",
      "Epoch 268/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7612\n",
      "Epoch 269/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6440\n",
      "Epoch 270/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5935\n",
      "Epoch 271/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7091\n",
      "Epoch 272/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6197\n",
      "Epoch 273/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6057\n",
      "Epoch 274/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5616\n",
      "Epoch 275/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5639\n",
      "Epoch 276/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6442\n",
      "Epoch 277/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6618\n",
      "Epoch 278/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.813 - 0s 8us/step - loss: 2.7874\n",
      "Epoch 279/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5988\n",
      "Epoch 280/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7652\n",
      "Epoch 281/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7103\n",
      "Epoch 282/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7387\n",
      "Epoch 283/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5912\n",
      "Epoch 284/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6576\n",
      "Epoch 285/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6258\n",
      "Epoch 286/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5446\n",
      "Epoch 287/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5400\n",
      "Epoch 288/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6293\n",
      "Epoch 289/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6804\n",
      "Epoch 290/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6154\n",
      "Epoch 291/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6709\n",
      "Epoch 292/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6474\n",
      "Epoch 293/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5460\n",
      "Epoch 294/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5841\n",
      "Epoch 295/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5751\n",
      "Epoch 296/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6868\n",
      "Epoch 297/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6785\n",
      "Epoch 298/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6198\n",
      "Epoch 299/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8361\n",
      "Epoch 300/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5789\n",
      "Epoch 301/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5601\n",
      "Epoch 302/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6548\n",
      "Epoch 303/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6340\n",
      "Epoch 304/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5678\n",
      "Epoch 305/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5384\n",
      "Epoch 306/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5509\n",
      "Epoch 307/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5509\n",
      "Epoch 308/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5752\n",
      "Epoch 309/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6937\n",
      "Epoch 310/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5914\n",
      "Epoch 311/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5350\n",
      "Epoch 312/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5704\n",
      "Epoch 313/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6504\n",
      "Epoch 314/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7107\n",
      "Epoch 315/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5579\n",
      "Epoch 316/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5378\n",
      "Epoch 317/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5268\n",
      "Epoch 318/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5785\n",
      "Epoch 319/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5550\n",
      "Epoch 320/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6110\n",
      "Epoch 321/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5655\n",
      "Epoch 322/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5729\n",
      "Epoch 323/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6095\n",
      "Epoch 324/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5173\n",
      "Epoch 325/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5205\n",
      "Epoch 326/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5842\n",
      "Epoch 327/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9117\n",
      "Epoch 328/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6418\n",
      "Epoch 329/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6575\n",
      "Epoch 330/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5557\n",
      "Epoch 331/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6925\n",
      "Epoch 332/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6469\n",
      "Epoch 333/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5481\n",
      "Epoch 334/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6003\n",
      "Epoch 335/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5539\n",
      "Epoch 336/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6079\n",
      "Epoch 337/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8845\n",
      "Epoch 338/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5778\n",
      "Epoch 339/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6223\n",
      "Epoch 340/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5163\n",
      "Epoch 341/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5323\n",
      "Epoch 342/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5037\n",
      "Epoch 343/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5184\n",
      "Epoch 344/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6594\n",
      "Epoch 345/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6383\n",
      "Epoch 346/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5372\n",
      "Epoch 347/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5406\n",
      "Epoch 348/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5328\n",
      "Epoch 349/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7026\n",
      "Epoch 350/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6441\n",
      "Epoch 351/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6304\n",
      "Epoch 352/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6794\n",
      "Epoch 353/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7729\n",
      "Epoch 354/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5191\n",
      "Epoch 355/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6266\n",
      "Epoch 356/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5437\n",
      "Epoch 357/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6364\n",
      "Epoch 358/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5764\n",
      "Epoch 359/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7335\n",
      "Epoch 360/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6589\n",
      "Epoch 361/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5683\n",
      "Epoch 362/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5031\n",
      "Epoch 363/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5387\n",
      "Epoch 364/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5232\n",
      "Epoch 365/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5861\n",
      "Epoch 366/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5166\n",
      "Epoch 367/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5205\n",
      "Epoch 368/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.600 - 0s 6us/step - loss: 2.6414\n",
      "Epoch 369/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5508\n",
      "Epoch 370/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5151\n",
      "Epoch 371/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6992\n",
      "Epoch 372/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5664\n",
      "Epoch 373/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6521\n",
      "Epoch 374/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5206\n",
      "Epoch 375/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5071\n",
      "Epoch 376/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5240\n",
      "Epoch 377/600\n",
      "1295/1295 [==============================] - 0s 42us/step - loss: 2.5701\n",
      "Epoch 378/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5818\n",
      "Epoch 379/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5023\n",
      "Epoch 380/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9680\n",
      "Epoch 381/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4948\n",
      "Epoch 382/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6304\n",
      "Epoch 383/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6111\n",
      "Epoch 384/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5209\n",
      "Epoch 385/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5109\n",
      "Epoch 386/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5771\n",
      "Epoch 387/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6876\n",
      "Epoch 388/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5752\n",
      "Epoch 389/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4894\n",
      "Epoch 390/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4902\n",
      "Epoch 391/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4848\n",
      "Epoch 392/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6209\n",
      "Epoch 393/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6133\n",
      "Epoch 394/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5483\n",
      "Epoch 395/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6281\n",
      "Epoch 396/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5745\n",
      "Epoch 397/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6751\n",
      "Epoch 398/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5974\n",
      "Epoch 399/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5640\n",
      "Epoch 400/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6244\n",
      "Epoch 401/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5565\n",
      "Epoch 402/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5350\n",
      "Epoch 403/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5138\n",
      "Epoch 404/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6234\n",
      "Epoch 405/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5874\n",
      "Epoch 406/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5538\n",
      "Epoch 407/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5479\n",
      "Epoch 408/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5930\n",
      "Epoch 409/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5277\n",
      "Epoch 410/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6041\n",
      "Epoch 411/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4966\n",
      "Epoch 412/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4599\n",
      "Epoch 413/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4664\n",
      "Epoch 414/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4713\n",
      "Epoch 415/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4875\n",
      "Epoch 416/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6728\n",
      "Epoch 417/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6674\n",
      "Epoch 418/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7752\n",
      "Epoch 419/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4900\n",
      "Epoch 420/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4590\n",
      "Epoch 421/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4895\n",
      "Epoch 422/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5524\n",
      "Epoch 423/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6853\n",
      "Epoch 424/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5027\n",
      "Epoch 425/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7239\n",
      "Epoch 426/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6012\n",
      "Epoch 427/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5306\n",
      "Epoch 428/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5084\n",
      "Epoch 429/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5283\n",
      "Epoch 430/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6537\n",
      "Epoch 431/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5223\n",
      "Epoch 432/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4882\n",
      "Epoch 433/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5016\n",
      "Epoch 434/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5897\n",
      "Epoch 435/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5128\n",
      "Epoch 436/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5229\n",
      "Epoch 438/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4843\n",
      "Epoch 439/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4835\n",
      "Epoch 440/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5873\n",
      "Epoch 441/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5488\n",
      "Epoch 442/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7300\n",
      "Epoch 443/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4631\n",
      "Epoch 444/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4838\n",
      "Epoch 445/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5107\n",
      "Epoch 446/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5150\n",
      "Epoch 447/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6684\n",
      "Epoch 448/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4893\n",
      "Epoch 449/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4649\n",
      "Epoch 450/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4429\n",
      "Epoch 451/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4999\n",
      "Epoch 452/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5985\n",
      "Epoch 453/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4991\n",
      "Epoch 454/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4867\n",
      "Epoch 455/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4430\n",
      "Epoch 456/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4453\n",
      "Epoch 457/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4387\n",
      "Epoch 458/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4742\n",
      "Epoch 459/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6539\n",
      "Epoch 460/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5346\n",
      "Epoch 461/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5229\n",
      "Epoch 462/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6061\n",
      "Epoch 463/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6133\n",
      "Epoch 464/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4511\n",
      "Epoch 465/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4764\n",
      "Epoch 466/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4452\n",
      "Epoch 467/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4894\n",
      "Epoch 468/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6833\n",
      "Epoch 469/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6470\n",
      "Epoch 470/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5706\n",
      "Epoch 471/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5267\n",
      "Epoch 472/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5029\n",
      "Epoch 473/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6365\n",
      "Epoch 474/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5754\n",
      "Epoch 475/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6333\n",
      "Epoch 476/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4413\n",
      "Epoch 477/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5509\n",
      "Epoch 478/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5146\n",
      "Epoch 479/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5784\n",
      "Epoch 480/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4853\n",
      "Epoch 481/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5206\n",
      "Epoch 482/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4820\n",
      "Epoch 483/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4686\n",
      "Epoch 484/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4590\n",
      "Epoch 485/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4691\n",
      "Epoch 486/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4834\n",
      "Epoch 487/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5746\n",
      "Epoch 488/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6972\n",
      "Epoch 489/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5158\n",
      "Epoch 490/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5362\n",
      "Epoch 491/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4717\n",
      "Epoch 492/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5039\n",
      "Epoch 493/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4576\n",
      "Epoch 494/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6482\n",
      "Epoch 495/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5673\n",
      "Epoch 496/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6062\n",
      "Epoch 497/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4446\n",
      "Epoch 498/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5842\n",
      "Epoch 499/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5524\n",
      "Epoch 500/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6087\n",
      "Epoch 501/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4332\n",
      "Epoch 502/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5310\n",
      "Epoch 503/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4661\n",
      "Epoch 504/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4787\n",
      "Epoch 505/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5030\n",
      "Epoch 506/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5342\n",
      "Epoch 507/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4998\n",
      "Epoch 508/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4911\n",
      "Epoch 509/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7195\n",
      "Epoch 510/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5104\n",
      "Epoch 511/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4762\n",
      "Epoch 512/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5305\n",
      "Epoch 513/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4177\n",
      "Epoch 514/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5986\n",
      "Epoch 515/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4631\n",
      "Epoch 516/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4376\n",
      "Epoch 517/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5290\n",
      "Epoch 518/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6600\n",
      "Epoch 519/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4961\n",
      "Epoch 520/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4346\n",
      "Epoch 521/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5884\n",
      "Epoch 522/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4979\n",
      "Epoch 523/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5772\n",
      "Epoch 524/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4544\n",
      "Epoch 525/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4215\n",
      "Epoch 526/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5174\n",
      "Epoch 527/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4663\n",
      "Epoch 528/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4986\n",
      "Epoch 529/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5679\n",
      "Epoch 530/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4393\n",
      "Epoch 531/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5647\n",
      "Epoch 532/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5506\n",
      "Epoch 533/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4479\n",
      "Epoch 534/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4408\n",
      "Epoch 535/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6308\n",
      "Epoch 536/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4700\n",
      "Epoch 537/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6227\n",
      "Epoch 538/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4826\n",
      "Epoch 539/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5099\n",
      "Epoch 540/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5398\n",
      "Epoch 541/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4377\n",
      "Epoch 542/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4507\n",
      "Epoch 543/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5543\n",
      "Epoch 544/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4086\n",
      "Epoch 545/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4228\n",
      "Epoch 546/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4077\n",
      "Epoch 547/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5284\n",
      "Epoch 548/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4365\n",
      "Epoch 549/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5365\n",
      "Epoch 550/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4758\n",
      "Epoch 551/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4711\n",
      "Epoch 552/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5762\n",
      "Epoch 553/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4955\n",
      "Epoch 554/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4531\n",
      "Epoch 555/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5505\n",
      "Epoch 556/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4774\n",
      "Epoch 557/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4002\n",
      "Epoch 558/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5928\n",
      "Epoch 559/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5028\n",
      "Epoch 560/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4009\n",
      "Epoch 561/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4892\n",
      "Epoch 562/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4234\n",
      "Epoch 563/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5043\n",
      "Epoch 564/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4938\n",
      "Epoch 565/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4957\n",
      "Epoch 566/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5452\n",
      "Epoch 567/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4063\n",
      "Epoch 568/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4652\n",
      "Epoch 569/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5610\n",
      "Epoch 570/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5476\n",
      "Epoch 571/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5639\n",
      "Epoch 572/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6135\n",
      "Epoch 573/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4708\n",
      "Epoch 574/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4372\n",
      "Epoch 575/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4431\n",
      "Epoch 576/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4333\n",
      "Epoch 577/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4651\n",
      "Epoch 578/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5979\n",
      "Epoch 579/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4084\n",
      "Epoch 580/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4246\n",
      "Epoch 581/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5010\n",
      "Epoch 582/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5481\n",
      "Epoch 583/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4234\n",
      "Epoch 584/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3836\n",
      "Epoch 585/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.3979\n",
      "Epoch 586/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4763\n",
      "Epoch 587/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4863\n",
      "Epoch 588/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5419\n",
      "Epoch 589/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4453\n",
      "Epoch 590/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5134\n",
      "Epoch 591/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5679\n",
      "Epoch 592/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3889\n",
      "Epoch 593/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3927\n",
      "Epoch 594/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4323\n",
      "Epoch 595/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4753\n",
      "Epoch 596/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4987\n",
      "Epoch 597/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.3887\n",
      "Epoch 598/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4540\n",
      "Epoch 599/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5549\n",
      "Epoch 600/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [56.73655776977539,\n",
       "  49.06821212768555,\n",
       "  33.95803565979004,\n",
       "  19.341653442382814,\n",
       "  16.621379470825197,\n",
       "  16.279120254516602,\n",
       "  15.339823150634766,\n",
       "  13.48583698272705,\n",
       "  10.89020004272461,\n",
       "  9.036713027954102,\n",
       "  8.715934753417969,\n",
       "  8.654388427734375,\n",
       "  8.53632469177246,\n",
       "  8.483577728271484,\n",
       "  8.36639175415039,\n",
       "  8.239337348937989,\n",
       "  8.113457489013673,\n",
       "  7.943838310241699,\n",
       "  7.769909477233886,\n",
       "  7.553604316711426,\n",
       "  7.346997547149658,\n",
       "  7.127396869659424,\n",
       "  6.91102705001831,\n",
       "  6.795470333099365,\n",
       "  6.472955513000488,\n",
       "  6.319761848449707,\n",
       "  6.095861625671387,\n",
       "  5.8649595260620115,\n",
       "  5.796298980712891,\n",
       "  5.527859401702881,\n",
       "  5.301926708221435,\n",
       "  5.158630657196045,\n",
       "  4.778032207489014,\n",
       "  4.4932780265808105,\n",
       "  4.374104404449463,\n",
       "  4.141073322296142,\n",
       "  4.0732008934021,\n",
       "  4.4240720748901365,\n",
       "  4.638255405426025,\n",
       "  3.761649322509766,\n",
       "  4.2495927810668945,\n",
       "  4.102700710296631,\n",
       "  3.9236178398132324,\n",
       "  3.7269850730895997,\n",
       "  3.6257391929626466,\n",
       "  3.6545502662658693,\n",
       "  3.5060572624206543,\n",
       "  3.4908010482788088,\n",
       "  4.009277391433716,\n",
       "  3.684170866012573,\n",
       "  3.260085344314575,\n",
       "  3.3753284931182863,\n",
       "  3.602993869781494,\n",
       "  3.327546977996826,\n",
       "  3.2757437229156494,\n",
       "  3.4542248249053955,\n",
       "  3.2537913799285887,\n",
       "  3.516253185272217,\n",
       "  3.1479025363922117,\n",
       "  3.116568374633789,\n",
       "  3.113138484954834,\n",
       "  3.108224058151245,\n",
       "  3.1746737003326415,\n",
       "  3.2305554866790773,\n",
       "  3.3873594284057615,\n",
       "  3.3551698684692384,\n",
       "  3.0486775398254395,\n",
       "  3.047703409194946,\n",
       "  3.0444640636444094,\n",
       "  3.03428111076355,\n",
       "  2.999424171447754,\n",
       "  3.0140166759490965,\n",
       "  3.0722493648529055,\n",
       "  3.1990694046020507,\n",
       "  3.2328322887420655,\n",
       "  3.030296468734741,\n",
       "  3.442322874069214,\n",
       "  3.4487863063812254,\n",
       "  3.0175826072692873,\n",
       "  3.1525368690490723,\n",
       "  3.097629261016846,\n",
       "  2.944338846206665,\n",
       "  3.0205106258392336,\n",
       "  2.9395702838897706,\n",
       "  3.0987972259521483,\n",
       "  3.099555826187134,\n",
       "  3.2610668659210207,\n",
       "  2.951385021209717,\n",
       "  2.887873649597168,\n",
       "  2.95717453956604,\n",
       "  2.902460289001465,\n",
       "  2.94079213142395,\n",
       "  3.4138643741607666,\n",
       "  3.070455455780029,\n",
       "  3.1174153804779055,\n",
       "  2.8909480571746826,\n",
       "  2.996508836746216,\n",
       "  2.9745503425598145,\n",
       "  2.876818132400513,\n",
       "  2.9582733631134035,\n",
       "  3.0071339130401613,\n",
       "  3.1581235408782957,\n",
       "  2.899909019470215,\n",
       "  2.910226821899414,\n",
       "  3.1690033435821534,\n",
       "  2.9677881717681887,\n",
       "  2.924331045150757,\n",
       "  2.976498603820801,\n",
       "  2.8930506229400637,\n",
       "  2.8254178524017335,\n",
       "  2.9478538036346436,\n",
       "  2.963402032852173,\n",
       "  2.846147871017456,\n",
       "  2.8030551433563233,\n",
       "  2.830870199203491,\n",
       "  3.222226285934448,\n",
       "  3.083892250061035,\n",
       "  2.88078932762146,\n",
       "  2.8659871101379393,\n",
       "  2.8841679096221924,\n",
       "  2.7972664833068848,\n",
       "  2.8064699172973633,\n",
       "  2.959270191192627,\n",
       "  2.8342645168304443,\n",
       "  2.968363332748413,\n",
       "  2.871633243560791,\n",
       "  2.778917932510376,\n",
       "  2.8150762557983398,\n",
       "  2.898225927352905,\n",
       "  3.011095905303955,\n",
       "  2.8324842929840086,\n",
       "  2.780057668685913,\n",
       "  2.818976306915283,\n",
       "  2.9438554286956786,\n",
       "  2.778983640670776,\n",
       "  2.8917348384857178,\n",
       "  2.8460968494415284,\n",
       "  2.7846970081329347,\n",
       "  2.7607558727264405,\n",
       "  2.7569902420043944,\n",
       "  2.7416555881500244,\n",
       "  2.7887954235076906,\n",
       "  2.999498891830444,\n",
       "  2.863904523849487,\n",
       "  2.76849365234375,\n",
       "  2.8846139430999758,\n",
       "  2.751276063919067,\n",
       "  2.750732898712158,\n",
       "  2.7229761600494387,\n",
       "  2.793201208114624,\n",
       "  2.794995641708374,\n",
       "  2.959586191177368,\n",
       "  2.823301362991333,\n",
       "  2.7834139347076414,\n",
       "  2.8750028133392336,\n",
       "  2.8295549869537355,\n",
       "  2.8420603275299072,\n",
       "  2.8397327423095704,\n",
       "  2.8895498752593993,\n",
       "  3.05425124168396,\n",
       "  2.7135027408599854,\n",
       "  2.7371353626251222,\n",
       "  2.7718491554260254,\n",
       "  2.807874155044556,\n",
       "  2.806908702850342,\n",
       "  2.8849879264831544,\n",
       "  2.733736181259155,\n",
       "  2.711523962020874,\n",
       "  2.6939011096954344,\n",
       "  2.7242444515228272,\n",
       "  2.7861177921295166,\n",
       "  3.0608505725860597,\n",
       "  2.915051317214966,\n",
       "  2.8387855052948,\n",
       "  2.7126283645629883,\n",
       "  2.7933605194091795,\n",
       "  2.863895845413208,\n",
       "  2.712344026565552,\n",
       "  2.712143325805664,\n",
       "  2.692652130126953,\n",
       "  2.768230104446411,\n",
       "  2.720835781097412,\n",
       "  2.71370005607605,\n",
       "  2.6605836391448974,\n",
       "  2.6941532135009765,\n",
       "  2.827351522445679,\n",
       "  2.686907482147217,\n",
       "  2.7343976497650146,\n",
       "  2.660813236236572,\n",
       "  2.6456443309783935,\n",
       "  2.641778993606567,\n",
       "  2.8088358879089355,\n",
       "  2.9054879665374758,\n",
       "  2.7267300128936767,\n",
       "  2.739993762969971,\n",
       "  2.777516794204712,\n",
       "  2.7439334392547607,\n",
       "  2.898955774307251,\n",
       "  3.000604248046875,\n",
       "  2.731294775009155,\n",
       "  2.78797869682312,\n",
       "  2.8120769023895265,\n",
       "  2.6649824142456056,\n",
       "  2.848947048187256,\n",
       "  2.815753221511841,\n",
       "  2.820398139953613,\n",
       "  2.7133076190948486,\n",
       "  2.7502662181854247,\n",
       "  2.8934287071228026,\n",
       "  2.8052391529083254,\n",
       "  2.6529025554656984,\n",
       "  2.801305055618286,\n",
       "  2.882970190048218,\n",
       "  2.6757269382476805,\n",
       "  2.648850107192993,\n",
       "  2.7810193061828614,\n",
       "  2.805961275100708,\n",
       "  2.6576518535614015,\n",
       "  2.646209192276001,\n",
       "  2.6101929187774657,\n",
       "  2.6033216953277587,\n",
       "  2.6417681694030763,\n",
       "  2.6367812633514403,\n",
       "  2.671092414855957,\n",
       "  2.6108353614807127,\n",
       "  2.68281626701355,\n",
       "  2.621660327911377,\n",
       "  2.8080159187316895,\n",
       "  2.793491744995117,\n",
       "  2.6254138469696047,\n",
       "  2.6053773403167724,\n",
       "  2.9281337738037108,\n",
       "  2.8590373516082765,\n",
       "  2.6517733097076417,\n",
       "  2.6335455417633056,\n",
       "  2.6324891567230226,\n",
       "  2.832396459579468,\n",
       "  2.6336092948913574,\n",
       "  2.6289815425872805,\n",
       "  2.7450066089630125,\n",
       "  2.813034009933472,\n",
       "  2.6236249446868896,\n",
       "  2.7626928806304933,\n",
       "  2.650842046737671,\n",
       "  2.5745750427246095,\n",
       "  2.663296842575073,\n",
       "  2.6405920028686523,\n",
       "  2.5884524822235107,\n",
       "  2.600048780441284,\n",
       "  2.593403196334839,\n",
       "  2.6215038776397703,\n",
       "  2.6999510288238526,\n",
       "  2.6052598476409914,\n",
       "  2.6609601974487305,\n",
       "  2.7693347454071047,\n",
       "  2.6197890758514406,\n",
       "  2.595965528488159,\n",
       "  2.6967124462127687,\n",
       "  2.674012231826782,\n",
       "  2.7103385448455812,\n",
       "  2.5957247257232665,\n",
       "  2.5982755184173585,\n",
       "  2.5817553997039795,\n",
       "  2.6025814533233644,\n",
       "  2.6641592502593996,\n",
       "  2.7408183574676515,\n",
       "  2.6175889492034914,\n",
       "  2.7612162113189695,\n",
       "  2.6439602375030518,\n",
       "  2.593457651138306,\n",
       "  2.709072732925415,\n",
       "  2.6197018146514894,\n",
       "  2.6057222843170167,\n",
       "  2.561624002456665,\n",
       "  2.5638649463653564,\n",
       "  2.644188737869263,\n",
       "  2.66177659034729,\n",
       "  2.7874054431915285,\n",
       "  2.5987825870513914,\n",
       "  2.765229368209839,\n",
       "  2.710279083251953,\n",
       "  2.738668346405029,\n",
       "  2.591224765777588,\n",
       "  2.6575825214385986,\n",
       "  2.6257755756378174,\n",
       "  2.5445924282073973,\n",
       "  2.539992332458496,\n",
       "  2.6292530059814454,\n",
       "  2.680436849594116,\n",
       "  2.6153684139251707,\n",
       "  2.6708953380584717,\n",
       "  2.6474325180053713,\n",
       "  2.545974779129028,\n",
       "  2.58412504196167,\n",
       "  2.5751379489898683,\n",
       "  2.6867987155914306,\n",
       "  2.6784516334533692,\n",
       "  2.619831609725952,\n",
       "  2.8361110210418703,\n",
       "  2.5789375782012938,\n",
       "  2.560103940963745,\n",
       "  2.6547737598419188,\n",
       "  2.634031820297241,\n",
       "  2.5677963733673095,\n",
       "  2.53843207359314,\n",
       "  2.5509292602539064,\n",
       "  2.550867700576782,\n",
       "  2.5751696109771727,\n",
       "  2.693706750869751,\n",
       "  2.591403341293335,\n",
       "  2.53497052192688,\n",
       "  2.570376920700073,\n",
       "  2.6503740310668946,\n",
       "  2.710723543167114,\n",
       "  2.5579185962677,\n",
       "  2.5377686500549315,\n",
       "  2.5267858028411867,\n",
       "  2.57845196723938,\n",
       "  2.554958534240723,\n",
       "  2.610954999923706,\n",
       "  2.5654797077178957,\n",
       "  2.5728920459747315,\n",
       "  2.609465980529785,\n",
       "  2.5172988414764403,\n",
       "  2.5204829216003417,\n",
       "  2.584159803390503,\n",
       "  2.9117089748382567,\n",
       "  2.6418243408203126,\n",
       "  2.6574775695800783,\n",
       "  2.555710506439209,\n",
       "  2.6925233364105225,\n",
       "  2.6468839168548586,\n",
       "  2.5481024742126466,\n",
       "  2.6003371715545653,\n",
       "  2.5538707256317137,\n",
       "  2.6079021453857423,\n",
       "  2.8845423221588136,\n",
       "  2.5777690410614014,\n",
       "  2.622265625,\n",
       "  2.516264629364014,\n",
       "  2.532323408126831,\n",
       "  2.5037271976470947,\n",
       "  2.518406534194946,\n",
       "  2.659374237060547,\n",
       "  2.6382922172546386,\n",
       "  2.5372361660003664,\n",
       "  2.5406189441680906,\n",
       "  2.5328388690948485,\n",
       "  2.7026161670684816,\n",
       "  2.6441195487976072,\n",
       "  2.6304269313812254,\n",
       "  2.6793541431427004,\n",
       "  2.772917318344116,\n",
       "  2.519148921966553,\n",
       "  2.626624822616577,\n",
       "  2.543695831298828,\n",
       "  2.636408472061157,\n",
       "  2.5764235973358156,\n",
       "  2.7335114002227785,\n",
       "  2.65889048576355,\n",
       "  2.568349742889404,\n",
       "  2.503088855743408,\n",
       "  2.53869514465332,\n",
       "  2.5231621265411377,\n",
       "  2.5860588550567627,\n",
       "  2.5166181087493897,\n",
       "  2.5205453872680663,\n",
       "  2.641437816619873,\n",
       "  2.5507559776306152,\n",
       "  2.5150660037994386,\n",
       "  2.6991806507110594,\n",
       "  2.5664369106292724,\n",
       "  2.652057647705078,\n",
       "  2.520582437515259,\n",
       "  2.5071043491363527,\n",
       "  2.5240376949310304,\n",
       "  2.570113515853882,\n",
       "  2.5817874908447265,\n",
       "  2.5023232460021974,\n",
       "  2.9680161476135254,\n",
       "  2.4948145389556884,\n",
       "  2.630419111251831,\n",
       "  2.6110923290252686,\n",
       "  2.5209156036376954,\n",
       "  2.5108588695526124,\n",
       "  2.57706036567688,\n",
       "  2.6876062870025637,\n",
       "  2.5751851558685304,\n",
       "  2.48937611579895,\n",
       "  2.490190267562866,\n",
       "  2.4847545623779297,\n",
       "  2.6209149837493895,\n",
       "  2.613331985473633,\n",
       "  2.5482965469360352,\n",
       "  2.6280972003936767,\n",
       "  2.5745264530181884,\n",
       "  2.6751404285430906,\n",
       "  2.5973796367645265,\n",
       "  2.563975524902344,\n",
       "  2.624388837814331,\n",
       "  2.556474542617798,\n",
       "  2.534979057312012,\n",
       "  2.513836479187012,\n",
       "  2.6233750343322755,\n",
       "  2.5873870849609375,\n",
       "  2.5538403511047365,\n",
       "  2.547948884963989,\n",
       "  2.59301962852478,\n",
       "  2.527651882171631,\n",
       "  2.604092741012573,\n",
       "  2.496637535095215,\n",
       "  2.4598667144775392,\n",
       "  2.466436243057251,\n",
       "  2.471300554275513,\n",
       "  2.4875357151031494,\n",
       "  2.672820806503296,\n",
       "  2.6674209117889403,\n",
       "  2.7752450466156007,\n",
       "  2.490016460418701,\n",
       "  2.4589852809906008,\n",
       "  2.4894741058349608,\n",
       "  2.5524113178253174,\n",
       "  2.6852558135986326,\n",
       "  2.5026691913604737,\n",
       "  2.723897361755371,\n",
       "  2.601157808303833,\n",
       "  2.53061957359314,\n",
       "  2.508354997634888,\n",
       "  2.528340530395508,\n",
       "  2.6536865711212156,\n",
       "  2.522262382507324,\n",
       "  2.488151693344116,\n",
       "  2.5015961170196532,\n",
       "  2.589721441268921,\n",
       "  2.5128310680389405,\n",
       "  2.557449531555176,\n",
       "  2.522891092300415,\n",
       "  2.4842793941497803,\n",
       "  2.4834502220153807,\n",
       "  2.587284517288208,\n",
       "  2.548783779144287,\n",
       "  2.730049753189087,\n",
       "  2.4631024837493896,\n",
       "  2.483782243728638,\n",
       "  2.5106759548187254,\n",
       "  2.5149913311004637,\n",
       "  2.6684139251708983,\n",
       "  2.489250135421753,\n",
       "  2.4648561477661133,\n",
       "  2.4428849697113035,\n",
       "  2.4999077320098877,\n",
       "  2.5985424518585205,\n",
       "  2.49910626411438,\n",
       "  2.48665452003479,\n",
       "  2.4429887771606444,\n",
       "  2.445330810546875,\n",
       "  2.4387138366699217,\n",
       "  2.474244546890259,\n",
       "  2.6539467334747315,\n",
       "  2.5345978260040285,\n",
       "  2.5229445934295653,\n",
       "  2.6061098098754885,\n",
       "  2.613284635543823,\n",
       "  2.451098108291626,\n",
       "  2.476445722579956,\n",
       "  2.4451980113983156,\n",
       "  2.4894490242004395,\n",
       "  2.683263635635376,\n",
       "  2.646958923339844,\n",
       "  2.5705854892730713,\n",
       "  2.5266836643218995,\n",
       "  2.502903699874878,\n",
       "  2.63647723197937,\n",
       "  2.5754043102264403,\n",
       "  2.633303737640381,\n",
       "  2.441346836090088,\n",
       "  2.5508877277374267,\n",
       "  2.5145796298980714,\n",
       "  2.578396940231323,\n",
       "  2.4853354930877685,\n",
       "  2.520589256286621,\n",
       "  2.481955718994141,\n",
       "  2.46858434677124,\n",
       "  2.4589650630950928,\n",
       "  2.469121217727661,\n",
       "  2.483445882797241,\n",
       "  2.5746416568756105,\n",
       "  2.6972023487091064,\n",
       "  2.5157673358917236,\n",
       "  2.5362343311309816,\n",
       "  2.4716784954071045,\n",
       "  2.5039151668548585,\n",
       "  2.457593488693237,\n",
       "  2.648242139816284,\n",
       "  2.5672937870025634,\n",
       "  2.606231451034546,\n",
       "  2.4445512294769287,\n",
       "  2.5841559886932375,\n",
       "  2.552441120147705,\n",
       "  2.6086566925048826,\n",
       "  2.4332488536834718,\n",
       "  2.530957508087158,\n",
       "  2.4661242961883545,\n",
       "  2.478749418258667,\n",
       "  2.503040409088135,\n",
       "  2.534152603149414,\n",
       "  2.499804449081421,\n",
       "  2.4911296367645264,\n",
       "  2.7194995403289797,\n",
       "  2.5104405879974365,\n",
       "  2.4761919498443605,\n",
       "  2.5304752349853517,\n",
       "  2.417731428146362,\n",
       "  2.5986183643341065,\n",
       "  2.4631192684173584,\n",
       "  2.4376341342926025,\n",
       "  2.5290135860443117,\n",
       "  2.6600000858306885,\n",
       "  2.496102142333984,\n",
       "  2.4346317768096926,\n",
       "  2.5884153842926025,\n",
       "  2.4978911876678467,\n",
       "  2.5771770000457765,\n",
       "  2.4543890953063965,\n",
       "  2.4214500904083254,\n",
       "  2.5173975467681884,\n",
       "  2.466332960128784,\n",
       "  2.498607110977173,\n",
       "  2.5678950786590575,\n",
       "  2.4392903327941893,\n",
       "  2.564694356918335,\n",
       "  2.550626516342163,\n",
       "  2.447946882247925,\n",
       "  2.440772867202759,\n",
       "  2.6307828426361084,\n",
       "  2.4699984073638914,\n",
       "  2.6227022647857665,\n",
       "  2.4825656414031982,\n",
       "  2.509863996505737,\n",
       "  2.539805269241333,\n",
       "  2.4377076625823975,\n",
       "  2.4507147789001467,\n",
       "  2.554278612136841,\n",
       "  2.408625078201294,\n",
       "  2.4227861404418944,\n",
       "  2.4077483654022216,\n",
       "  2.528375005722046,\n",
       "  2.436458158493042,\n",
       "  2.5365009784698485,\n",
       "  2.4758443355560305,\n",
       "  2.471089553833008,\n",
       "  2.576211404800415,\n",
       "  2.4955495834350585,\n",
       "  2.4531423568725588,\n",
       "  2.5504804134368895,\n",
       "  2.477367973327637,\n",
       "  2.4001537799835204,\n",
       "  2.592839765548706,\n",
       "  2.5028018951416016,\n",
       "  2.400946617126465,\n",
       "  2.489162015914917,\n",
       "  2.4234312534332276,\n",
       "  2.5042887210845945,\n",
       "  2.4938344955444336,\n",
       "  2.495659685134888,\n",
       "  2.5452130794525147,\n",
       "  2.4062652587890625,\n",
       "  2.4651843070983888,\n",
       "  2.561008834838867,\n",
       "  2.5476001262664796,\n",
       "  2.5639113903045656,\n",
       "  2.613496685028076,\n",
       "  2.47084002494812,\n",
       "  2.4372127056121826,\n",
       "  2.443069648742676,\n",
       "  2.433252763748169,\n",
       "  2.4650998592376707,\n",
       "  2.597876787185669,\n",
       "  2.4084130764007567,\n",
       "  2.42460560798645,\n",
       "  2.500982570648193,\n",
       "  2.548060655593872,\n",
       "  2.4233721256256104,\n",
       "  2.383569097518921,\n",
       "  2.397885322570801,\n",
       "  2.4762896060943604,\n",
       "  2.4862817764282226,\n",
       "  2.541928291320801,\n",
       "  2.445270872116089,\n",
       "  2.513410806655884,\n",
       "  2.5678800106048585,\n",
       "  2.3889323711395263,\n",
       "  2.3926953315734862,\n",
       "  2.4323051452636717,\n",
       "  2.475251054763794,\n",
       "  2.498714303970337,\n",
       "  2.388747453689575,\n",
       "  2.453989553451538,\n",
       "  2.554918098449707,\n",
       "  2.4038276195526125]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyVal_NoReg = []\n",
    "historyTr_NoReg = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_NoReg()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=1036).history\n",
    "    historyVal_NoReg.append(history['val_loss'])\n",
    "    historyTr_NoReg.append(history['loss'])\n",
    "model=create_model_NoReg()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=600, \n",
    "                      batch_size=1036).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_NoReg_mean=np.mean(historyVal_NoReg, axis=0)\n",
    "historyTr_NoReg_mean=np.mean(historyTr_NoReg, axis=0)\n",
    "\n",
    "historyVal_NoReg_sd=np.std(historyVal_NoReg, axis=0)\n",
    "historyTr_NoReg_sd=np.std(historyTr_NoReg, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_NoReg_mean)),historyTr_NoReg_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_NoReg_mean)), historyTr_NoReg_mean - historyTr_NoReg_sd, \n",
    "                 historyTr_NoReg_mean + historyTr_NoReg_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_NoReg_mean)), historyVal_NoReg_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_NoReg_mean)), historyVal_NoReg_mean - historyVal_NoReg_sd, \n",
    "                 historyVal_NoReg_mean + historyVal_NoReg_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.title('Learning curves no alpha', fontsize = 18)\n",
    "plt.legend()\n",
    "plt.ylim(5,20)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 21, +2))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No regularization (no alpha) result:\n",
      "MEE on the validation 2.859019660949707 with standard deviation 0.17027188690564501\n",
      "MEE on the training 2.5702057838439942 with standard deviation 0.055443494913659894\n"
     ]
    }
   ],
   "source": [
    "print(\"No regularization (no alpha) result:\")\n",
    "print(\"MEE on the validation\",historyVal_NoReg_mean[-1],\"with standard deviation\",historyVal_NoReg_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_NoReg_mean[-1],\"with standard deviation\",historyTr_NoReg_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_Low_LR():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=0.01, \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 520us/step - loss: 55.9093 - val_loss: 56.5554\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 55.8178 - val_loss: 56.4173\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 55.6800 - val_loss: 56.2555\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 55.5185 - val_loss: 56.0814\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.3448 - val_loss: 55.9004\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 55.1645 - val_loss: 55.7154\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 54.9799 - val_loss: 55.5277\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 54.7928 - val_loss: 55.3379\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 54.6035 - val_loss: 55.1462\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 54.4122 - val_loss: 54.9527\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 54.2192 - val_loss: 54.7574\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.0244 - val_loss: 54.5603\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 53.8279 - val_loss: 54.3612\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 53.6293 - val_loss: 54.1602\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 53.4288 - val_loss: 53.9571\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 53.2261 - val_loss: 53.7518\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 53.0214 - val_loss: 53.5443\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 52.8144 - val_loss: 53.3345\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 52.6053 - val_loss: 53.1224\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 52.3935 - val_loss: 52.9078\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 52.1795 - val_loss: 52.6907\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 51.9628 - val_loss: 52.4710\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 51.7437 - val_loss: 52.2487\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 51.5220 - val_loss: 52.0238\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 51.2977 - val_loss: 51.7962\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 51.0704 - val_loss: 51.5658\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 50.8406 - val_loss: 51.3327\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 50.6079 - val_loss: 51.0967\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 50.3725 - val_loss: 50.8580\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 50.1343 - val_loss: 50.6164\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 49.8932 - val_loss: 50.3720\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 49.6493 - val_loss: 50.1247\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.4024 - val_loss: 49.8745\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.1528 - val_loss: 49.6215\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 48.9003 - val_loss: 49.3657\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 48.6450 - val_loss: 49.1071\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 48.3867 - val_loss: 48.8457\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 48.1259 - val_loss: 48.5814\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 47.8621 - val_loss: 48.3145\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 47.5956 - val_loss: 48.0448\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 47.3263 - val_loss: 47.7725\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 47.0545 - val_loss: 47.4976\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 46.7799 - val_loss: 47.2201\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 46.5029 - val_loss: 46.9400\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 46.2231 - val_loss: 46.6576\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 45.9410 - val_loss: 46.3727\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 45.6567 - val_loss: 46.0854\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 45.3697 - val_loss: 45.7960\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 45.0805 - val_loss: 45.5042\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.7892 - val_loss: 45.2104\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 44.4957 - val_loss: 44.9145\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 44.2001 - val_loss: 44.6166\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 43.9025 - val_loss: 44.3168\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 43.6030 - val_loss: 44.0151\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 43.3016 - val_loss: 43.7117\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 42.9985 - val_loss: 43.4066\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 42.6935 - val_loss: 43.0999\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 42.3871 - val_loss: 42.7916\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 42.0791 - val_loss: 42.4819\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 41.7695 - val_loss: 42.1709\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 41.4586 - val_loss: 41.8584\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 41.1465 - val_loss: 41.5448\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 40.8330 - val_loss: 41.2300\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 40.5185 - val_loss: 40.9142\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 40.2027 - val_loss: 40.5973\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 39.8860 - val_loss: 40.2795\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 39.5684 - val_loss: 39.9609\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 39.2500 - val_loss: 39.6415\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 38.9307 - val_loss: 39.3213\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 38.6108 - val_loss: 39.0006\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 38.2902 - val_loss: 38.6793\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 37.9690 - val_loss: 38.3575\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 37.6474 - val_loss: 38.0354\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 37.3255 - val_loss: 37.7129\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 37.0032 - val_loss: 37.3902\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 36.6807 - val_loss: 37.0674\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 36.3581 - val_loss: 36.7445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 36.0354 - val_loss: 36.4216\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 35.7129 - val_loss: 36.0989\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 35.3903 - val_loss: 35.7764\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 35.0681 - val_loss: 35.4542\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 34.7463 - val_loss: 35.1325\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 34.4248 - val_loss: 34.8113\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 34.1040 - val_loss: 34.4908\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 33.7837 - val_loss: 34.1710\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 33.4644 - val_loss: 33.8521\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 33.1458 - val_loss: 33.5342\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 32.8283 - val_loss: 33.2175\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 32.5120 - val_loss: 32.9020\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 32.1970 - val_loss: 32.5879\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 31.8832 - val_loss: 32.2753\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 31.5710 - val_loss: 31.9644\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 31.2605 - val_loss: 31.6553\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 30.9519 - val_loss: 31.3482\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 30.6450 - val_loss: 31.0432\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 30.3403 - val_loss: 30.7403\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 30.0377 - val_loss: 30.4399\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 29.7375 - val_loss: 30.1419\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 29.4397 - val_loss: 29.8467\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 29.1443 - val_loss: 29.5542\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 28.8518 - val_loss: 29.2646\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 28.5620 - val_loss: 28.9782\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 28.2752 - val_loss: 28.6949\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 27.9915 - val_loss: 28.4149\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 27.7109 - val_loss: 28.1384\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 27.4336 - val_loss: 27.8655\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 27.1597 - val_loss: 27.5963\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 26.8892 - val_loss: 27.3310\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 26.6225 - val_loss: 27.0696\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 26.3594 - val_loss: 26.8122\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 26.1003 - val_loss: 26.5589\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 25.8451 - val_loss: 26.3099\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 25.5940 - val_loss: 26.0651\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 25.3472 - val_loss: 25.8247\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 25.1047 - val_loss: 25.5886\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 24.8667 - val_loss: 25.3570\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 24.6332 - val_loss: 25.1297\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 24.4044 - val_loss: 24.9069\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 24.1804 - val_loss: 24.6885\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 23.9613 - val_loss: 24.4746\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.7473 - val_loss: 24.2652\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 23.5384 - val_loss: 24.0603\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 23.3348 - val_loss: 23.8605\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 23.1368 - val_loss: 23.6663\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.9445 - val_loss: 23.4782\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 22.7576 - val_loss: 23.2972\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.5767 - val_loss: 23.1241\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 22.4020 - val_loss: 22.9570\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.2334 - val_loss: 22.7955\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 22.0712 - val_loss: 22.6391\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.9148 - val_loss: 22.4880\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.7638 - val_loss: 22.3412\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 21.6178 - val_loss: 22.1995\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.4763 - val_loss: 22.0611\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.3388 - val_loss: 21.9259\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.2050 - val_loss: 21.7934\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.0745 - val_loss: 21.6636\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.9471 - val_loss: 21.5363\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.8226 - val_loss: 21.4115\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 20.7008 - val_loss: 21.2891\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.5817 - val_loss: 21.1692\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.4651 - val_loss: 21.0516\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.3509 - val_loss: 20.9363\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.2388 - val_loss: 20.8232\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.1290 - val_loss: 20.7123\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.0213 - val_loss: 20.6035\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.9156 - val_loss: 20.4966\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.8117 - val_loss: 20.3916\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.7098 - val_loss: 20.2884\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.6096 - val_loss: 20.1870\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.5111 - val_loss: 20.0872\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 19.4143 - val_loss: 19.9890\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.3190 - val_loss: 19.8923\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.2254 - val_loss: 19.7972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 19.1332 - val_loss: 19.7035\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.0425 - val_loss: 19.6113\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.9532 - val_loss: 19.5204\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.8653 - val_loss: 19.4309\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.7786 - val_loss: 19.3427\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 18.6933 - val_loss: 19.2559\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 18.6093 - val_loss: 19.1702\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.5265 - val_loss: 19.0859\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.4449 - val_loss: 19.0027\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.3645 - val_loss: 18.9207\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 18.2851 - val_loss: 18.8399\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.2069 - val_loss: 18.7602\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.1298 - val_loss: 18.6816\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.0537 - val_loss: 18.6041\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.9786 - val_loss: 18.5276\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.9045 - val_loss: 18.4521\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.8313 - val_loss: 18.3776\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.7591 - val_loss: 18.3041\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 17.6877 - val_loss: 18.2315\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 17.6173 - val_loss: 18.1597\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.5476 - val_loss: 18.0889\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.4788 - val_loss: 18.0189\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.4108 - val_loss: 17.9497\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.3435 - val_loss: 17.8812\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.2770 - val_loss: 17.8136\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.2112 - val_loss: 17.7467\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.1461 - val_loss: 17.6805\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.0817 - val_loss: 17.6150\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 17.0179 - val_loss: 17.5501\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 16.9547 - val_loss: 17.4859\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.8922 - val_loss: 17.4223\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.8302 - val_loss: 17.3593\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.7687 - val_loss: 17.2969\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.7077 - val_loss: 17.2350\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.6473 - val_loss: 17.1736\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.5873 - val_loss: 17.1127\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.5278 - val_loss: 17.0523\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.4686 - val_loss: 16.9923\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.4099 - val_loss: 16.9328\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.3514 - val_loss: 16.8736\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.2934 - val_loss: 16.8148\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.2357 - val_loss: 16.7563\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.1782 - val_loss: 16.6981\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.1209 - val_loss: 16.6402\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 16.0639 - val_loss: 16.5825\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 16.0070 - val_loss: 16.5251\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 15.9504 - val_loss: 16.4678\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.8939 - val_loss: 16.4107\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.8374 - val_loss: 16.3537\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.7810 - val_loss: 16.2969\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.7247 - val_loss: 16.2401\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.6683 - val_loss: 16.1833\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.6120 - val_loss: 16.1266\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.5557 - val_loss: 16.0698\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.4992 - val_loss: 16.0130\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.4427 - val_loss: 15.9562\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.3861 - val_loss: 15.8992\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.3293 - val_loss: 15.8422\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.2723 - val_loss: 15.7851\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.2153 - val_loss: 15.7278\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.1580 - val_loss: 15.6703\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.1005 - val_loss: 15.6128\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.0428 - val_loss: 15.5550\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.9849 - val_loss: 15.4971\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.9268 - val_loss: 15.4390\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.8685 - val_loss: 15.3808\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.8099 - val_loss: 15.3224\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 14.7512 - val_loss: 15.2639\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 14.6923 - val_loss: 15.2052\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 14.6332 - val_loss: 15.1465\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 14.5740 - val_loss: 15.0876\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 14.5147 - val_loss: 15.0287\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 14.4552 - val_loss: 14.9698\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 14.3957 - val_loss: 14.9109\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 14.3362 - val_loss: 14.8520\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 14.2767 - val_loss: 14.7932\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 14.2172 - val_loss: 14.7345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 14.1578 - val_loss: 14.6759\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.0985 - val_loss: 14.6174\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.0393 - val_loss: 14.5591\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.9802 - val_loss: 14.5010\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.9213 - val_loss: 14.4431\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.8627 - val_loss: 14.3854\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.8042 - val_loss: 14.3280\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.7460 - val_loss: 14.2708\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 13.6880 - val_loss: 14.2139\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.6302 - val_loss: 14.1573\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 13.5728 - val_loss: 14.1010\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.5156 - val_loss: 14.0449\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.4587 - val_loss: 13.9892\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.4021 - val_loss: 13.9337\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.3458 - val_loss: 13.8785\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 13.2897 - val_loss: 13.8236\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 13.2339 - val_loss: 13.7689\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.1783 - val_loss: 13.7145\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 13.1230 - val_loss: 13.6604\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 13.0680 - val_loss: 13.6065\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.0132 - val_loss: 13.5528\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 12.9586 - val_loss: 13.4994\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.9042 - val_loss: 13.4462\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.8500 - val_loss: 13.3932\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.7960 - val_loss: 13.3404\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.7421 - val_loss: 13.2877\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.6884 - val_loss: 13.2353\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 12.6349 - val_loss: 13.1829\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 12.5814 - val_loss: 13.1307\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.5280 - val_loss: 13.0786\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 12.4748 - val_loss: 13.0267\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 12.4216 - val_loss: 12.9748\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 12.3684 - val_loss: 12.9230\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 12.3153 - val_loss: 12.8713\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 12.2622 - val_loss: 12.8196\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.2092 - val_loss: 12.7679\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 12.1561 - val_loss: 12.7163\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.1030 - val_loss: 12.6647\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.0499 - val_loss: 12.6132\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.9967 - val_loss: 12.5616\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 11.9435 - val_loss: 12.5100\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 11.8902 - val_loss: 12.4585\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.8368 - val_loss: 12.4068\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 11.7835 - val_loss: 12.3552\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.7300 - val_loss: 12.3036\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.6764 - val_loss: 12.2519\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.6228 - val_loss: 12.2003\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.5691 - val_loss: 12.1486\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.5154 - val_loss: 12.0969\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 11.4616 - val_loss: 12.0452\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.4077 - val_loss: 11.9935\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.3539 - val_loss: 11.9419\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.3001 - val_loss: 11.8902\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 11.2462 - val_loss: 11.8387\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.1924 - val_loss: 11.7872\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 11.1387 - val_loss: 11.7361\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 11.0850 - val_loss: 11.6856\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 11.0315 - val_loss: 11.6352\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 10.9781 - val_loss: 11.5851\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 10.9248 - val_loss: 11.5351\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.8717 - val_loss: 11.4854\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 10.8189 - val_loss: 11.4359\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 10.7664 - val_loss: 11.3866\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 10.7141 - val_loss: 11.3377\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.6622 - val_loss: 11.2891\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.6107 - val_loss: 11.2408\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.5595 - val_loss: 11.1929\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 10.5089 - val_loss: 11.1455\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 10.4587 - val_loss: 11.0985\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 10.4091 - val_loss: 11.0519\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 10.3600 - val_loss: 11.0059\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.3116 - val_loss: 10.9603\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 10.2638 - val_loss: 10.9153\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.2167 - val_loss: 10.8709\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.1703 - val_loss: 10.8271\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.1247 - val_loss: 10.7838\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 10.0798 - val_loss: 10.7412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.0355 - val_loss: 10.6992\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.9920 - val_loss: 10.6578\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.9493 - val_loss: 10.6171\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.9072 - val_loss: 10.5771\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 9.8658 - val_loss: 10.5377\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.8251 - val_loss: 10.4989\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.7851 - val_loss: 10.4609\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.7459 - val_loss: 10.4235\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.7073 - val_loss: 10.3869\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 9.6695 - val_loss: 10.3510\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.6323 - val_loss: 10.3158\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.5960 - val_loss: 10.2814\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.5605 - val_loss: 10.2477\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.5258 - val_loss: 10.2147\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.4917 - val_loss: 10.1825\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.4584 - val_loss: 10.1509\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.4257 - val_loss: 10.1200\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.3937 - val_loss: 10.0898\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.3625 - val_loss: 10.0603\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 9.3320 - val_loss: 10.0315\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.3022 - val_loss: 10.0033\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 9.2730 - val_loss: 9.9757\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.2445 - val_loss: 9.9488\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.2167 - val_loss: 9.9225\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.1894 - val_loss: 9.8968\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 9.1628 - val_loss: 9.8717\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 9.1368 - val_loss: 9.8471\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 9.1115 - val_loss: 9.8232\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.0868 - val_loss: 9.7999\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 9.0627 - val_loss: 9.7772\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.0393 - val_loss: 9.7550\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0165 - val_loss: 9.7334\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.9943 - val_loss: 9.7124\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.9728 - val_loss: 9.6920\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.9517 - val_loss: 9.6721\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.9312 - val_loss: 9.6528\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.9113 - val_loss: 9.6341\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8919 - val_loss: 9.6159\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.8730 - val_loss: 9.5983\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.8546 - val_loss: 9.5812\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.8368 - val_loss: 9.5646\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.8195 - val_loss: 9.5485\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.8028 - val_loss: 9.5329\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7866 - val_loss: 9.5178\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.7710 - val_loss: 9.5031\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.7558 - val_loss: 9.4889\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7412 - val_loss: 9.4751\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.7270 - val_loss: 9.4617\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.7133 - val_loss: 9.4487\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.6999 - val_loss: 9.4361\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6870 - val_loss: 9.4239\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6746 - val_loss: 9.4121\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6625 - val_loss: 9.4007\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.6508 - val_loss: 9.3896\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6394 - val_loss: 9.3788\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.6284 - val_loss: 9.3683\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.6176 - val_loss: 9.3582\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.6073 - val_loss: 9.3483\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.5971 - val_loss: 9.3388\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.5874 - val_loss: 9.3295\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5779 - val_loss: 9.3205\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5686 - val_loss: 9.3118\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.5597 - val_loss: 9.3033\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.5511 - val_loss: 9.2951\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5427 - val_loss: 9.2871\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5346 - val_loss: 9.2794\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5268 - val_loss: 9.2719\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5192 - val_loss: 9.2646\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5119 - val_loss: 9.2576\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5047 - val_loss: 9.2507\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4979 - val_loss: 9.2441\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4912 - val_loss: 9.2376\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4847 - val_loss: 9.2313\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4784 - val_loss: 9.2251\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.4723 - val_loss: 9.2191\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4665 - val_loss: 9.2133\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4608 - val_loss: 9.2076\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4553 - val_loss: 9.2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4500 - val_loss: 9.1966\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4449 - val_loss: 9.1913\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.4398 - val_loss: 9.1860\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4349 - val_loss: 9.1809\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.4301 - val_loss: 9.1758\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4255 - val_loss: 9.1708\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.4209 - val_loss: 9.1659\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4164 - val_loss: 9.1611\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4120 - val_loss: 9.1563\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.4077 - val_loss: 9.1516\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.4035 - val_loss: 9.1470\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.3994 - val_loss: 9.1424\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.3953 - val_loss: 9.1379\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.3913 - val_loss: 9.1334\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.3874 - val_loss: 9.1290\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3835 - val_loss: 9.1247\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.3797 - val_loss: 9.1203\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.3759 - val_loss: 9.1161\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.3722 - val_loss: 9.1118\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3685 - val_loss: 9.1076\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3649 - val_loss: 9.1035\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3613 - val_loss: 9.0994\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.3578 - val_loss: 9.0954\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.3543 - val_loss: 9.0914\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3509 - val_loss: 9.0875\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.3475 - val_loss: 9.0835\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3441 - val_loss: 9.0796\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3408 - val_loss: 9.0757\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3375 - val_loss: 9.0719\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3341 - val_loss: 9.0680\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3309 - val_loss: 9.0642\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3276 - val_loss: 9.0604\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.3243 - val_loss: 9.0567\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3211 - val_loss: 9.0529\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3179 - val_loss: 9.0492\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.3147 - val_loss: 9.0454\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.3115 - val_loss: 9.0417\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3083 - val_loss: 9.0381\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3052 - val_loss: 9.0344\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.3020 - val_loss: 9.0309\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2989 - val_loss: 9.0273\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.2958 - val_loss: 9.0238\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2928 - val_loss: 9.0202\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2897 - val_loss: 9.0168\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2866 - val_loss: 9.0133\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.2836 - val_loss: 9.0098\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.2806 - val_loss: 9.0064\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.2775 - val_loss: 9.0029\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2745 - val_loss: 8.9995\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2714 - val_loss: 8.9961\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.2684 - val_loss: 8.9927\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.2654 - val_loss: 8.9893\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2623 - val_loss: 8.9859\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2593 - val_loss: 8.9825\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2562 - val_loss: 8.9791\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.2532 - val_loss: 8.9758\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2502 - val_loss: 8.9724\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2472 - val_loss: 8.9691\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2442 - val_loss: 8.9658\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2412 - val_loss: 8.9625\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.2382 - val_loss: 8.9592\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.2352 - val_loss: 8.9560\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2321 - val_loss: 8.9527\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2291 - val_loss: 8.9494\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2261 - val_loss: 8.9461\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.2230 - val_loss: 8.9428\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2199 - val_loss: 8.9395\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2169 - val_loss: 8.9362\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.2138 - val_loss: 8.9329\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2107 - val_loss: 8.9296\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2076 - val_loss: 8.9264\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2045 - val_loss: 8.9230\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2014 - val_loss: 8.9197\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.1982 - val_loss: 8.9164\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1951 - val_loss: 8.9131\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1919 - val_loss: 8.9098\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1888 - val_loss: 8.9065\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.1856 - val_loss: 8.9032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1824 - val_loss: 8.8998\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1792 - val_loss: 8.8965\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.1760 - val_loss: 8.8932\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.1728 - val_loss: 8.8898\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1695 - val_loss: 8.8865\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1663 - val_loss: 8.8831\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1630 - val_loss: 8.8798\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1597 - val_loss: 8.8764\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.1564 - val_loss: 8.8730\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1530 - val_loss: 8.8696\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.1497 - val_loss: 8.8662\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1463 - val_loss: 8.8628\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1429 - val_loss: 8.8594\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1395 - val_loss: 8.8560\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1361 - val_loss: 8.8525\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1326 - val_loss: 8.8491\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.1292 - val_loss: 8.8456\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.1257 - val_loss: 8.8421\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.1222 - val_loss: 8.8386\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.1187 - val_loss: 8.8351\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1151 - val_loss: 8.8316\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1116 - val_loss: 8.8280\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.1080 - val_loss: 8.8245\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1044 - val_loss: 8.8209\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.1007 - val_loss: 8.8173\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.0971 - val_loss: 8.8137\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.0934 - val_loss: 8.8101\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0897 - val_loss: 8.8065\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0860 - val_loss: 8.8028\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.0823 - val_loss: 8.7992\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.0785 - val_loss: 8.7955\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.0747 - val_loss: 8.7918\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.0709 - val_loss: 8.7881\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.0671 - val_loss: 8.7844\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.0632 - val_loss: 8.7806\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0593 - val_loss: 8.7769\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.0554 - val_loss: 8.7731\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.0515 - val_loss: 8.7693\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0476 - val_loss: 8.7655\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0436 - val_loss: 8.7616\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0396 - val_loss: 8.7578\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.0356 - val_loss: 8.7539\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0316 - val_loss: 8.7501\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0275 - val_loss: 8.7462\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.0235 - val_loss: 8.7422\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.0194 - val_loss: 8.7383\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.0153 - val_loss: 8.7344\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.0111 - val_loss: 8.7304\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0070 - val_loss: 8.7264\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0028 - val_loss: 8.7224\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.9986 - val_loss: 8.7184\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.9944 - val_loss: 8.7144\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.9901 - val_loss: 8.7104\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9859 - val_loss: 8.7063\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9816 - val_loss: 8.7023\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9773 - val_loss: 8.6982\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9730 - val_loss: 8.6941\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9687 - val_loss: 8.6901\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.9644 - val_loss: 8.6860\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9600 - val_loss: 8.6819\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.9556 - val_loss: 8.6778\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.9512 - val_loss: 8.6736\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.9468 - val_loss: 8.6695\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9424 - val_loss: 8.6653\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9380 - val_loss: 8.6612\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.9335 - val_loss: 8.6570\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9291 - val_loss: 8.6528\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.9246 - val_loss: 8.6486\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9201 - val_loss: 8.6444\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.9156 - val_loss: 8.6401\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.9111 - val_loss: 8.6359\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.9065 - val_loss: 8.6316\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.9020 - val_loss: 8.6274\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8974 - val_loss: 8.6231\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8928 - val_loss: 8.6188\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.8883 - val_loss: 8.6145\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.8837 - val_loss: 8.6102\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.8791 - val_loss: 8.6059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8745 - val_loss: 8.6015\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8699 - val_loss: 8.5972\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8652 - val_loss: 8.5928\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.8606 - val_loss: 8.5885\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8560 - val_loss: 8.5841\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.8513 - val_loss: 8.5797\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8467 - val_loss: 8.5754\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8420 - val_loss: 8.5710\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8374 - val_loss: 8.5666\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8327 - val_loss: 8.5622\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.8280 - val_loss: 8.5577\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8233 - val_loss: 8.5533\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.8186 - val_loss: 8.5489\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.8139 - val_loss: 8.5444\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.8092 - val_loss: 8.5400\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8045 - val_loss: 8.5356\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.7998 - val_loss: 8.5311\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.7951 - val_loss: 8.5266\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.7904 - val_loss: 8.5222\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.7857 - val_loss: 8.5177\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.7810 - val_loss: 8.5132\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.7763 - val_loss: 8.5087\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.7716 - val_loss: 8.5042\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.7668 - val_loss: 8.4997\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.7621 - val_loss: 8.4952\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.7574 - val_loss: 8.4907\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.7526 - val_loss: 8.4862\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.7479 - val_loss: 8.4817\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.7432 - val_loss: 8.4772\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.7384 - val_loss: 8.4726\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.7337 - val_loss: 8.4681\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.7290 - val_loss: 8.4636\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.7242 - val_loss: 8.4590\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.7195 - val_loss: 8.4545\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.7148 - val_loss: 8.4499\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.7100 - val_loss: 8.4454\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.7053 - val_loss: 8.4408\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.7005 - val_loss: 8.4363\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.6958 - val_loss: 8.4317\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.6911 - val_loss: 8.4271\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.6863 - val_loss: 8.4225\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.6816 - val_loss: 8.4179\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.6768 - val_loss: 8.4134\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.6721 - val_loss: 8.4088\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.6673 - val_loss: 8.4042\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.6626 - val_loss: 8.3996\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.6579 - val_loss: 8.3950\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6531 - val_loss: 8.3904\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.6484 - val_loss: 8.3857\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6436 - val_loss: 8.3811\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.6389 - val_loss: 8.3765\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6341 - val_loss: 8.3719\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.6294 - val_loss: 8.3673\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.6246 - val_loss: 8.3626\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.6199 - val_loss: 8.3580\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.6151 - val_loss: 8.3533\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.6104 - val_loss: 8.3487\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.6056 - val_loss: 8.3441\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 658us/step - loss: 57.2661 - val_loss: 58.9190\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 57.1732 - val_loss: 58.7803\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 57.0340 - val_loss: 58.6188\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 56.8720 - val_loss: 58.4462\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 56.6991 - val_loss: 58.2685\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 56.5208 - val_loss: 58.0884\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 56.3402 - val_loss: 57.9073\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 56.1586 - val_loss: 57.7258\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.9765 - val_loss: 57.5443\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 55.7945 - val_loss: 57.3627\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 55.6124 - val_loss: 57.1812\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 55.4305 - val_loss: 56.9996\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 55.2485 - val_loss: 56.8179\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.0663 - val_loss: 56.6361\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 54.8839 - val_loss: 56.4539\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 54.7013 - val_loss: 56.2714\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 54.5185 - val_loss: 56.0885\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.3350 - val_loss: 55.9051\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 54.1512 - val_loss: 55.7211\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 53.9667 - val_loss: 55.5364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 53.7817 - val_loss: 55.3510\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 53.5958 - val_loss: 55.1649\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 53.4091 - val_loss: 54.9778\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 53.2216 - val_loss: 54.7897\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 53.0332 - val_loss: 54.6007\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 52.8437 - val_loss: 54.4106\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 52.6532 - val_loss: 54.2193\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 52.4614 - val_loss: 54.0268\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 52.2685 - val_loss: 53.8331\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 52.0742 - val_loss: 53.6380\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 51.8788 - val_loss: 53.4416\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 51.6819 - val_loss: 53.2437\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 51.4835 - val_loss: 53.0444\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 51.2837 - val_loss: 52.8435\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 51.0825 - val_loss: 52.6411\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 50.8795 - val_loss: 52.4370\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 50.6749 - val_loss: 52.2313\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 50.4687 - val_loss: 52.0238\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 50.2607 - val_loss: 51.8146\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 50.0511 - val_loss: 51.6036\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 49.8395 - val_loss: 51.3908\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 49.6263 - val_loss: 51.1761\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 49.4110 - val_loss: 50.9595\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 49.1938 - val_loss: 50.7409\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 48.9747 - val_loss: 50.5203\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 48.7537 - val_loss: 50.2978\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 48.5306 - val_loss: 50.0732\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 48.3055 - val_loss: 49.8465\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 48.0782 - val_loss: 49.6178\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 47.8490 - val_loss: 49.3869\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 47.6176 - val_loss: 49.1538\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 47.3839 - val_loss: 48.9186\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 47.1483 - val_loss: 48.6812\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 46.9103 - val_loss: 48.4416\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 46.6702 - val_loss: 48.1998\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 46.4278 - val_loss: 47.9557\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 46.1834 - val_loss: 47.7094\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 45.9366 - val_loss: 47.4609\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 45.6876 - val_loss: 47.2101\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 45.4364 - val_loss: 46.9571\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 45.1830 - val_loss: 46.7019\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.9272 - val_loss: 46.4444\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 44.6694 - val_loss: 46.1847\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 44.4093 - val_loss: 45.9227\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 44.1470 - val_loss: 45.6586\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 43.8825 - val_loss: 45.3923\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 43.6159 - val_loss: 45.1239\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 43.3471 - val_loss: 44.8532\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 43.0761 - val_loss: 44.5805\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 42.8032 - val_loss: 44.3056\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 42.5280 - val_loss: 44.0286\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 42.2508 - val_loss: 43.7496\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 41.9716 - val_loss: 43.4685\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 41.6904 - val_loss: 43.1853\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 41.4070 - val_loss: 42.9002\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 41.1217 - val_loss: 42.6130\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 40.8346 - val_loss: 42.3239\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 40.5454 - val_loss: 42.0329\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 40.2544 - val_loss: 41.7399\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 39.9614 - val_loss: 41.4450\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 39.6668 - val_loss: 41.1482\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 39.3701 - val_loss: 40.8497\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 39.0718 - val_loss: 40.5493\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 38.7716 - val_loss: 40.2471\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 38.4697 - val_loss: 39.9432\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 38.1662 - val_loss: 39.6376\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 37.8610 - val_loss: 39.3303\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 37.5542 - val_loss: 39.0215\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 37.2460 - val_loss: 38.7111\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 36.9362 - val_loss: 38.3992\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 36.6250 - val_loss: 38.0860\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 36.3125 - val_loss: 37.7714\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 35.9988 - val_loss: 37.4555\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 35.6839 - val_loss: 37.1385\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 35.3680 - val_loss: 36.8205\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 35.0510 - val_loss: 36.5015\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 34.7332 - val_loss: 36.1817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 34.4148 - val_loss: 35.8612\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 34.0956 - val_loss: 35.5401\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 33.7761 - val_loss: 35.2185\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 33.4562 - val_loss: 34.8967\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 33.1362 - val_loss: 34.5748\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 32.8161 - val_loss: 34.2529\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 32.4963 - val_loss: 33.9311\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 32.1769 - val_loss: 33.6099\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 31.8580 - val_loss: 33.2891\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 31.5399 - val_loss: 32.9692\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 31.2227 - val_loss: 32.6502\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 30.9068 - val_loss: 32.3323\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 30.5922 - val_loss: 32.0158\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 30.2792 - val_loss: 31.7009\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 29.9680 - val_loss: 31.3877\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 29.6589 - val_loss: 31.0765\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 29.3520 - val_loss: 30.7674\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 29.0476 - val_loss: 30.4607\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 28.7458 - val_loss: 30.1566\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 28.4469 - val_loss: 29.8551\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 28.1511 - val_loss: 29.5567\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 27.8586 - val_loss: 29.2613\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 27.5696 - val_loss: 28.9693\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 27.2842 - val_loss: 28.6807\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 27.0026 - val_loss: 28.3958\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 26.7251 - val_loss: 28.1146\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 26.4517 - val_loss: 27.8375\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 26.1826 - val_loss: 27.5646\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 25.9180 - val_loss: 27.2959\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 25.6580 - val_loss: 27.0317\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 25.4028 - val_loss: 26.7722\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 25.1524 - val_loss: 26.5174\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 24.9068 - val_loss: 26.2675\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 24.6665 - val_loss: 26.0227\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 24.4311 - val_loss: 25.7830\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 24.2009 - val_loss: 25.5487\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.9760 - val_loss: 25.3199\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 23.7564 - val_loss: 25.0967\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 23.5422 - val_loss: 24.8794\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 23.3335 - val_loss: 24.6679\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.1306 - val_loss: 24.4624\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 22.9338 - val_loss: 24.2629\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 22.7432 - val_loss: 24.0704\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 22.5593 - val_loss: 23.8847\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 22.3825 - val_loss: 23.7065\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 22.2124 - val_loss: 23.5355\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.0490 - val_loss: 23.3717\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 21.8919 - val_loss: 23.2127\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 21.7412 - val_loss: 23.0583\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 21.5965 - val_loss: 22.9084\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 21.4571 - val_loss: 22.7626\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.3221 - val_loss: 22.6205\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 21.1909 - val_loss: 22.4818\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 21.0632 - val_loss: 22.3463\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.9386 - val_loss: 22.2139\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 20.8169 - val_loss: 22.0844\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.6981 - val_loss: 21.9577\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.5820 - val_loss: 21.8335\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.4685 - val_loss: 21.7118\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.3576 - val_loss: 21.5925\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.2491 - val_loss: 21.4754\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.1429 - val_loss: 21.3605\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.0391 - val_loss: 21.2477\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.9374 - val_loss: 21.1369\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.8379 - val_loss: 21.0280\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.7403 - val_loss: 20.9210\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.6446 - val_loss: 20.8157\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.5508 - val_loss: 20.7121\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.4588 - val_loss: 20.6101\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.3686 - val_loss: 20.5097\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 19.2799 - val_loss: 20.4108\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.1929 - val_loss: 20.3134\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.1074 - val_loss: 20.2175\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 19.0234 - val_loss: 20.1229\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.9409 - val_loss: 20.0296\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.8597 - val_loss: 19.9377\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.7799 - val_loss: 19.8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.7015 - val_loss: 19.7576\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.6243 - val_loss: 19.6694\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 18.5485 - val_loss: 19.5825\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 18.4738 - val_loss: 19.4966\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 18.4002 - val_loss: 19.4119\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 18.3279 - val_loss: 19.3284\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 18.2566 - val_loss: 19.2459\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.1863 - val_loss: 19.1645\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 18.1172 - val_loss: 19.0841\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 18.0489 - val_loss: 19.0048\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.9817 - val_loss: 18.9264\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.9154 - val_loss: 18.8490\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.8499 - val_loss: 18.7725\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 17.7854 - val_loss: 18.6969\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 17.7216 - val_loss: 18.6222\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.6586 - val_loss: 18.5484\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.5964 - val_loss: 18.4754\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 17.5350 - val_loss: 18.4032\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.4742 - val_loss: 18.3318\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 17.4141 - val_loss: 18.2612\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.3546 - val_loss: 18.1913\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.2958 - val_loss: 18.1221\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 17.2376 - val_loss: 18.0536\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 17.1799 - val_loss: 17.9857\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 17.1228 - val_loss: 17.9185\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 17.0662 - val_loss: 17.8520\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 17.0101 - val_loss: 17.7860\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 16.9544 - val_loss: 17.7206\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.8992 - val_loss: 17.6557\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 16.8444 - val_loss: 17.5913\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.7901 - val_loss: 17.5274\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.7360 - val_loss: 17.4640\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.6824 - val_loss: 17.4011\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.6291 - val_loss: 17.3385\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.5760 - val_loss: 17.2764\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.5233 - val_loss: 17.2146\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.4708 - val_loss: 17.1532\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 16.4184 - val_loss: 17.0921\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.3664 - val_loss: 17.0312\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.3145 - val_loss: 16.9707\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 16.2628 - val_loss: 16.9104\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 16.2112 - val_loss: 16.8503\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.1597 - val_loss: 16.7904\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.1083 - val_loss: 16.7306\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.0570 - val_loss: 16.6710\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 16.0057 - val_loss: 16.6116\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 15.9545 - val_loss: 16.5522\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.9032 - val_loss: 16.4929\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.8519 - val_loss: 16.4337\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.8006 - val_loss: 16.3744\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 15.7491 - val_loss: 16.3152\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.6976 - val_loss: 16.2559\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.6460 - val_loss: 16.1966\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.5941 - val_loss: 16.1372\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 15.5421 - val_loss: 16.0777\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.4899 - val_loss: 16.0180\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.4375 - val_loss: 15.9582\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.3847 - val_loss: 15.8982\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 15.3317 - val_loss: 15.8379\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.2784 - val_loss: 15.7774\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 15.2247 - val_loss: 15.7166\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.1706 - val_loss: 15.6555\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.1162 - val_loss: 15.5941\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.0613 - val_loss: 15.5323\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 15.0060 - val_loss: 15.4701\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 14.9501 - val_loss: 15.4074\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.8938 - val_loss: 15.3444\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.8369 - val_loss: 15.2808\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 14.7794 - val_loss: 15.2167\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.7214 - val_loss: 15.1521\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 14.6627 - val_loss: 15.0869\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 14.6034 - val_loss: 15.0212\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.5435 - val_loss: 14.9549\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.4830 - val_loss: 14.8880\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 14.4218 - val_loss: 14.8206\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.3600 - val_loss: 14.7525\n",
      "Epoch 251/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.2976 - val_loss: 14.6839\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 14.2345 - val_loss: 14.6147\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.1709 - val_loss: 14.5450\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.1067 - val_loss: 14.4748\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.0420 - val_loss: 14.4040\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 13.9768 - val_loss: 14.3329\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 13.9112 - val_loss: 14.2613\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 13.8452 - val_loss: 14.1893\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 13.7788 - val_loss: 14.1171\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.7122 - val_loss: 14.0445\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.6453 - val_loss: 13.9717\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.5782 - val_loss: 13.8988\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.5110 - val_loss: 13.8256\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.4436 - val_loss: 13.7524\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.3762 - val_loss: 13.6791\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.3087 - val_loss: 13.6058\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.2413 - val_loss: 13.5325\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.1738 - val_loss: 13.4591\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.1064 - val_loss: 13.3859\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.0391 - val_loss: 13.3126\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.9718 - val_loss: 13.2395\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.9046 - val_loss: 13.1664\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 12.8374 - val_loss: 13.0935\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 12.7704 - val_loss: 13.0207\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.7036 - val_loss: 12.9481\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.6370 - val_loss: 12.8757\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 12.5706 - val_loss: 12.8035\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.5043 - val_loss: 12.7314\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 12.4381 - val_loss: 12.6596\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 12.3720 - val_loss: 12.5879\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.3060 - val_loss: 12.5165\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.2401 - val_loss: 12.4453\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 12.1744 - val_loss: 12.3744\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 12.1088 - val_loss: 12.3037\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 12.0434 - val_loss: 12.2333\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.9782 - val_loss: 12.1633\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.9132 - val_loss: 12.0935\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.8485 - val_loss: 12.0241\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.7841 - val_loss: 11.9551\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 11.7200 - val_loss: 11.8865\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 11.6563 - val_loss: 11.8183\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 11.5931 - val_loss: 11.7505\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 11.5303 - val_loss: 11.6833\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.4680 - val_loss: 11.6166\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 11.4062 - val_loss: 11.5504\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.3449 - val_loss: 11.4847\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.2842 - val_loss: 11.4197\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 11.2239 - val_loss: 11.3552\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.1643 - val_loss: 11.2914\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 11.1053 - val_loss: 11.2284\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.0469 - val_loss: 11.1660\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.9891 - val_loss: 11.1043\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.9320 - val_loss: 11.0434\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 10.8755 - val_loss: 10.9833\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 10.8198 - val_loss: 10.9240\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.7647 - val_loss: 10.8654\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 10.7104 - val_loss: 10.8076\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.6568 - val_loss: 10.7506\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 10.6041 - val_loss: 10.6945\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.5523 - val_loss: 10.6393\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.5012 - val_loss: 10.5848\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.4509 - val_loss: 10.5312\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 10.4014 - val_loss: 10.4785\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.3527 - val_loss: 10.4266\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 10.3047 - val_loss: 10.3756\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.2576 - val_loss: 10.3254\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.2113 - val_loss: 10.2762\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.1658 - val_loss: 10.2279\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.1210 - val_loss: 10.1805\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.0772 - val_loss: 10.1341\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.0343 - val_loss: 10.0887\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.9922 - val_loss: 10.0444\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.9510 - val_loss: 10.0010\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.9105 - val_loss: 9.9586\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.8708 - val_loss: 9.9171\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.8320 - val_loss: 9.8765\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.7940 - val_loss: 9.8368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.7568 - val_loss: 9.7980\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.7205 - val_loss: 9.7601\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.6851 - val_loss: 9.7231\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.6505 - val_loss: 9.6870\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.6166 - val_loss: 9.6517\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.5836 - val_loss: 9.6175\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.5514 - val_loss: 9.5842\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.5199 - val_loss: 9.5518\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.4892 - val_loss: 9.5202\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.4593 - val_loss: 9.4894\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.4301 - val_loss: 9.4594\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.4016 - val_loss: 9.4302\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.3738 - val_loss: 9.4017\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.3467 - val_loss: 9.3740\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.3203 - val_loss: 9.3471\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.2945 - val_loss: 9.3209\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.2694 - val_loss: 9.2954\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.2450 - val_loss: 9.2706\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.2212 - val_loss: 9.2466\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.1980 - val_loss: 9.2231\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.1754 - val_loss: 9.2004\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.1535 - val_loss: 9.1783\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.1321 - val_loss: 9.1568\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.1113 - val_loss: 9.1360\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.0910 - val_loss: 9.1158\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.0713 - val_loss: 9.0961\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0521 - val_loss: 9.0771\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.0335 - val_loss: 9.0586\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.0153 - val_loss: 9.0407\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9977 - val_loss: 9.0233\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.9806 - val_loss: 9.0064\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.9639 - val_loss: 8.9900\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.9477 - val_loss: 8.9742\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.9320 - val_loss: 8.9588\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.9167 - val_loss: 8.9439\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.9019 - val_loss: 8.9294\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8875 - val_loss: 8.9154\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8735 - val_loss: 8.9019\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8599 - val_loss: 8.8887\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8467 - val_loss: 8.8760\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.8339 - val_loss: 8.8636\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8214 - val_loss: 8.8517\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.8093 - val_loss: 8.8401\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7976 - val_loss: 8.8289\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7863 - val_loss: 8.8180\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7752 - val_loss: 8.8075\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.7645 - val_loss: 8.7973\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7541 - val_loss: 8.7874\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 8.7440 - val_loss: 8.7778\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7342 - val_loss: 8.7685\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7247 - val_loss: 8.7595\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7155 - val_loss: 8.7508\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7066 - val_loss: 8.7424\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.6979 - val_loss: 8.7342\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6895 - val_loss: 8.7263\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6814 - val_loss: 8.7186\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6736 - val_loss: 8.7111\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6660 - val_loss: 8.7038\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6587 - val_loss: 8.6968\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6516 - val_loss: 8.6900\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6447 - val_loss: 8.6834\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6381 - val_loss: 8.6770\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6316 - val_loss: 8.6708\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6253 - val_loss: 8.6648\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6193 - val_loss: 8.6590\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6134 - val_loss: 8.6534\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6076 - val_loss: 8.6479\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6021 - val_loss: 8.6427\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5967 - val_loss: 8.6375\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.5915 - val_loss: 8.6326\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5864 - val_loss: 8.6277\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5815 - val_loss: 8.6231\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5767 - val_loss: 8.6186\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5720 - val_loss: 8.6142\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5675 - val_loss: 8.6099\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5631 - val_loss: 8.6058\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5588 - val_loss: 8.6018\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5546 - val_loss: 8.5979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5505 - val_loss: 8.5942\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5466 - val_loss: 8.5905\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5427 - val_loss: 8.5870\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5389 - val_loss: 8.5835\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5352 - val_loss: 8.5802\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5317 - val_loss: 8.5769\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5282 - val_loss: 8.5737\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5247 - val_loss: 8.5707\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5214 - val_loss: 8.5676\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.5181 - val_loss: 8.5647\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5149 - val_loss: 8.5618\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5117 - val_loss: 8.5590\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5086 - val_loss: 8.5563\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5055 - val_loss: 8.5536\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5025 - val_loss: 8.5509\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4996 - val_loss: 8.5483\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4967 - val_loss: 8.5458\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4939 - val_loss: 8.5433\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4910 - val_loss: 8.5408\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4882 - val_loss: 8.5384\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4855 - val_loss: 8.5360\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4828 - val_loss: 8.5337\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4801 - val_loss: 8.5314\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4774 - val_loss: 8.5291\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4748 - val_loss: 8.5269\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4722 - val_loss: 8.5248\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4696 - val_loss: 8.5226\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4671 - val_loss: 8.5205\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4646 - val_loss: 8.5185\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4621 - val_loss: 8.5165\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4596 - val_loss: 8.5145\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4572 - val_loss: 8.5126\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4547 - val_loss: 8.5108\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4523 - val_loss: 8.5089\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4500 - val_loss: 8.5071\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4476 - val_loss: 8.5053\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4453 - val_loss: 8.5036\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4430 - val_loss: 8.5019\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4407 - val_loss: 8.5002\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4384 - val_loss: 8.4985\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4362 - val_loss: 8.4968\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4340 - val_loss: 8.4952\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4318 - val_loss: 8.4936\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4296 - val_loss: 8.4920\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4274 - val_loss: 8.4904\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4253 - val_loss: 8.4888\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4231 - val_loss: 8.4873\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4210 - val_loss: 8.4857\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4189 - val_loss: 8.4842\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4168 - val_loss: 8.4826\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4148 - val_loss: 8.4811\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4127 - val_loss: 8.4796\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4106 - val_loss: 8.4781\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4086 - val_loss: 8.4765\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.4066 - val_loss: 8.4750\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4045 - val_loss: 8.4735\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4025 - val_loss: 8.4720\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4005 - val_loss: 8.4705\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3985 - val_loss: 8.4691\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3966 - val_loss: 8.4676\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3946 - val_loss: 8.4661\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3927 - val_loss: 8.4647\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3907 - val_loss: 8.4632\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3888 - val_loss: 8.4618\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3868 - val_loss: 8.4603\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3849 - val_loss: 8.4589\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3830 - val_loss: 8.4575\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3811 - val_loss: 8.4561\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3792 - val_loss: 8.4546\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3773 - val_loss: 8.4532\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3754 - val_loss: 8.4518\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3735 - val_loss: 8.4503\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3716 - val_loss: 8.4488\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3697 - val_loss: 8.4474\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3679 - val_loss: 8.4459\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3660 - val_loss: 8.4444\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3642 - val_loss: 8.4429\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3623 - val_loss: 8.4415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3604 - val_loss: 8.4400\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3586 - val_loss: 8.4385\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3567 - val_loss: 8.4370\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3548 - val_loss: 8.4355\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3530 - val_loss: 8.4340\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3512 - val_loss: 8.4325\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3493 - val_loss: 8.4310\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3474 - val_loss: 8.4295\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3456 - val_loss: 8.4280\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3437 - val_loss: 8.4265\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3419 - val_loss: 8.4250\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.3400 - val_loss: 8.4235\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.3382 - val_loss: 8.4220\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3363 - val_loss: 8.4204\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3344 - val_loss: 8.4189\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3326 - val_loss: 8.4174\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3307 - val_loss: 8.4158\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3288 - val_loss: 8.4143\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3270 - val_loss: 8.4127\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3251 - val_loss: 8.4112\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3233 - val_loss: 8.4096\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3214 - val_loss: 8.4080\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3195 - val_loss: 8.4064\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3177 - val_loss: 8.4048\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3158 - val_loss: 8.4032\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3139 - val_loss: 8.4016\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3121 - val_loss: 8.4000\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3102 - val_loss: 8.3983\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3083 - val_loss: 8.3967\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3065 - val_loss: 8.3950\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3046 - val_loss: 8.3934\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3027 - val_loss: 8.3917\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3008 - val_loss: 8.3900\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.2989 - val_loss: 8.3883\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.2970 - val_loss: 8.3866\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.2952 - val_loss: 8.3849\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2933 - val_loss: 8.3832\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2914 - val_loss: 8.3815\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2895 - val_loss: 8.3798\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2876 - val_loss: 8.3781\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2856 - val_loss: 8.3763\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2837 - val_loss: 8.3746\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2818 - val_loss: 8.3728\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2799 - val_loss: 8.3710\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2780 - val_loss: 8.3693\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2760 - val_loss: 8.3675\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2741 - val_loss: 8.3657\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2722 - val_loss: 8.3639\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2702 - val_loss: 8.3621\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2683 - val_loss: 8.3602\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2663 - val_loss: 8.3584\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2644 - val_loss: 8.3566\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2624 - val_loss: 8.3547\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2605 - val_loss: 8.3528\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2585 - val_loss: 8.3510\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2565 - val_loss: 8.3491\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2545 - val_loss: 8.3472\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2525 - val_loss: 8.3453\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2505 - val_loss: 8.3434\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2486 - val_loss: 8.3414\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2466 - val_loss: 8.3395\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2445 - val_loss: 8.3376\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2425 - val_loss: 8.3356\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2405 - val_loss: 8.3337\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2385 - val_loss: 8.3317\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2365 - val_loss: 8.3297\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2344 - val_loss: 8.3277\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2324 - val_loss: 8.3257\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2303 - val_loss: 8.3237\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2283 - val_loss: 8.3217\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2262 - val_loss: 8.3197\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2242 - val_loss: 8.3177\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2221 - val_loss: 8.3156\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2200 - val_loss: 8.3136\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2179 - val_loss: 8.3115\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2159 - val_loss: 8.3095\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2138 - val_loss: 8.3074\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2116 - val_loss: 8.3053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2096 - val_loss: 8.3032\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2074 - val_loss: 8.3011\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2053 - val_loss: 8.2990\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2032 - val_loss: 8.2969\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2010 - val_loss: 8.2948\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1989 - val_loss: 8.2926\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1968 - val_loss: 8.2905\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.1946 - val_loss: 8.2883\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.1924 - val_loss: 8.2862\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 8.1903 - val_loss: 8.2840\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1881 - val_loss: 8.2818\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1859 - val_loss: 8.2796\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1837 - val_loss: 8.2774\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1815 - val_loss: 8.2752\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1793 - val_loss: 8.2730\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1771 - val_loss: 8.2708\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1749 - val_loss: 8.2685\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1726 - val_loss: 8.2663\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1704 - val_loss: 8.2640\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1681 - val_loss: 8.2618\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1659 - val_loss: 8.2595\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1636 - val_loss: 8.2572\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1614 - val_loss: 8.2549\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1591 - val_loss: 8.2526\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1568 - val_loss: 8.2503\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1545 - val_loss: 8.2480\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1522 - val_loss: 8.2457\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1499 - val_loss: 8.2433\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1476 - val_loss: 8.2410\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1453 - val_loss: 8.2386\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1429 - val_loss: 8.2362\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1406 - val_loss: 8.2338\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1383 - val_loss: 8.2315\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1359 - val_loss: 8.2290\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1335 - val_loss: 8.2266\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1312 - val_loss: 8.2242\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1288 - val_loss: 8.2218\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1264 - val_loss: 8.2193\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1240 - val_loss: 8.2169\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 562us/step - loss: 56.9082 - val_loss: 56.6192\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 56.8174 - val_loss: 56.4829\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 56.6814 - val_loss: 56.3239\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 56.5225 - val_loss: 56.1536\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 56.3525 - val_loss: 55.9776\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 56.1767 - val_loss: 55.7986\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.9979 - val_loss: 55.6180\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.8174 - val_loss: 55.4364\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.6361 - val_loss: 55.2541\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 55.4539 - val_loss: 55.0711\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.2710 - val_loss: 54.8874\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 55.0877 - val_loss: 54.7031\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 54.9034 - val_loss: 54.5180\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.7187 - val_loss: 54.3320\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 54.5328 - val_loss: 54.1452\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 54.3461 - val_loss: 53.9573\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 54.1585 - val_loss: 53.7684\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 53.9697 - val_loss: 53.5783\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 53.7797 - val_loss: 53.3869\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 53.5887 - val_loss: 53.1943\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 53.3961 - val_loss: 53.0002\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 53.2021 - val_loss: 52.8047\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 53.0068 - val_loss: 52.6076\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 52.8100 - val_loss: 52.4090\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 52.6114 - val_loss: 52.2087\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 52.4112 - val_loss: 52.0067\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 52.2094 - val_loss: 51.8029\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 52.0058 - val_loss: 51.5973\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 51.8002 - val_loss: 51.3898\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 51.5928 - val_loss: 51.1804\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 51.3836 - val_loss: 50.9690\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 51.1723 - val_loss: 50.7556\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 50.9589 - val_loss: 50.5402\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 50.7436 - val_loss: 50.3227\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 50.5263 - val_loss: 50.1030\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 50.3066 - val_loss: 49.8813\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 50.0849 - val_loss: 49.6573\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 49.8611 - val_loss: 49.4312\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 49.6351 - val_loss: 49.2029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 49.4067 - val_loss: 48.9724\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 49.1762 - val_loss: 48.7396\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 48.9436 - val_loss: 48.5046\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 48.7086 - val_loss: 48.2674\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 48.4715 - val_loss: 48.0280\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 48.2320 - val_loss: 47.7863\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 47.9903 - val_loss: 47.5424\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 47.7465 - val_loss: 47.2963\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 47.5003 - val_loss: 47.0479\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 47.2519 - val_loss: 46.7973\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 47.0014 - val_loss: 46.5446\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 46.7485 - val_loss: 46.2896\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 46.4936 - val_loss: 46.0325\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 46.2364 - val_loss: 45.7732\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 45.9771 - val_loss: 45.5118\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 45.7156 - val_loss: 45.2483\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 45.4521 - val_loss: 44.9827\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 45.1864 - val_loss: 44.7150\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 44.9186 - val_loss: 44.4452\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 44.6488 - val_loss: 44.1734\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.3769 - val_loss: 43.8997\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.1030 - val_loss: 43.6239\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 43.8271 - val_loss: 43.3462\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 43.5492 - val_loss: 43.0665\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 43.2695 - val_loss: 42.7850\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 42.9877 - val_loss: 42.5016\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 42.7041 - val_loss: 42.2163\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 42.4186 - val_loss: 41.9292\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 42.1313 - val_loss: 41.6403\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 41.8421 - val_loss: 41.3497\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 41.5512 - val_loss: 41.0573\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 41.2585 - val_loss: 40.7632\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 40.9641 - val_loss: 40.4675\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 40.6681 - val_loss: 40.1701\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 40.3704 - val_loss: 39.8712\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 40.0711 - val_loss: 39.5707\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 39.7702 - val_loss: 39.2688\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 39.4678 - val_loss: 38.9653\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 39.1639 - val_loss: 38.6605\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 38.8585 - val_loss: 38.3544\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 38.5518 - val_loss: 38.0469\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 38.2438 - val_loss: 37.7383\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 37.9345 - val_loss: 37.4285\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 37.6241 - val_loss: 37.1175\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 37.3125 - val_loss: 36.8057\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 36.9998 - val_loss: 36.4928\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 36.6862 - val_loss: 36.1792\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 36.3717 - val_loss: 35.8648\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 36.0564 - val_loss: 35.5498\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 35.7404 - val_loss: 35.2343\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 35.4238 - val_loss: 34.9183\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 35.1069 - val_loss: 34.6021\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 34.7895 - val_loss: 34.2857\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 34.4718 - val_loss: 33.9693\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 34.1541 - val_loss: 33.6531\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 33.8364 - val_loss: 33.3371\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 33.5189 - val_loss: 33.0215\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 33.2017 - val_loss: 32.7065\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 32.8850 - val_loss: 32.3923\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 32.5691 - val_loss: 32.0790\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 32.2539 - val_loss: 31.7669\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 31.9397 - val_loss: 31.4559\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 31.6267 - val_loss: 31.1465\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 31.3149 - val_loss: 30.8387\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 31.0048 - val_loss: 30.5327\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 30.6963 - val_loss: 30.2288\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 30.3897 - val_loss: 29.9269\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 30.0853 - val_loss: 29.6274\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 29.7830 - val_loss: 29.3305\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 29.4832 - val_loss: 29.0362\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 29.1860 - val_loss: 28.7447\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 28.8916 - val_loss: 28.4563\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 28.6001 - val_loss: 28.1709\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 28.3117 - val_loss: 27.8889\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 28.0266 - val_loss: 27.6103\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 27.7449 - val_loss: 27.3352\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 27.4668 - val_loss: 27.0637\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 27.1923 - val_loss: 26.7961\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 26.9218 - val_loss: 26.5324\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 26.6552 - val_loss: 26.2726\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 26.3928 - val_loss: 26.0170\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 26.1346 - val_loss: 25.7655\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 25.8808 - val_loss: 25.5184\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 25.6315 - val_loss: 25.2757\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 25.3867 - val_loss: 25.0375\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 25.1467 - val_loss: 24.8039\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.9112 - val_loss: 24.5750\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.6807 - val_loss: 24.3509\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.4549 - val_loss: 24.1316\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 24.2339 - val_loss: 23.9173\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 24.0178 - val_loss: 23.7080\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 23.8067 - val_loss: 23.5038\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 23.6005 - val_loss: 23.3050\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 23.3994 - val_loss: 23.1120\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.2035 - val_loss: 22.9248\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 23.0130 - val_loss: 22.7431\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 22.8282 - val_loss: 22.5668\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.6502 - val_loss: 22.3968\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.4798 - val_loss: 22.2329\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.3159 - val_loss: 22.0745\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.1587 - val_loss: 21.9214\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 22.0072 - val_loss: 21.7740\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 21.8606 - val_loss: 21.6319\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.7186 - val_loss: 21.4941\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.5807 - val_loss: 21.3605\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.4464 - val_loss: 21.2305\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.3153 - val_loss: 21.1035\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 21.1871 - val_loss: 20.9794\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.0617 - val_loss: 20.8581\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.9389 - val_loss: 20.7393\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 20.8186 - val_loss: 20.6232\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 20.7007 - val_loss: 20.5094\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.5851 - val_loss: 20.3981\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 20.4717 - val_loss: 20.2891\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.3604 - val_loss: 20.1823\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 20.2512 - val_loss: 20.0778\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 20.1439 - val_loss: 19.9754\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.0385 - val_loss: 19.8751\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.9349 - val_loss: 19.7768\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.8331 - val_loss: 19.6805\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 19.7328 - val_loss: 19.5861\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.6342 - val_loss: 19.4936\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.5372 - val_loss: 19.4028\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.4416 - val_loss: 19.3138\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 19.3475 - val_loss: 19.2264\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 19.2547 - val_loss: 19.1406\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.1634 - val_loss: 19.0563\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.0734 - val_loss: 18.9735\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.9847 - val_loss: 18.8922\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.8973 - val_loss: 18.8123\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.8112 - val_loss: 18.7337\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 18.7263 - val_loss: 18.6565\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.6426 - val_loss: 18.5805\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.5601 - val_loss: 18.5057\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.4787 - val_loss: 18.4321\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.3985 - val_loss: 18.3596\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.3194 - val_loss: 18.2883\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.2415 - val_loss: 18.2180\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.1645 - val_loss: 18.1487\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.0886 - val_loss: 18.0803\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.0137 - val_loss: 18.0129\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.9398 - val_loss: 17.9465\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.8669 - val_loss: 17.8808\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.7949 - val_loss: 17.8160\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.7238 - val_loss: 17.7520\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 17.6536 - val_loss: 17.6888\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.5842 - val_loss: 17.6263\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 17.5157 - val_loss: 17.5644\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.4480 - val_loss: 17.5032\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.3810 - val_loss: 17.4427\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.3148 - val_loss: 17.3828\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.2493 - val_loss: 17.3234\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.1846 - val_loss: 17.2646\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.1205 - val_loss: 17.2063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.0571 - val_loss: 17.1484\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.9943 - val_loss: 17.0911\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.9321 - val_loss: 17.0341\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.8704 - val_loss: 16.9776\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.8094 - val_loss: 16.9215\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.7488 - val_loss: 16.8657\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.6888 - val_loss: 16.8103\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.6292 - val_loss: 16.7552\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.5700 - val_loss: 16.7004\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.5113 - val_loss: 16.6458\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.4531 - val_loss: 16.5915\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.3951 - val_loss: 16.5374\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.3375 - val_loss: 16.4835\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.2802 - val_loss: 16.4298\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.2233 - val_loss: 16.3763\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.1666 - val_loss: 16.3229\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.1102 - val_loss: 16.2696\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.0540 - val_loss: 16.2165\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.9981 - val_loss: 16.1635\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.9424 - val_loss: 16.1105\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.8868 - val_loss: 16.0576\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.8314 - val_loss: 16.0048\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.7762 - val_loss: 15.9520\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.7211 - val_loss: 15.8993\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.6661 - val_loss: 15.8466\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.6113 - val_loss: 15.7939\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.5565 - val_loss: 15.7412\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.5018 - val_loss: 15.6885\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.4471 - val_loss: 15.6358\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.3926 - val_loss: 15.5831\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.3381 - val_loss: 15.5304\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.2837 - val_loss: 15.4777\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.2293 - val_loss: 15.4249\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.1749 - val_loss: 15.3722\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.1207 - val_loss: 15.3194\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.0664 - val_loss: 15.2666\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.0121 - val_loss: 15.2138\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.9579 - val_loss: 15.1610\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.9037 - val_loss: 15.1082\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.8496 - val_loss: 15.0554\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.7954 - val_loss: 15.0025\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.7413 - val_loss: 14.9497\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.6872 - val_loss: 14.8968\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.6332 - val_loss: 14.8439\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 14.5792 - val_loss: 14.7911\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.5252 - val_loss: 14.7382\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.4712 - val_loss: 14.6854\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.4173 - val_loss: 14.6325\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.3634 - val_loss: 14.5797\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.3096 - val_loss: 14.5269\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.2557 - val_loss: 14.4741\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.2020 - val_loss: 14.4213\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.1483 - val_loss: 14.3685\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.0946 - val_loss: 14.3158\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.0409 - val_loss: 14.2631\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.9873 - val_loss: 14.2104\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.9338 - val_loss: 14.1577\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 13.8803 - val_loss: 14.1050\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.8268 - val_loss: 14.0523\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.7734 - val_loss: 13.9997\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.7201 - val_loss: 13.9471\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.6667 - val_loss: 13.8945\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.6134 - val_loss: 13.8419\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.5601 - val_loss: 13.7892\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.5068 - val_loss: 13.7366\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 13.4536 - val_loss: 13.6840\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.4004 - val_loss: 13.6313\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.3472 - val_loss: 13.5786\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.2940 - val_loss: 13.5259\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.2408 - val_loss: 13.4731\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.1876 - val_loss: 13.4203\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.1343 - val_loss: 13.3674\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.0811 - val_loss: 13.3144\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.0277 - val_loss: 13.2613\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.9744 - val_loss: 13.2081\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.9209 - val_loss: 13.1547\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.8674 - val_loss: 13.1012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.8138 - val_loss: 13.0476\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.7600 - val_loss: 12.9937\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.7062 - val_loss: 12.9397\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 12.6523 - val_loss: 12.8854\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.5982 - val_loss: 12.8309\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 12.5439 - val_loss: 12.7762\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.4895 - val_loss: 12.7212\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.4349 - val_loss: 12.6659\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.3802 - val_loss: 12.6103\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.3252 - val_loss: 12.5544\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.2701 - val_loss: 12.4982\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.2147 - val_loss: 12.4417\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.1592 - val_loss: 12.3848\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.1034 - val_loss: 12.3276\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.0474 - val_loss: 12.2700\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.9911 - val_loss: 12.2120\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.9347 - val_loss: 12.1537\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.8781 - val_loss: 12.0950\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.8213 - val_loss: 12.0360\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.7642 - val_loss: 11.9766\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.7072 - val_loss: 11.9170\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.6501 - val_loss: 11.8572\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.5930 - val_loss: 11.7972\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.5358 - val_loss: 11.7368\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.4784 - val_loss: 11.6763\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.4211 - val_loss: 11.6155\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.3637 - val_loss: 11.5545\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.3063 - val_loss: 11.4934\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.2489 - val_loss: 11.4321\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.1915 - val_loss: 11.3707\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.1343 - val_loss: 11.3093\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.0772 - val_loss: 11.2479\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.0202 - val_loss: 11.1865\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.9635 - val_loss: 11.1253\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.9071 - val_loss: 11.0643\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.8510 - val_loss: 11.0036\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.7953 - val_loss: 10.9433\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.7400 - val_loss: 10.8835\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.6853 - val_loss: 10.8243\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.6312 - val_loss: 10.7657\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.5777 - val_loss: 10.7079\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.5247 - val_loss: 10.6508\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 10.4724 - val_loss: 10.5946\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 10.4208 - val_loss: 10.5391\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.3698 - val_loss: 10.4846\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.3195 - val_loss: 10.4308\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.2701 - val_loss: 10.3779\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.2214 - val_loss: 10.3259\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.1736 - val_loss: 10.2748\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.1266 - val_loss: 10.2246\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.0805 - val_loss: 10.1753\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.0352 - val_loss: 10.1276\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.9909 - val_loss: 10.0809\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.9475 - val_loss: 10.0352\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.9050 - val_loss: 9.9905\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.8634 - val_loss: 9.9467\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.8227 - val_loss: 9.9040\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.7829 - val_loss: 9.8623\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.7441 - val_loss: 9.8217\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.7062 - val_loss: 9.7822\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.6691 - val_loss: 9.7436\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.6330 - val_loss: 9.7060\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.5978 - val_loss: 9.6693\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.5635 - val_loss: 9.6335\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.5301 - val_loss: 9.5986\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.4976 - val_loss: 9.5647\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.4660 - val_loss: 9.5316\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.4353 - val_loss: 9.4994\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.4055 - val_loss: 9.4681\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.3764 - val_loss: 9.4376\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.3482 - val_loss: 9.4079\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.3207 - val_loss: 9.3790\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.2940 - val_loss: 9.3509\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.2680 - val_loss: 9.3236\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.2428 - val_loss: 9.2970\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.2183 - val_loss: 9.2713\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.1945 - val_loss: 9.2463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.1716 - val_loss: 9.2223\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.1493 - val_loss: 9.1990\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.1278 - val_loss: 9.1766\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.1070 - val_loss: 9.1551\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0868 - val_loss: 9.1342\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.0673 - val_loss: 9.1141\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.0484 - val_loss: 9.0946\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.0300 - val_loss: 9.0757\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.0122 - val_loss: 9.0574\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.9949 - val_loss: 9.0396\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.9782 - val_loss: 9.0225\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.9620 - val_loss: 9.0058\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9463 - val_loss: 8.9897\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9311 - val_loss: 8.9740\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.9163 - val_loss: 8.9589\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9020 - val_loss: 8.9442\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8882 - val_loss: 8.9301\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8747 - val_loss: 8.9163\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8618 - val_loss: 8.9031\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8492 - val_loss: 8.8902\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8370 - val_loss: 8.8777\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8252 - val_loss: 8.8656\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8137 - val_loss: 8.8538\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8026 - val_loss: 8.8424\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7919 - val_loss: 8.8313\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7815 - val_loss: 8.8205\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7714 - val_loss: 8.8101\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7616 - val_loss: 8.7999\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7522 - val_loss: 8.7900\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7430 - val_loss: 8.7803\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7341 - val_loss: 8.7710\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7255 - val_loss: 8.7619\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7172 - val_loss: 8.7530\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7091 - val_loss: 8.7444\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7013 - val_loss: 8.7360\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6938 - val_loss: 8.7279\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6864 - val_loss: 8.7200\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6793 - val_loss: 8.7123\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6724 - val_loss: 8.7048\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6658 - val_loss: 8.6975\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6593 - val_loss: 8.6904\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6530 - val_loss: 8.6834\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.6470 - val_loss: 8.6767\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6411 - val_loss: 8.6701\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6354 - val_loss: 8.6637\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6299 - val_loss: 8.6575\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6245 - val_loss: 8.6515\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6193 - val_loss: 8.6456\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6142 - val_loss: 8.6399\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6093 - val_loss: 8.6343\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6045 - val_loss: 8.6289\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5999 - val_loss: 8.6237\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5954 - val_loss: 8.6186\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5909 - val_loss: 8.6136\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5866 - val_loss: 8.6087\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5825 - val_loss: 8.6039\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5784 - val_loss: 8.5993\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5744 - val_loss: 8.5947\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5706 - val_loss: 8.5903\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5668 - val_loss: 8.5859\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5631 - val_loss: 8.5816\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5595 - val_loss: 8.5774\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5560 - val_loss: 8.5733\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5526 - val_loss: 8.5692\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5492 - val_loss: 8.5653\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5459 - val_loss: 8.5614\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5426 - val_loss: 8.5575\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5394 - val_loss: 8.5538\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.5363 - val_loss: 8.5502\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5332 - val_loss: 8.5467\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5301 - val_loss: 8.5432\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5271 - val_loss: 8.5398\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5241 - val_loss: 8.5364\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5212 - val_loss: 8.5331\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5183 - val_loss: 8.5298\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5154 - val_loss: 8.5266\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5126 - val_loss: 8.5234\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5099 - val_loss: 8.5203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5071 - val_loss: 8.5172\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5044 - val_loss: 8.5141\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5018 - val_loss: 8.5111\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4991 - val_loss: 8.5081\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4965 - val_loss: 8.5052\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4939 - val_loss: 8.5023\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4914 - val_loss: 8.4994\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4888 - val_loss: 8.4965\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4863 - val_loss: 8.4937\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4838 - val_loss: 8.4909\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4813 - val_loss: 8.4881\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4788 - val_loss: 8.4853\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4763 - val_loss: 8.4826\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4739 - val_loss: 8.4798\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4714 - val_loss: 8.4771\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4690 - val_loss: 8.4744\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4666 - val_loss: 8.4718\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4642 - val_loss: 8.4691\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4618 - val_loss: 8.4665\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4594 - val_loss: 8.4639\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4571 - val_loss: 8.4613\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4548 - val_loss: 8.4588\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4525 - val_loss: 8.4562\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4502 - val_loss: 8.4537\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4480 - val_loss: 8.4513\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4457 - val_loss: 8.4488\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4435 - val_loss: 8.4463\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4412 - val_loss: 8.4439\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4390 - val_loss: 8.4415\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4368 - val_loss: 8.4391\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4346 - val_loss: 8.4367\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4324 - val_loss: 8.4343\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4302 - val_loss: 8.4320\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4280 - val_loss: 8.4296\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4258 - val_loss: 8.4273\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4237 - val_loss: 8.4250\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4215 - val_loss: 8.4227\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4193 - val_loss: 8.4204\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4172 - val_loss: 8.4181\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4150 - val_loss: 8.4158\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4128 - val_loss: 8.4136\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4107 - val_loss: 8.4113\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4085 - val_loss: 8.4091\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4064 - val_loss: 8.4069\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4042 - val_loss: 8.4047\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.4020 - val_loss: 8.4026\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3999 - val_loss: 8.4004\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3977 - val_loss: 8.3983\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3956 - val_loss: 8.3961\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3934 - val_loss: 8.3940\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3913 - val_loss: 8.3918\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 8.3891 - val_loss: 8.3897\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.3869 - val_loss: 8.3876\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3848 - val_loss: 8.3855\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3826 - val_loss: 8.3833\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3804 - val_loss: 8.3812\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3783 - val_loss: 8.3791\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3761 - val_loss: 8.3770\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3739 - val_loss: 8.3750\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3717 - val_loss: 8.3729\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3696 - val_loss: 8.3708\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3674 - val_loss: 8.3688\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.3652 - val_loss: 8.3668\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3630 - val_loss: 8.3647\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3608 - val_loss: 8.3627\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3586 - val_loss: 8.3606\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3564 - val_loss: 8.3586\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3542 - val_loss: 8.3566\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3520 - val_loss: 8.3545\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3498 - val_loss: 8.3525\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3476 - val_loss: 8.3504\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3454 - val_loss: 8.3484\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3432 - val_loss: 8.3463\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3410 - val_loss: 8.3443\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3388 - val_loss: 8.3422\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3366 - val_loss: 8.3401\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3344 - val_loss: 8.3380\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3321 - val_loss: 8.3359\n",
      "Epoch 504/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3299 - val_loss: 8.3337\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3277 - val_loss: 8.3316\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3254 - val_loss: 8.3295\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3232 - val_loss: 8.3273\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3209 - val_loss: 8.3252\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3186 - val_loss: 8.3230\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3163 - val_loss: 8.3208\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3140 - val_loss: 8.3187\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3117 - val_loss: 8.3165\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3094 - val_loss: 8.3143\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3071 - val_loss: 8.3121\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3048 - val_loss: 8.3100\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3025 - val_loss: 8.3078\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3001 - val_loss: 8.3056\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2978 - val_loss: 8.3034\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2954 - val_loss: 8.3011\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2930 - val_loss: 8.2989\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2906 - val_loss: 8.2967\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2882 - val_loss: 8.2945\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2858 - val_loss: 8.2922\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2834 - val_loss: 8.2900\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2810 - val_loss: 8.2877\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2786 - val_loss: 8.2854\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2761 - val_loss: 8.2832\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2737 - val_loss: 8.2809\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2712 - val_loss: 8.2786\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2687 - val_loss: 8.2763\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2663 - val_loss: 8.2740\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2638 - val_loss: 8.2717\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2613 - val_loss: 8.2693\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2587 - val_loss: 8.2670\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2562 - val_loss: 8.2646\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2537 - val_loss: 8.2623\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2511 - val_loss: 8.2599\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2486 - val_loss: 8.2576\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2460 - val_loss: 8.2552\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2434 - val_loss: 8.2528\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2408 - val_loss: 8.2504\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2382 - val_loss: 8.2480\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2356 - val_loss: 8.2455\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2330 - val_loss: 8.2431\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2303 - val_loss: 8.2406\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2277 - val_loss: 8.2382\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2251 - val_loss: 8.2357\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2224 - val_loss: 8.2332\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2197 - val_loss: 8.2308\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2170 - val_loss: 8.2283\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2143 - val_loss: 8.2257\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2116 - val_loss: 8.2232\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2089 - val_loss: 8.2207\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2061 - val_loss: 8.2181\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2034 - val_loss: 8.2156\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2006 - val_loss: 8.2130\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1979 - val_loss: 8.2104\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1951 - val_loss: 8.2078\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1923 - val_loss: 8.2052\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1894 - val_loss: 8.2026\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1866 - val_loss: 8.2000\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.1838 - val_loss: 8.1974\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1810 - val_loss: 8.1947\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1781 - val_loss: 8.1920\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1752 - val_loss: 8.1894\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1723 - val_loss: 8.1867\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1694 - val_loss: 8.1840\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1665 - val_loss: 8.1813\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1636 - val_loss: 8.1785\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1607 - val_loss: 8.1758\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1578 - val_loss: 8.1731\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1548 - val_loss: 8.1703\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1518 - val_loss: 8.1675\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1489 - val_loss: 8.1647\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1459 - val_loss: 8.1619\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1429 - val_loss: 8.1591\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1399 - val_loss: 8.1563\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1368 - val_loss: 8.1534\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1338 - val_loss: 8.1506\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1307 - val_loss: 8.1477\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1277 - val_loss: 8.1448\n",
      "Epoch 582/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1246 - val_loss: 8.1420\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1215 - val_loss: 8.1391\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1184 - val_loss: 8.1361\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1153 - val_loss: 8.1332\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1122 - val_loss: 8.1303\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1091 - val_loss: 8.1273\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1059 - val_loss: 8.1243\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1028 - val_loss: 8.1213\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0996 - val_loss: 8.1184\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0964 - val_loss: 8.1153\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0932 - val_loss: 8.1123\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0900 - val_loss: 8.1093\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0868 - val_loss: 8.1062\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0835 - val_loss: 8.1032\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0803 - val_loss: 8.1001\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0770 - val_loss: 8.0970\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.0738 - val_loss: 8.0939\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.0705 - val_loss: 8.0908\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0672 - val_loss: 8.0877\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 569us/step - loss: 57.1392 - val_loss: 56.8176\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 57.0444 - val_loss: 56.6754\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 56.9022 - val_loss: 56.5095\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 56.7361 - val_loss: 56.3317\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 56.5581 - val_loss: 56.1478\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 56.3740 - val_loss: 55.9607\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 56.1869 - val_loss: 55.7718\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 55.9979 - val_loss: 55.5817\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 55.8076 - val_loss: 55.3907\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 55.6165 - val_loss: 55.1990\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 55.4246 - val_loss: 55.0064\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.2319 - val_loss: 54.8129\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 55.0384 - val_loss: 54.6185\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.8438 - val_loss: 54.4232\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.6484 - val_loss: 54.2267\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.4520 - val_loss: 54.0290\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 54.2542 - val_loss: 53.8302\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.0552 - val_loss: 53.6300\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 53.8550 - val_loss: 53.4284\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 53.6534 - val_loss: 53.2254\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 53.4503 - val_loss: 53.0208\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 53.2459 - val_loss: 52.8147\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 53.0396 - val_loss: 52.6070\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 52.8320 - val_loss: 52.3975\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 52.6225 - val_loss: 52.1864\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 52.4116 - val_loss: 51.9735\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 52.1987 - val_loss: 51.7589\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 51.9842 - val_loss: 51.5423\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 51.7677 - val_loss: 51.3240\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 51.5496 - val_loss: 51.1038\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 51.3295 - val_loss: 50.8817\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 51.1075 - val_loss: 50.6578\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 50.8837 - val_loss: 50.4319\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 50.6580 - val_loss: 50.2041\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 50.4305 - val_loss: 49.9744\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 50.2010 - val_loss: 49.7428\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 49.9696 - val_loss: 49.5093\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.7363 - val_loss: 49.2739\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 49.5013 - val_loss: 49.0366\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 49.2641 - val_loss: 48.7974\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 49.0253 - val_loss: 48.5564\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 48.7846 - val_loss: 48.3135\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 48.5421 - val_loss: 48.0688\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 48.2978 - val_loss: 47.8222\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 48.0517 - val_loss: 47.5739\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 47.8038 - val_loss: 47.3238\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 47.5542 - val_loss: 47.0720\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 47.3027 - val_loss: 46.8184\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 47.0498 - val_loss: 46.5631\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 46.7949 - val_loss: 46.3061\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 46.5385 - val_loss: 46.0474\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 46.2804 - val_loss: 45.7871\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 46.0208 - val_loss: 45.5251\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 45.7594 - val_loss: 45.2616\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 45.4965 - val_loss: 44.9965\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 45.2321 - val_loss: 44.7297\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.9661 - val_loss: 44.4615\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 44.6987 - val_loss: 44.1917\n",
      "Epoch 59/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.4296 - val_loss: 43.9204\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 44.1591 - val_loss: 43.6476\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 43.8872 - val_loss: 43.3734\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 43.6138 - val_loss: 43.0976\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 43.3389 - val_loss: 42.8204\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 43.0626 - val_loss: 42.5418\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 42.7851 - val_loss: 42.2618\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 42.5060 - val_loss: 41.9804\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 42.2256 - val_loss: 41.6976\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 41.9438 - val_loss: 41.4134\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 41.6608 - val_loss: 41.1279\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 41.3764 - val_loss: 40.8411\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 41.0908 - val_loss: 40.5529\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 40.8038 - val_loss: 40.2635\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 40.5156 - val_loss: 39.9728\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 40.2262 - val_loss: 39.6808\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 39.9356 - val_loss: 39.3877\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 39.6439 - val_loss: 39.0933\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 39.3510 - val_loss: 38.7978\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 39.0570 - val_loss: 38.5012\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 38.7619 - val_loss: 38.2035\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 38.4658 - val_loss: 37.9047\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 38.1687 - val_loss: 37.6049\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 37.8707 - val_loss: 37.3042\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 37.5718 - val_loss: 37.0025\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 37.2721 - val_loss: 36.7001\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 36.9716 - val_loss: 36.3968\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 36.6704 - val_loss: 36.0928\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 36.3686 - val_loss: 35.7881\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 36.0662 - val_loss: 35.4829\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 35.7634 - val_loss: 35.1772\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 35.4602 - val_loss: 34.8711\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 35.1567 - val_loss: 34.5646\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 34.8530 - val_loss: 34.2579\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 34.5491 - val_loss: 33.9511\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 34.2453 - val_loss: 33.6443\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 33.9417 - val_loss: 33.3376\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 33.6382 - val_loss: 33.0311\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 33.3353 - val_loss: 32.7250\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 33.0327 - val_loss: 32.4193\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 32.7309 - val_loss: 32.1142\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 32.4299 - val_loss: 31.8099\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 32.1297 - val_loss: 31.5064\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 31.8306 - val_loss: 31.2040\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 31.5328 - val_loss: 30.9027\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 31.2363 - val_loss: 30.6027\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 30.9414 - val_loss: 30.3042\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 30.6482 - val_loss: 30.0073\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 30.3569 - val_loss: 29.7122\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 30.0674 - val_loss: 29.4190\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 29.7802 - val_loss: 29.1278\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 29.4954 - val_loss: 28.8389\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 29.2129 - val_loss: 28.5523\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 28.9331 - val_loss: 28.2683\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 28.6560 - val_loss: 27.9869\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 28.3819 - val_loss: 27.7083\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 28.1108 - val_loss: 27.4327\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 27.8429 - val_loss: 27.1602\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 27.5784 - val_loss: 26.8910\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 27.3172 - val_loss: 26.6252\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 27.0598 - val_loss: 26.3630\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 26.8059 - val_loss: 26.1044\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 26.5559 - val_loss: 25.8498\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 26.3099 - val_loss: 25.5991\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 26.0679 - val_loss: 25.3526\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 25.8300 - val_loss: 25.1102\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 25.5964 - val_loss: 24.8723\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 25.3670 - val_loss: 24.6387\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 25.1421 - val_loss: 24.4096\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.9218 - val_loss: 24.1851\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.7061 - val_loss: 23.9651\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.4949 - val_loss: 23.7498\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 24.2887 - val_loss: 23.5391\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.0874 - val_loss: 23.3333\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.8911 - val_loss: 23.1323\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 23.7002 - val_loss: 22.9364\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.5149 - val_loss: 22.7457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 23.3354 - val_loss: 22.5606\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 23.1619 - val_loss: 22.3816\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.9949 - val_loss: 22.2083\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.8345 - val_loss: 22.0406\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.6804 - val_loss: 21.8796\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.5320 - val_loss: 21.7249\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.3887 - val_loss: 21.5763\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 22.2496 - val_loss: 21.4320\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 22.1146 - val_loss: 21.2916\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 21.9835 - val_loss: 21.1547\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 21.8556 - val_loss: 21.0209\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.7306 - val_loss: 20.8902\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.6082 - val_loss: 20.7623\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 21.4883 - val_loss: 20.6372\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.3707 - val_loss: 20.5146\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.2553 - val_loss: 20.3944\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.1419 - val_loss: 20.2765\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.0306 - val_loss: 20.1607\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.9212 - val_loss: 20.0469\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.8136 - val_loss: 19.9351\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.7079 - val_loss: 19.8252\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.6039 - val_loss: 19.7170\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 20.5015 - val_loss: 19.6105\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 20.4008 - val_loss: 19.5056\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.3016 - val_loss: 19.4023\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 20.2039 - val_loss: 19.3006\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.1076 - val_loss: 19.2003\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.0127 - val_loss: 19.1015\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.9191 - val_loss: 19.0041\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.8267 - val_loss: 18.9080\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 19.7356 - val_loss: 18.8133\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.6457 - val_loss: 18.7199\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.5569 - val_loss: 18.6277\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.4693 - val_loss: 18.5367\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.3826 - val_loss: 18.4469\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.2972 - val_loss: 18.3582\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.2127 - val_loss: 18.2706\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.1292 - val_loss: 18.1841\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 19.0468 - val_loss: 18.0986\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 18.9653 - val_loss: 18.0141\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.8847 - val_loss: 17.9305\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.8051 - val_loss: 17.8479\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.7264 - val_loss: 17.7662\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 18.6485 - val_loss: 17.6853\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.5714 - val_loss: 17.6053\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.4952 - val_loss: 17.5261\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.4198 - val_loss: 17.4477\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.3452 - val_loss: 17.3700\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 18.2713 - val_loss: 17.2931\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.1981 - val_loss: 17.2170\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.1257 - val_loss: 17.1415\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.0539 - val_loss: 17.0667\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.9828 - val_loss: 16.9927\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.9123 - val_loss: 16.9193\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.8424 - val_loss: 16.8465\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.7732 - val_loss: 16.7744\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.7044 - val_loss: 16.7029\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.6363 - val_loss: 16.6321\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.5687 - val_loss: 16.5618\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.5016 - val_loss: 16.4921\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.4350 - val_loss: 16.4230\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.3690 - val_loss: 16.3545\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.3033 - val_loss: 16.2865\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.2381 - val_loss: 16.2190\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.1734 - val_loss: 16.1520\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.1091 - val_loss: 16.0856\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.0451 - val_loss: 16.0196\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.9816 - val_loss: 15.9542\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.9185 - val_loss: 15.8891\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.8558 - val_loss: 15.8246\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.7933 - val_loss: 15.7605\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.7313 - val_loss: 15.6968\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.6695 - val_loss: 15.6335\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 16.6081 - val_loss: 15.5706\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 16.5471 - val_loss: 15.5081\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.4863 - val_loss: 15.4459\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.4258 - val_loss: 15.3842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.3656 - val_loss: 15.3227\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.3057 - val_loss: 15.2616\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.2461 - val_loss: 15.2008\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.1867 - val_loss: 15.1404\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.1276 - val_loss: 15.0802\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.0687 - val_loss: 15.0203\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.0101 - val_loss: 14.9607\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.9517 - val_loss: 14.9014\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.8935 - val_loss: 14.8424\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.8356 - val_loss: 14.7836\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.7779 - val_loss: 14.7250\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.7204 - val_loss: 14.6667\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.6632 - val_loss: 14.6086\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.6062 - val_loss: 14.5508\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.5493 - val_loss: 14.4931\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.4926 - val_loss: 14.4357\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.4362 - val_loss: 14.3785\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.3800 - val_loss: 14.3215\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.3239 - val_loss: 14.2646\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.2681 - val_loss: 14.2080\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.2124 - val_loss: 14.1515\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.1569 - val_loss: 14.0952\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.1016 - val_loss: 14.0391\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.0465 - val_loss: 13.9832\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.9915 - val_loss: 13.9274\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.9367 - val_loss: 13.8718\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.8821 - val_loss: 13.8163\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.8276 - val_loss: 13.7609\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.7733 - val_loss: 13.7057\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.7191 - val_loss: 13.6506\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 14.6650 - val_loss: 13.5957\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.6112 - val_loss: 13.5408\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.5574 - val_loss: 13.4861\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.5038 - val_loss: 13.4315\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.4503 - val_loss: 13.3770\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.3970 - val_loss: 13.3226\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.3437 - val_loss: 13.2682\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.2905 - val_loss: 13.2140\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.2375 - val_loss: 13.1598\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.1846 - val_loss: 13.1057\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 14.1317 - val_loss: 13.0516\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.0789 - val_loss: 12.9976\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.0262 - val_loss: 12.9436\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 13.9735 - val_loss: 12.8896\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.9209 - val_loss: 12.8357\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.8684 - val_loss: 12.7818\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.8158 - val_loss: 12.7278\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.7633 - val_loss: 12.6739\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.7108 - val_loss: 12.6200\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.6583 - val_loss: 12.5660\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 13.6058 - val_loss: 12.5119\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.5533 - val_loss: 12.4579\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.5007 - val_loss: 12.4037\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.4481 - val_loss: 12.3495\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.3955 - val_loss: 12.2952\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.3428 - val_loss: 12.2408\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.2900 - val_loss: 12.1863\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.2371 - val_loss: 12.1316\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.1841 - val_loss: 12.0769\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.1310 - val_loss: 12.0220\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.0777 - val_loss: 11.9670\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.0243 - val_loss: 11.9118\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.9707 - val_loss: 11.8564\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.9170 - val_loss: 11.8009\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.8631 - val_loss: 11.7452\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.8090 - val_loss: 11.6893\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.7547 - val_loss: 11.6333\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.7002 - val_loss: 11.5770\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.6454 - val_loss: 11.5206\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.5905 - val_loss: 11.4640\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.5353 - val_loss: 11.4072\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.4798 - val_loss: 11.3503\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.4241 - val_loss: 11.2931\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.3682 - val_loss: 11.2359\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.3120 - val_loss: 11.1785\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.2555 - val_loss: 11.1209\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.1988 - val_loss: 11.0633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.1419 - val_loss: 11.0055\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.0847 - val_loss: 10.9477\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.0274 - val_loss: 10.8899\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.9698 - val_loss: 10.8320\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.9120 - val_loss: 10.7742\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.8541 - val_loss: 10.7164\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.7961 - val_loss: 10.6587\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.7381 - val_loss: 10.6012\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.6800 - val_loss: 10.5438\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.6220 - val_loss: 10.4866\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.5640 - val_loss: 10.4297\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.5062 - val_loss: 10.3730\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.4484 - val_loss: 10.3167\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.3909 - val_loss: 10.2607\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.3335 - val_loss: 10.2050\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.2764 - val_loss: 10.1498\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.2196 - val_loss: 10.0949\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.1632 - val_loss: 10.0406\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 11.1072 - val_loss: 9.9868\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.0516 - val_loss: 9.9336\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.9965 - val_loss: 9.8809\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.9420 - val_loss: 9.8289\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.8881 - val_loss: 9.7775\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.8348 - val_loss: 9.7268\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.7821 - val_loss: 9.6770\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.7303 - val_loss: 9.6279\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.6791 - val_loss: 9.5798\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.6287 - val_loss: 9.5325\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.5791 - val_loss: 9.4860\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.5303 - val_loss: 9.4403\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.4824 - val_loss: 9.3954\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 10.4353 - val_loss: 9.3513\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.3890 - val_loss: 9.3080\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.3436 - val_loss: 9.2655\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.2991 - val_loss: 9.2239\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.2554 - val_loss: 9.1833\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.2127 - val_loss: 9.1437\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.1707 - val_loss: 9.1052\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.1298 - val_loss: 9.0674\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.0897 - val_loss: 9.0306\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.0505 - val_loss: 8.9946\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.0124 - val_loss: 8.9594\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.9752 - val_loss: 8.9251\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.9389 - val_loss: 8.8917\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.9036 - val_loss: 8.8590\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.8691 - val_loss: 8.8272\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.8355 - val_loss: 8.7961\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.8027 - val_loss: 8.7659\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.7707 - val_loss: 8.7364\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.7396 - val_loss: 8.7076\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.7092 - val_loss: 8.6796\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.6795 - val_loss: 8.6524\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.6507 - val_loss: 8.6259\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.6226 - val_loss: 8.6000\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.5952 - val_loss: 8.5749\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.5685 - val_loss: 8.5505\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.5426 - val_loss: 8.5268\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.5174 - val_loss: 8.5038\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.4928 - val_loss: 8.4815\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.4690 - val_loss: 8.4598\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.4458 - val_loss: 8.4388\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.4232 - val_loss: 8.4184\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.4013 - val_loss: 8.3986\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.3801 - val_loss: 8.3795\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.3594 - val_loss: 8.3610\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.3393 - val_loss: 8.3431\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.3198 - val_loss: 8.3257\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 9.3009 - val_loss: 8.3089\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.2824 - val_loss: 8.2927\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.2646 - val_loss: 8.2770\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.2473 - val_loss: 8.2618\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.2304 - val_loss: 8.2471\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.2141 - val_loss: 8.2329\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 9.1983 - val_loss: 8.2192\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.1830 - val_loss: 8.2060\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.1681 - val_loss: 8.1932\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 9.1536 - val_loss: 8.1808\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.1396 - val_loss: 8.1688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.1259 - val_loss: 8.1573\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.1127 - val_loss: 8.1461\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0998 - val_loss: 8.1353\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.0872 - val_loss: 8.1248\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.0750 - val_loss: 8.1148\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.0631 - val_loss: 8.1050\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.0516 - val_loss: 8.0957\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0404 - val_loss: 8.0867\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.0295 - val_loss: 8.0780\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.0189 - val_loss: 8.0697\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0085 - val_loss: 8.0616\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.9984 - val_loss: 8.0539\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.9886 - val_loss: 8.0465\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.9791 - val_loss: 8.0393\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.9698 - val_loss: 8.0324\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.9608 - val_loss: 8.0258\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.9520 - val_loss: 8.0194\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.9434 - val_loss: 8.0133\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9350 - val_loss: 8.0074\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.9268 - val_loss: 8.0018\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9188 - val_loss: 7.9963\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9110 - val_loss: 7.9911\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.9034 - val_loss: 7.9862\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8960 - val_loss: 7.9814\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.8887 - val_loss: 7.9768\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8816 - val_loss: 7.9724\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8747 - val_loss: 7.9682\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.8680 - val_loss: 7.9641\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8614 - val_loss: 7.9602\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.8549 - val_loss: 7.9565\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8486 - val_loss: 7.9529\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8424 - val_loss: 7.9495\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8363 - val_loss: 7.9462\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8304 - val_loss: 7.9430\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8247 - val_loss: 7.9399\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8191 - val_loss: 7.9369\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8136 - val_loss: 7.9340\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8083 - val_loss: 7.9311\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8030 - val_loss: 7.9283\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7978 - val_loss: 7.9255\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7928 - val_loss: 7.9228\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.7878 - val_loss: 7.9201\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7830 - val_loss: 7.9175\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7782 - val_loss: 7.9148\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7735 - val_loss: 7.9122\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7688 - val_loss: 7.9096\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7643 - val_loss: 7.9071\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7597 - val_loss: 7.9045\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7553 - val_loss: 7.9020\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7509 - val_loss: 7.8995\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7465 - val_loss: 7.8971\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7422 - val_loss: 7.8947\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7380 - val_loss: 7.8923\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7338 - val_loss: 7.8899\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7296 - val_loss: 7.8875\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7255 - val_loss: 7.8852\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7214 - val_loss: 7.8829\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7174 - val_loss: 7.8806\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7134 - val_loss: 7.8784\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7093 - val_loss: 7.8761\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7054 - val_loss: 7.8738\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7014 - val_loss: 7.8716\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6975 - val_loss: 7.8693\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6936 - val_loss: 7.8671\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6897 - val_loss: 7.8649\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6858 - val_loss: 7.8626\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6820 - val_loss: 7.8604\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6782 - val_loss: 7.8582\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6745 - val_loss: 7.8559\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6708 - val_loss: 7.8537\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6672 - val_loss: 7.8514\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6635 - val_loss: 7.8492\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6599 - val_loss: 7.8469\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6563 - val_loss: 7.8447\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6527 - val_loss: 7.8424\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6492 - val_loss: 7.8402\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6456 - val_loss: 7.8379\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6421 - val_loss: 7.8356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6385 - val_loss: 7.8333\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6351 - val_loss: 7.8310\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6316 - val_loss: 7.8287\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6282 - val_loss: 7.8264\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6248 - val_loss: 7.8240\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6214 - val_loss: 7.8217\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6180 - val_loss: 7.8193\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6146 - val_loss: 7.8169\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6112 - val_loss: 7.8146\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6079 - val_loss: 7.8122\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6045 - val_loss: 7.8098\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6012 - val_loss: 7.8074\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5978 - val_loss: 7.8049\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5945 - val_loss: 7.8025\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.5912 - val_loss: 7.8001\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5879 - val_loss: 7.7976\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5845 - val_loss: 7.7952\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5812 - val_loss: 7.7927\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5779 - val_loss: 7.7902\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5746 - val_loss: 7.7877\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5713 - val_loss: 7.7852\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5680 - val_loss: 7.7826\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5647 - val_loss: 7.7801\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5614 - val_loss: 7.7775\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5581 - val_loss: 7.7750\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5547 - val_loss: 7.7724\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5514 - val_loss: 7.7698\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5481 - val_loss: 7.7671\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5448 - val_loss: 7.7645\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5415 - val_loss: 7.7618\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.5381 - val_loss: 7.7592\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5348 - val_loss: 7.7565\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5315 - val_loss: 7.7538\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.5281 - val_loss: 7.7511\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5248 - val_loss: 7.7483\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5215 - val_loss: 7.7456\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5181 - val_loss: 7.7428\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5148 - val_loss: 7.7400\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5114 - val_loss: 7.7372\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5080 - val_loss: 7.7344\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.5046 - val_loss: 7.7316\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5013 - val_loss: 7.7287\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4979 - val_loss: 7.7259\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4945 - val_loss: 7.7230\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4911 - val_loss: 7.7201\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.4877 - val_loss: 7.7172\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4843 - val_loss: 7.7143\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.4809 - val_loss: 7.7113\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4775 - val_loss: 7.7084\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4740 - val_loss: 7.7054\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4706 - val_loss: 7.7024\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4672 - val_loss: 7.6994\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4637 - val_loss: 7.6964\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4602 - val_loss: 7.6934\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4567 - val_loss: 7.6904\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4532 - val_loss: 7.6873\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4498 - val_loss: 7.6843\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4463 - val_loss: 7.6812\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4427 - val_loss: 7.6781\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4392 - val_loss: 7.6750\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4357 - val_loss: 7.6719\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4321 - val_loss: 7.6688\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4286 - val_loss: 7.6656\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4250 - val_loss: 7.6625\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4214 - val_loss: 7.6593\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4178 - val_loss: 7.6561\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4142 - val_loss: 7.6529\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4106 - val_loss: 7.6497\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4070 - val_loss: 7.6465\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4034 - val_loss: 7.6432\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3998 - val_loss: 7.6400\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3961 - val_loss: 7.6367\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3925 - val_loss: 7.6334\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3888 - val_loss: 7.6301\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3851 - val_loss: 7.6268\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3814 - val_loss: 7.6235\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3777 - val_loss: 7.6201\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3740 - val_loss: 7.6168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3703 - val_loss: 7.6134\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3665 - val_loss: 7.6100\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3628 - val_loss: 7.6066\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3590 - val_loss: 7.6032\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3553 - val_loss: 7.5997\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3515 - val_loss: 7.5963\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3477 - val_loss: 7.5928\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3439 - val_loss: 7.5893\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3401 - val_loss: 7.5858\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3363 - val_loss: 7.5823\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3324 - val_loss: 7.5788\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3286 - val_loss: 7.5752\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3248 - val_loss: 7.5717\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3209 - val_loss: 7.5681\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3170 - val_loss: 7.5645\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3131 - val_loss: 7.5609\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3092 - val_loss: 7.5573\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3053 - val_loss: 7.5537\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.3014 - val_loss: 7.5500\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2975 - val_loss: 7.5463\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2935 - val_loss: 7.5427\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2896 - val_loss: 7.5390\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2856 - val_loss: 7.5353\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2817 - val_loss: 7.5315\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2777 - val_loss: 7.5278\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2737 - val_loss: 7.5241\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2697 - val_loss: 7.5203\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2657 - val_loss: 7.5165\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2616 - val_loss: 7.5127\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2576 - val_loss: 7.5089\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2536 - val_loss: 7.5051\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2495 - val_loss: 7.5012\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.2454 - val_loss: 7.4974\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.2413 - val_loss: 7.4935\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2373 - val_loss: 7.4896\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2332 - val_loss: 7.4857\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2291 - val_loss: 7.4818\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2249 - val_loss: 7.4779\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2208 - val_loss: 7.4740\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2166 - val_loss: 7.4700\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2125 - val_loss: 7.4660\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2083 - val_loss: 7.4621\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2041 - val_loss: 7.4581\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1999 - val_loss: 7.4541\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1957 - val_loss: 7.4500\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1915 - val_loss: 7.4460\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1873 - val_loss: 7.4420\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1831 - val_loss: 7.4379\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1788 - val_loss: 7.4338\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1746 - val_loss: 7.4297\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1703 - val_loss: 7.4256\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1660 - val_loss: 7.4215\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1617 - val_loss: 7.4174\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1574 - val_loss: 7.4133\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1531 - val_loss: 7.4091\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1487 - val_loss: 7.4049\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1444 - val_loss: 7.4008\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1400 - val_loss: 7.3966\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.1356 - val_loss: 7.3923\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1313 - val_loss: 7.3881\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.1269 - val_loss: 7.3839\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1225 - val_loss: 7.3796\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1181 - val_loss: 7.3754\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1136 - val_loss: 7.3711\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1092 - val_loss: 7.3668\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1047 - val_loss: 7.3625\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1003 - val_loss: 7.3582\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0958 - val_loss: 7.3539\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0913 - val_loss: 7.3495\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0868 - val_loss: 7.3452\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0823 - val_loss: 7.3408\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0778 - val_loss: 7.3364\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0732 - val_loss: 7.3320\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0687 - val_loss: 7.3276\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0641 - val_loss: 7.3232\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.0595 - val_loss: 7.3188\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0549 - val_loss: 7.3143\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 575us/step - loss: 58.5390 - val_loss: 56.3134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 58.4471 - val_loss: 56.1759\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 58.3091 - val_loss: 56.0157\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 58.1487 - val_loss: 55.8443\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 57.9770 - val_loss: 55.6676\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 57.7999 - val_loss: 55.4882\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 57.6201 - val_loss: 55.3076\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 57.4393 - val_loss: 55.1264\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 57.2576 - val_loss: 54.9447\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 57.0756 - val_loss: 54.7629\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 56.8935 - val_loss: 54.5807\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 56.7109 - val_loss: 54.3982\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 56.5282 - val_loss: 54.2154\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 56.3448 - val_loss: 54.0321\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 56.1612 - val_loss: 53.8482\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 55.9770 - val_loss: 53.6638\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 55.7923 - val_loss: 53.4787\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 55.6068 - val_loss: 53.2928\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 55.4205 - val_loss: 53.1060\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 55.2334 - val_loss: 52.9183\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 55.0454 - val_loss: 52.7297\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 54.8564 - val_loss: 52.5400\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 54.6663 - val_loss: 52.3491\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 54.4750 - val_loss: 52.1571\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 54.2827 - val_loss: 51.9638\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 54.0889 - val_loss: 51.7692\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 53.8940 - val_loss: 51.5732\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 53.6977 - val_loss: 51.3759\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 53.4999 - val_loss: 51.1770\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 53.3007 - val_loss: 50.9766\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 53.0999 - val_loss: 50.7747\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 52.8975 - val_loss: 50.5712\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 52.6937 - val_loss: 50.3660\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 52.4881 - val_loss: 50.1591\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 52.2807 - val_loss: 49.9505\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 52.0718 - val_loss: 49.7402\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 51.8611 - val_loss: 49.5280\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 51.6485 - val_loss: 49.3140\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 51.4342 - val_loss: 49.0982\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 51.2181 - val_loss: 48.8805\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 51.0000 - val_loss: 48.6609\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 50.7801 - val_loss: 48.4394\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 50.5582 - val_loss: 48.2159\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 50.3344 - val_loss: 47.9905\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 50.1088 - val_loss: 47.7631\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.8810 - val_loss: 47.5338\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 49.6513 - val_loss: 47.3024\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 49.4197 - val_loss: 47.0690\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.1860 - val_loss: 46.8336\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 48.9503 - val_loss: 46.5962\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 48.7127 - val_loss: 46.3567\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 48.4730 - val_loss: 46.1152\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 48.2312 - val_loss: 45.8717\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 47.9873 - val_loss: 45.6261\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 47.7416 - val_loss: 45.3785\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 47.4936 - val_loss: 45.1288\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 47.2437 - val_loss: 44.8771\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 46.9918 - val_loss: 44.6235\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 46.7379 - val_loss: 44.3678\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 46.4819 - val_loss: 44.1101\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 46.2240 - val_loss: 43.8503\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 45.9640 - val_loss: 43.5887\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 45.7021 - val_loss: 43.3251\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 45.4380 - val_loss: 43.0596\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 45.1722 - val_loss: 42.7922\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 44.9043 - val_loss: 42.5228\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.6347 - val_loss: 42.2517\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.3631 - val_loss: 41.9787\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.0895 - val_loss: 41.7039\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 43.8143 - val_loss: 41.4273\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 43.5371 - val_loss: 41.1491\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 43.2583 - val_loss: 40.8692\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 42.9777 - val_loss: 40.5876\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 42.6953 - val_loss: 40.3044\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 42.4115 - val_loss: 40.0197\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 42.1259 - val_loss: 39.7334\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 41.8388 - val_loss: 39.4458\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 41.5501 - val_loss: 39.1567\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 41.2601 - val_loss: 38.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 40.9685 - val_loss: 38.5746\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 40.6757 - val_loss: 38.2817\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 40.3815 - val_loss: 37.9876\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 40.0860 - val_loss: 37.6925\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 39.7895 - val_loss: 37.3963\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 39.4917 - val_loss: 37.0991\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 39.1929 - val_loss: 36.8011\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 38.8931 - val_loss: 36.5023\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 38.5925 - val_loss: 36.2028\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 38.2910 - val_loss: 35.9026\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 37.9887 - val_loss: 35.6019\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 37.6857 - val_loss: 35.3007\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 37.3822 - val_loss: 34.9991\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 37.0781 - val_loss: 34.6972\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 36.7736 - val_loss: 34.3951\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 36.4687 - val_loss: 34.0930\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 36.1636 - val_loss: 33.7908\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 35.8583 - val_loss: 33.4888\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 35.5529 - val_loss: 33.1869\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 35.2476 - val_loss: 32.8854\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 34.9424 - val_loss: 32.5844\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 34.6375 - val_loss: 32.2839\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 34.3328 - val_loss: 31.9840\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 34.0288 - val_loss: 31.6850\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 33.7251 - val_loss: 31.3869\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 33.4223 - val_loss: 31.0899\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 33.1203 - val_loss: 30.7941\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 32.8191 - val_loss: 30.4996\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 32.5191 - val_loss: 30.2066\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 32.2202 - val_loss: 29.9152\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 31.9226 - val_loss: 29.6255\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 31.6265 - val_loss: 29.3377\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 31.3320 - val_loss: 29.0520\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 31.0391 - val_loss: 28.7684\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 30.7482 - val_loss: 28.4872\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 30.4592 - val_loss: 28.2084\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 30.1723 - val_loss: 27.9322\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 29.8875 - val_loss: 27.6588\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 29.6052 - val_loss: 27.3882\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 29.3254 - val_loss: 27.1206\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 29.0481 - val_loss: 26.8561\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 28.7736 - val_loss: 26.5949\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 28.5019 - val_loss: 26.3370\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 28.2332 - val_loss: 26.0826\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 27.9676 - val_loss: 25.8317\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 27.7051 - val_loss: 25.5846\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 27.4461 - val_loss: 25.3412\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 27.1903 - val_loss: 25.1017\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 26.9382 - val_loss: 24.8662\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 26.6895 - val_loss: 24.6347\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 26.4447 - val_loss: 24.4073\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 26.2036 - val_loss: 24.1842\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 25.9665 - val_loss: 23.9653\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 25.7332 - val_loss: 23.7508\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 25.5040 - val_loss: 23.5407\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 25.2788 - val_loss: 23.3351\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 25.0578 - val_loss: 23.1340\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.8409 - val_loss: 22.9376\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.6284 - val_loss: 22.7459\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.4202 - val_loss: 22.5590\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.2163 - val_loss: 22.3771\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 24.0171 - val_loss: 22.2003\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 23.8226 - val_loss: 22.0287\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.6327 - val_loss: 21.8623\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.4477 - val_loss: 21.7017\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 23.2678 - val_loss: 21.5470\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 23.0933 - val_loss: 21.3980\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.9244 - val_loss: 21.2554\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 22.7614 - val_loss: 21.1194\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 22.6045 - val_loss: 20.9892\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.4526 - val_loss: 20.8642\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 22.3059 - val_loss: 20.7435\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.1631 - val_loss: 20.6264\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.0240 - val_loss: 20.5125\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.8882 - val_loss: 20.4015\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.7557 - val_loss: 20.2930\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.6263 - val_loss: 20.1871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 21.4999 - val_loss: 20.0836\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.3763 - val_loss: 19.9823\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 21.2552 - val_loss: 19.8832\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.1368 - val_loss: 19.7861\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.0207 - val_loss: 19.6910\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.9069 - val_loss: 19.5978\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.7953 - val_loss: 19.5064\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 20.6858 - val_loss: 19.4168\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 20.5784 - val_loss: 19.3288\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 20.4729 - val_loss: 19.2425\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.3693 - val_loss: 19.1577\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 20.2676 - val_loss: 19.0745\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.1677 - val_loss: 18.9927\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 20.0695 - val_loss: 18.9124\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.9731 - val_loss: 18.8335\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.8783 - val_loss: 18.7559\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.7852 - val_loss: 18.6798\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.6938 - val_loss: 18.6049\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.6038 - val_loss: 18.5314\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 19.5155 - val_loss: 18.4593\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 19.4287 - val_loss: 18.3884\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.3433 - val_loss: 18.3187\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.2594 - val_loss: 18.2504\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.1770 - val_loss: 18.1832\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.0959 - val_loss: 18.1173\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 19.0162 - val_loss: 18.0526\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.9378 - val_loss: 17.9891\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 18.8607 - val_loss: 17.9268\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.7848 - val_loss: 17.8655\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.7102 - val_loss: 17.8054\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.6368 - val_loss: 17.7464\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 18.5645 - val_loss: 17.6885\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.4934 - val_loss: 17.6316\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 18.4234 - val_loss: 17.5757\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 18.3545 - val_loss: 17.5208\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 18.2867 - val_loss: 17.4669\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.2198 - val_loss: 17.4139\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 18.1540 - val_loss: 17.3618\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.0893 - val_loss: 17.3106\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.0254 - val_loss: 17.2602\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.9626 - val_loss: 17.2106\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.9006 - val_loss: 17.1618\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 17.8395 - val_loss: 17.1138\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.7794 - val_loss: 17.0665\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.7200 - val_loss: 17.0199\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.6616 - val_loss: 16.9740\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 17.6040 - val_loss: 16.9287\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.5471 - val_loss: 16.8841\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 17.4910 - val_loss: 16.8401\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.4357 - val_loss: 16.7966\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.3812 - val_loss: 16.7537\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.3272 - val_loss: 16.7114\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.2741 - val_loss: 16.6695\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.2215 - val_loss: 16.6281\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.1696 - val_loss: 16.5871\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 17.1183 - val_loss: 16.5466\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.0675 - val_loss: 16.5065\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.0173 - val_loss: 16.4667\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.9676 - val_loss: 16.4273\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.9184 - val_loss: 16.3883\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.8697 - val_loss: 16.3495\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.8214 - val_loss: 16.3110\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.7735 - val_loss: 16.2727\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.7260 - val_loss: 16.2347\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.6788 - val_loss: 16.1968\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.6319 - val_loss: 16.1591\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.5853 - val_loss: 16.1215\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.5390 - val_loss: 16.0840\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.4929 - val_loss: 16.0466\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 16.4469 - val_loss: 16.0093\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.4012 - val_loss: 15.9719\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 4us/step - loss: 16.3555 - val_loss: 15.9346\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.3099 - val_loss: 15.8971\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.2644 - val_loss: 15.8596\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.2189 - val_loss: 15.8220\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.1734 - val_loss: 15.7842\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.1279 - val_loss: 15.7463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.0822 - val_loss: 15.7081\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 16.0365 - val_loss: 15.6696\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.9906 - val_loss: 15.6309\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.9444 - val_loss: 15.5918\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.8981 - val_loss: 15.5524\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.8515 - val_loss: 15.5125\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.8046 - val_loss: 15.4722\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.7574 - val_loss: 15.4314\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.7097 - val_loss: 15.3901\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.6617 - val_loss: 15.3482\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.6131 - val_loss: 15.3057\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.5641 - val_loss: 15.2625\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.5146 - val_loss: 15.2187\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.4644 - val_loss: 15.1742\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.4137 - val_loss: 15.1288\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.3623 - val_loss: 15.0827\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.3101 - val_loss: 15.0358\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 15.2573 - val_loss: 14.9880\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.2037 - val_loss: 14.9393\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 15.1493 - val_loss: 14.8897\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.0941 - val_loss: 14.8391\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.0380 - val_loss: 14.7876\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.9811 - val_loss: 14.7352\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.9233 - val_loss: 14.6817\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.8646 - val_loss: 14.6273\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.8049 - val_loss: 14.5719\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.7444 - val_loss: 14.5156\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.6830 - val_loss: 14.4583\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.6207 - val_loss: 14.4001\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.5576 - val_loss: 14.3411\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.4936 - val_loss: 14.2812\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.4288 - val_loss: 14.2205\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 14.3632 - val_loss: 14.1591\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.2969 - val_loss: 14.0970\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.2300 - val_loss: 14.0344\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.1624 - val_loss: 13.9712\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.0943 - val_loss: 13.9075\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.0258 - val_loss: 13.8434\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.9568 - val_loss: 13.7790\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.8874 - val_loss: 13.7144\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.8178 - val_loss: 13.6495\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 13.7480 - val_loss: 13.5845\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.6780 - val_loss: 13.5194\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.6079 - val_loss: 13.4542\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.5377 - val_loss: 13.3890\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.4675 - val_loss: 13.3239\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 13.3974 - val_loss: 13.2588\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.3274 - val_loss: 13.1938\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.2574 - val_loss: 13.1288\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 13.1876 - val_loss: 13.0640\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.1179 - val_loss: 12.9993\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.0484 - val_loss: 12.9347\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.9790 - val_loss: 12.8703\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.9099 - val_loss: 12.8060\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.8409 - val_loss: 12.7418\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.7721 - val_loss: 12.6777\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.7035 - val_loss: 12.6138\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 12.6351 - val_loss: 12.5501\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.5669 - val_loss: 12.4864\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.4990 - val_loss: 12.4230\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.4312 - val_loss: 12.3597\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.3638 - val_loss: 12.2966\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.2965 - val_loss: 12.2337\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 12.2298 - val_loss: 12.1711\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.1634 - val_loss: 12.1088\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 12.0974 - val_loss: 12.0468\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.0317 - val_loss: 11.9850\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.9664 - val_loss: 11.9235\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.9015 - val_loss: 11.8624\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 11.8369 - val_loss: 11.8016\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.7728 - val_loss: 11.7412\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.7091 - val_loss: 11.6811\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.6459 - val_loss: 11.6216\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.5832 - val_loss: 11.5625\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.5210 - val_loss: 11.5040\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.4594 - val_loss: 11.4460\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.3984 - val_loss: 11.3887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.3381 - val_loss: 11.3320\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.2783 - val_loss: 11.2759\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 11.2194 - val_loss: 11.2206\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.1611 - val_loss: 11.1660\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.1035 - val_loss: 11.1122\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.0468 - val_loss: 11.0592\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.9908 - val_loss: 11.0070\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.9357 - val_loss: 10.9555\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.8813 - val_loss: 10.9049\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.8278 - val_loss: 10.8551\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 10.7751 - val_loss: 10.8061\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.7233 - val_loss: 10.7579\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.6723 - val_loss: 10.7105\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.6222 - val_loss: 10.6639\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.5729 - val_loss: 10.6182\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.5245 - val_loss: 10.5733\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.4770 - val_loss: 10.5291\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.4303 - val_loss: 10.4858\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.3846 - val_loss: 10.4432\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.3396 - val_loss: 10.4015\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.2956 - val_loss: 10.3604\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.2524 - val_loss: 10.3202\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.2101 - val_loss: 10.2808\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.1686 - val_loss: 10.2421\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.1280 - val_loss: 10.2042\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 10.0883 - val_loss: 10.1670\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.0494 - val_loss: 10.1306\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.0114 - val_loss: 10.0948\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.9742 - val_loss: 10.0598\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.9378 - val_loss: 10.0255\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.9021 - val_loss: 9.9919\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.8672 - val_loss: 9.9589\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.8331 - val_loss: 9.9266\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.7998 - val_loss: 9.8949\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.7671 - val_loss: 9.8638\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.7352 - val_loss: 9.8334\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.7040 - val_loss: 9.8036\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.6735 - val_loss: 9.7744\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.6437 - val_loss: 9.7458\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.6146 - val_loss: 9.7178\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.5861 - val_loss: 9.6903\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.5583 - val_loss: 9.6635\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.5311 - val_loss: 9.6372\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.5046 - val_loss: 9.6115\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.4787 - val_loss: 9.5863\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.4535 - val_loss: 9.5617\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.4289 - val_loss: 9.5376\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.4049 - val_loss: 9.5140\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.3814 - val_loss: 9.4910\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.3586 - val_loss: 9.4684\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.3362 - val_loss: 9.4463\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.3144 - val_loss: 9.4247\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.2931 - val_loss: 9.4036\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.2724 - val_loss: 9.3830\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.2521 - val_loss: 9.3628\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.2324 - val_loss: 9.3430\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.2130 - val_loss: 9.3237\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.1942 - val_loss: 9.3048\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.1758 - val_loss: 9.2863\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.1579 - val_loss: 9.2682\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.1404 - val_loss: 9.2505\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.1234 - val_loss: 9.2331\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 9.1068 - val_loss: 9.2162\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.0906 - val_loss: 9.1996\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0748 - val_loss: 9.1833\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0593 - val_loss: 9.1674\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.0442 - val_loss: 9.1518\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 9.0295 - val_loss: 9.1366\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.0151 - val_loss: 9.1216\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0011 - val_loss: 9.1070\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.9874 - val_loss: 9.0927\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.9740 - val_loss: 9.0787\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9610 - val_loss: 9.0650\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.9482 - val_loss: 9.0516\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9358 - val_loss: 9.0384\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.9236 - val_loss: 9.0256\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.9118 - val_loss: 9.0130\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.9002 - val_loss: 9.0007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8890 - val_loss: 8.9887\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.8781 - val_loss: 8.9770\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8674 - val_loss: 8.9655\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8571 - val_loss: 8.9543\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.8469 - val_loss: 8.9433\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.8370 - val_loss: 8.9326\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8273 - val_loss: 8.9221\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8178 - val_loss: 8.9118\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8086 - val_loss: 8.9018\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7996 - val_loss: 8.8920\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7908 - val_loss: 8.8824\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7822 - val_loss: 8.8730\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7738 - val_loss: 8.8638\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7655 - val_loss: 8.8548\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7575 - val_loss: 8.8459\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7496 - val_loss: 8.8373\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7419 - val_loss: 8.8288\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.7344 - val_loss: 8.8206\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7270 - val_loss: 8.8125\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7197 - val_loss: 8.8045\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.7127 - val_loss: 8.7968\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.7058 - val_loss: 8.7892\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6990 - val_loss: 8.7819\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6924 - val_loss: 8.7747\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.6859 - val_loss: 8.7677\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6795 - val_loss: 8.7609\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6733 - val_loss: 8.7542\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6672 - val_loss: 8.7476\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6612 - val_loss: 8.7412\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6553 - val_loss: 8.7349\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6495 - val_loss: 8.7287\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6439 - val_loss: 8.7226\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6383 - val_loss: 8.7166\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6328 - val_loss: 8.7107\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6274 - val_loss: 8.7049\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.6221 - val_loss: 8.6992\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6169 - val_loss: 8.6936\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6117 - val_loss: 8.6881\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6066 - val_loss: 8.6827\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6016 - val_loss: 8.6773\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5967 - val_loss: 8.6720\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5918 - val_loss: 8.6667\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5870 - val_loss: 8.6616\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5823 - val_loss: 8.6565\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.5776 - val_loss: 8.6515\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5730 - val_loss: 8.6465\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5685 - val_loss: 8.6415\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5640 - val_loss: 8.6366\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5595 - val_loss: 8.6318\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5551 - val_loss: 8.6270\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5508 - val_loss: 8.6222\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5465 - val_loss: 8.6175\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5423 - val_loss: 8.6128\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5380 - val_loss: 8.6082\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5339 - val_loss: 8.6035\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5297 - val_loss: 8.5990\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.5256 - val_loss: 8.5944\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5215 - val_loss: 8.5899\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5175 - val_loss: 8.5855\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5135 - val_loss: 8.5810\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5095 - val_loss: 8.5766\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5055 - val_loss: 8.5722\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5016 - val_loss: 8.5679\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4977 - val_loss: 8.5635\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4937 - val_loss: 8.5592\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4898 - val_loss: 8.5550\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4859 - val_loss: 8.5507\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4820 - val_loss: 8.5465\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4781 - val_loss: 8.5423\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4742 - val_loss: 8.5382\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4704 - val_loss: 8.5340\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4665 - val_loss: 8.5299\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4626 - val_loss: 8.5258\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.4587 - val_loss: 8.5217\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4549 - val_loss: 8.5176\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4510 - val_loss: 8.5136\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4471 - val_loss: 8.5095\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4432 - val_loss: 8.5055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4394 - val_loss: 8.5015\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4355 - val_loss: 8.4975\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4317 - val_loss: 8.4935\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4278 - val_loss: 8.4895\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4240 - val_loss: 8.4855\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4201 - val_loss: 8.4816\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.4163 - val_loss: 8.4776\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4124 - val_loss: 8.4737\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4085 - val_loss: 8.4697\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4047 - val_loss: 8.4658\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4008 - val_loss: 8.4618\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3969 - val_loss: 8.4579\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3930 - val_loss: 8.4540\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3891 - val_loss: 8.4500\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3852 - val_loss: 8.4461\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3813 - val_loss: 8.4422\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3774 - val_loss: 8.4383\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3734 - val_loss: 8.4343\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3695 - val_loss: 8.4304\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3655 - val_loss: 8.4265\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3615 - val_loss: 8.4225\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.3576 - val_loss: 8.4186\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3536 - val_loss: 8.4146\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3496 - val_loss: 8.4107\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3456 - val_loss: 8.4068\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3416 - val_loss: 8.4028\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3376 - val_loss: 8.3989\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3335 - val_loss: 8.3949\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3295 - val_loss: 8.3909\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3254 - val_loss: 8.3870\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3213 - val_loss: 8.3830\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3173 - val_loss: 8.3790\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3132 - val_loss: 8.3750\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.3090 - val_loss: 8.3710\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3049 - val_loss: 8.3670\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3008 - val_loss: 8.3629\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2966 - val_loss: 8.3589\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2924 - val_loss: 8.3549\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.2882 - val_loss: 8.3508\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2840 - val_loss: 8.3467\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2798 - val_loss: 8.3427\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2755 - val_loss: 8.3386\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2712 - val_loss: 8.3345\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.2669 - val_loss: 8.3304\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2627 - val_loss: 8.3262\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2583 - val_loss: 8.3221\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2540 - val_loss: 8.3180\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2497 - val_loss: 8.3138\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2453 - val_loss: 8.3096\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2409 - val_loss: 8.3054\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2365 - val_loss: 8.3012\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2321 - val_loss: 8.2970\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2276 - val_loss: 8.2928\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.2232 - val_loss: 8.2886\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2187 - val_loss: 8.2843\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2142 - val_loss: 8.2800\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.2097 - val_loss: 8.2758\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2052 - val_loss: 8.2715\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2007 - val_loss: 8.2672\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.1961 - val_loss: 8.2629\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1916 - val_loss: 8.2585\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1869 - val_loss: 8.2542\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1824 - val_loss: 8.2498\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1777 - val_loss: 8.2455\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1731 - val_loss: 8.2411\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1685 - val_loss: 8.2367\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1638 - val_loss: 8.2323\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1591 - val_loss: 8.2279\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1544 - val_loss: 8.2234\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1497 - val_loss: 8.2190\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.1450 - val_loss: 8.2146\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1402 - val_loss: 8.2101\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1355 - val_loss: 8.2056\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1307 - val_loss: 8.2011\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1260 - val_loss: 8.1966\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1211 - val_loss: 8.1921\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1163 - val_loss: 8.1875\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1115 - val_loss: 8.1830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.1067 - val_loss: 8.1784\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.1018 - val_loss: 8.1739\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.0970 - val_loss: 8.1693\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0921 - val_loss: 8.1647\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.0872 - val_loss: 8.1601\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0823 - val_loss: 8.1555\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0774 - val_loss: 8.1508\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0724 - val_loss: 8.1462\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0675 - val_loss: 8.1416\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0625 - val_loss: 8.1369\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0576 - val_loss: 8.1322\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0526 - val_loss: 8.1276\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0476 - val_loss: 8.1229\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0426 - val_loss: 8.1182\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0376 - val_loss: 8.1135\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0326 - val_loss: 8.1087\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0276 - val_loss: 8.1040\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 8.0225 - val_loss: 8.0993\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0175 - val_loss: 8.0945\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 8.0124 - val_loss: 8.0898\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0074 - val_loss: 8.0850\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.0023 - val_loss: 8.0802\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9972 - val_loss: 8.0754\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.9921 - val_loss: 8.0707\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.9870 - val_loss: 8.0659\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9819 - val_loss: 8.0611\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9768 - val_loss: 8.0562\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.9717 - val_loss: 8.0514\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.9665 - val_loss: 8.0466\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9614 - val_loss: 8.0418\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9562 - val_loss: 8.0369\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9511 - val_loss: 8.0321\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.9459 - val_loss: 8.0272\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9407 - val_loss: 8.0223\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9355 - val_loss: 8.0175\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.9304 - val_loss: 8.0126\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9252 - val_loss: 8.0077\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9200 - val_loss: 8.0028\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.9148 - val_loss: 7.9979\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9095 - val_loss: 7.9930\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9043 - val_loss: 7.9881\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.8991 - val_loss: 7.9832\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8939 - val_loss: 7.9783\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8887 - val_loss: 7.9733\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8834 - val_loss: 7.9684\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8782 - val_loss: 7.9635\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.8729 - val_loss: 7.9585\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8676 - val_loss: 7.9536\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 7.8624 - val_loss: 7.9486\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8571 - val_loss: 7.9437\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8519 - val_loss: 7.9387\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.8466 - val_loss: 7.9337\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8413 - val_loss: 7.9288\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.8360 - val_loss: 7.9238\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.8307 - val_loss: 7.9188\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8254 - val_loss: 7.9138\n",
      "Epoch 1/600\n",
      "1295/1295 [==============================] - 0s 336us/step - loss: 57.2145\n",
      "Epoch 2/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 50.6872\n",
      "Epoch 3/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 37.5449\n",
      "Epoch 4/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 21.5194\n",
      "Epoch 5/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 16.8470\n",
      "Epoch 6/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 16.2461\n",
      "Epoch 7/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 15.5246\n",
      "Epoch 8/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 14.2747\n",
      "Epoch 9/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 12.1786\n",
      "Epoch 10/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 9.8018\n",
      "Epoch 11/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 8.8111\n",
      "Epoch 12/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 8.6159\n",
      "Epoch 13/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.5249\n",
      "Epoch 14/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 8.4096\n",
      "Epoch 15/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 8.2765\n",
      "Epoch 16/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 8.1566\n",
      "Epoch 17/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 7.9921\n",
      "Epoch 18/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 7.8135\n",
      "Epoch 19/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 7.6145\n",
      "Epoch 20/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 7.3650\n",
      "Epoch 21/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 7.1605\n",
      "Epoch 22/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 6.9238\n",
      "Epoch 23/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 6.6667\n",
      "Epoch 24/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 6.4255\n",
      "Epoch 25/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 6.1896\n",
      "Epoch 26/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 5.9717\n",
      "Epoch 27/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 5.7638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 5.4974\n",
      "Epoch 29/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 5.3292\n",
      "Epoch 30/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 5.0337\n",
      "Epoch 31/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 4.8376\n",
      "Epoch 32/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 4.5093\n",
      "Epoch 33/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 4.2616\n",
      "Epoch 34/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 4.140 - 0s 8us/step - loss: 4.1394\n",
      "Epoch 35/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 3.9709\n",
      "Epoch 36/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.8604\n",
      "Epoch 37/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 4.3663\n",
      "Epoch 38/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 4.5191\n",
      "Epoch 39/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.8686\n",
      "Epoch 40/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.7050\n",
      "Epoch 41/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.9000\n",
      "Epoch 42/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 4.3677\n",
      "Epoch 43/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.6888\n",
      "Epoch 44/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 3.5197\n",
      "Epoch 45/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 3.7051\n",
      "Epoch 46/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.6757\n",
      "Epoch 47/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.3550\n",
      "Epoch 48/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.2743\n",
      "Epoch 49/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.3170\n",
      "Epoch 50/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.6174\n",
      "Epoch 51/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 4.1342\n",
      "Epoch 52/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.8932\n",
      "Epoch 53/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.8355\n",
      "Epoch 54/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.3574\n",
      "Epoch 55/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 3.4133\n",
      "Epoch 56/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.3577\n",
      "Epoch 57/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.2748\n",
      "Epoch 58/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1622\n",
      "Epoch 59/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2798\n",
      "Epoch 60/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1983\n",
      "Epoch 61/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 3.1114\n",
      "Epoch 62/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.5817\n",
      "Epoch 63/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.4955\n",
      "Epoch 64/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1759\n",
      "Epoch 65/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1835\n",
      "Epoch 66/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1354\n",
      "Epoch 67/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0777\n",
      "Epoch 68/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0519\n",
      "Epoch 69/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.3064\n",
      "Epoch 70/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.5164\n",
      "Epoch 71/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1852\n",
      "Epoch 72/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.2633\n",
      "Epoch 73/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1654\n",
      "Epoch 74/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0197\n",
      "Epoch 75/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9728\n",
      "Epoch 76/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1435\n",
      "Epoch 77/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2112\n",
      "Epoch 78/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.1904\n",
      "Epoch 79/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.5184\n",
      "Epoch 80/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0693\n",
      "Epoch 81/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9605\n",
      "Epoch 82/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.2008\n",
      "Epoch 83/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0166\n",
      "Epoch 84/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9627\n",
      "Epoch 85/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.1772\n",
      "Epoch 86/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.4531\n",
      "Epoch 87/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0591\n",
      "Epoch 88/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0601\n",
      "Epoch 89/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9714\n",
      "Epoch 90/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9544\n",
      "Epoch 91/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0003\n",
      "Epoch 92/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9599\n",
      "Epoch 93/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.0166\n",
      "Epoch 94/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 3.3802\n",
      "Epoch 95/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9882\n",
      "Epoch 96/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0403\n",
      "Epoch 97/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9681\n",
      "Epoch 98/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.9621\n",
      "Epoch 99/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9105\n",
      "Epoch 100/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0950\n",
      "Epoch 101/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0281\n",
      "Epoch 102/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.0018\n",
      "Epoch 103/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1194\n",
      "Epoch 104/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8949\n",
      "Epoch 105/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9410\n",
      "Epoch 106/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.1276\n",
      "Epoch 107/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9003\n",
      "Epoch 108/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0121\n",
      "Epoch 109/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.8294\n",
      "Epoch 110/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8273\n",
      "Epoch 111/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8968\n",
      "Epoch 112/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8568\n",
      "Epoch 113/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8604\n",
      "Epoch 114/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 3.2075\n",
      "Epoch 115/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.7521\n",
      "Epoch 116/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9872\n",
      "Epoch 117/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8380\n",
      "Epoch 118/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7913\n",
      "Epoch 119/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9318\n",
      "Epoch 120/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.8718\n",
      "Epoch 121/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0749\n",
      "Epoch 122/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0576\n",
      "Epoch 123/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8628\n",
      "Epoch 124/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8437\n",
      "Epoch 125/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 3.0331\n",
      "Epoch 126/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8305\n",
      "Epoch 127/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0644\n",
      "Epoch 128/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9051\n",
      "Epoch 129/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.9553\n",
      "Epoch 130/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8083\n",
      "Epoch 131/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8013\n",
      "Epoch 132/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.840 - 0s 10us/step - loss: 2.8040\n",
      "Epoch 133/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.8668\n",
      "Epoch 134/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8556\n",
      "Epoch 135/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.8197\n",
      "Epoch 136/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8623\n",
      "Epoch 137/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8323\n",
      "Epoch 138/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7796\n",
      "Epoch 139/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8188\n",
      "Epoch 140/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7981\n",
      "Epoch 141/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8515\n",
      "Epoch 142/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7248\n",
      "Epoch 143/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8974\n",
      "Epoch 144/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9393\n",
      "Epoch 145/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7870\n",
      "Epoch 146/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7567\n",
      "Epoch 147/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0118\n",
      "Epoch 148/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7653\n",
      "Epoch 149/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.9871\n",
      "Epoch 150/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.0385\n",
      "Epoch 151/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7888\n",
      "Epoch 152/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.9499\n",
      "Epoch 153/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.8121\n",
      "Epoch 154/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8068\n",
      "Epoch 155/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.7974\n",
      "Epoch 156/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.7342\n",
      "Epoch 157/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7573\n",
      "Epoch 158/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.8545\n",
      "Epoch 159/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7608\n",
      "Epoch 160/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7627\n",
      "Epoch 161/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9151\n",
      "Epoch 162/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9318\n",
      "Epoch 163/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9487\n",
      "Epoch 164/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6979\n",
      "Epoch 165/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7029\n",
      "Epoch 166/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6903\n",
      "Epoch 167/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7611\n",
      "Epoch 168/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7387\n",
      "Epoch 169/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6937\n",
      "Epoch 170/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7886\n",
      "Epoch 171/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8593\n",
      "Epoch 172/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7565\n",
      "Epoch 173/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7312\n",
      "Epoch 174/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7159\n",
      "Epoch 175/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8737\n",
      "Epoch 176/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.8824\n",
      "Epoch 177/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8365\n",
      "Epoch 178/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7744\n",
      "Epoch 179/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6751\n",
      "Epoch 180/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7052\n",
      "Epoch 181/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8297\n",
      "Epoch 182/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8831\n",
      "Epoch 183/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7752\n",
      "Epoch 184/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8033\n",
      "Epoch 185/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7048\n",
      "Epoch 186/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9295\n",
      "Epoch 187/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7102\n",
      "Epoch 188/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0550\n",
      "Epoch 189/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7028\n",
      "Epoch 190/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6492\n",
      "Epoch 191/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7533\n",
      "Epoch 192/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7936\n",
      "Epoch 193/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6786\n",
      "Epoch 194/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7091\n",
      "Epoch 195/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7528\n",
      "Epoch 196/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9161\n",
      "Epoch 197/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7346\n",
      "Epoch 198/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.7886\n",
      "Epoch 199/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7782\n",
      "Epoch 200/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6200\n",
      "Epoch 201/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6884\n",
      "Epoch 202/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8535\n",
      "Epoch 203/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.718 - 0s 8us/step - loss: 2.7782\n",
      "Epoch 204/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.7882\n",
      "Epoch 205/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7349\n",
      "Epoch 206/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8202\n",
      "Epoch 207/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8624\n",
      "Epoch 208/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6212\n",
      "Epoch 209/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7731\n",
      "Epoch 210/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6717\n",
      "Epoch 211/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.7287\n",
      "Epoch 212/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6538\n",
      "Epoch 213/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7897\n",
      "Epoch 214/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7262\n",
      "Epoch 215/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7629\n",
      "Epoch 216/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6812\n",
      "Epoch 217/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6576\n",
      "Epoch 218/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7442\n",
      "Epoch 219/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7446\n",
      "Epoch 220/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6767\n",
      "Epoch 221/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6513\n",
      "Epoch 222/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6797\n",
      "Epoch 223/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7803\n",
      "Epoch 224/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8421\n",
      "Epoch 225/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6327\n",
      "Epoch 226/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5992\n",
      "Epoch 227/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6090\n",
      "Epoch 228/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7514\n",
      "Epoch 229/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6085\n",
      "Epoch 230/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6372\n",
      "Epoch 231/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6098\n",
      "Epoch 232/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7957\n",
      "Epoch 233/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6973\n",
      "Epoch 234/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7067\n",
      "Epoch 235/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8261\n",
      "Epoch 236/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7460\n",
      "Epoch 237/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6003\n",
      "Epoch 238/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7299\n",
      "Epoch 239/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7944\n",
      "Epoch 240/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7895\n",
      "Epoch 241/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6473\n",
      "Epoch 242/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6657\n",
      "Epoch 243/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6709\n",
      "Epoch 244/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6006\n",
      "Epoch 245/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7688\n",
      "Epoch 246/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6991\n",
      "Epoch 247/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6977\n",
      "Epoch 248/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7486\n",
      "Epoch 249/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7000\n",
      "Epoch 250/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5833\n",
      "Epoch 251/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7790\n",
      "Epoch 252/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6774\n",
      "Epoch 253/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6938\n",
      "Epoch 254/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6617\n",
      "Epoch 255/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6614\n",
      "Epoch 256/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6159\n",
      "Epoch 257/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6327\n",
      "Epoch 258/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5798\n",
      "Epoch 259/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5699\n",
      "Epoch 260/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5774\n",
      "Epoch 261/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6254\n",
      "Epoch 262/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6058\n",
      "Epoch 263/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5816\n",
      "Epoch 264/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6116\n",
      "Epoch 265/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6963\n",
      "Epoch 266/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.9692\n",
      "Epoch 267/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6158\n",
      "Epoch 268/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6763\n",
      "Epoch 269/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6837\n",
      "Epoch 270/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6196\n",
      "Epoch 271/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7271\n",
      "Epoch 272/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7056\n",
      "Epoch 273/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6297\n",
      "Epoch 274/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8125\n",
      "Epoch 275/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7074\n",
      "Epoch 276/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6893\n",
      "Epoch 277/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7635\n",
      "Epoch 278/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6981\n",
      "Epoch 279/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5933\n",
      "Epoch 280/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5762\n",
      "Epoch 281/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5608\n",
      "Epoch 282/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5377\n",
      "Epoch 283/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5810\n",
      "Epoch 284/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6151\n",
      "Epoch 285/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6087\n",
      "Epoch 286/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6704\n",
      "Epoch 287/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6616\n",
      "Epoch 288/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6292\n",
      "Epoch 289/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6168\n",
      "Epoch 290/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5605\n",
      "Epoch 291/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7248\n",
      "Epoch 292/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7545\n",
      "Epoch 293/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6784\n",
      "Epoch 294/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7505\n",
      "Epoch 295/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6651\n",
      "Epoch 296/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6594\n",
      "Epoch 297/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5816\n",
      "Epoch 298/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.8188\n",
      "Epoch 299/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5874\n",
      "Epoch 300/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6057\n",
      "Epoch 301/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5282\n",
      "Epoch 302/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5886\n",
      "Epoch 303/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6916\n",
      "Epoch 304/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6763\n",
      "Epoch 305/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6000\n",
      "Epoch 306/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5369\n",
      "Epoch 307/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5466\n",
      "Epoch 308/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.7361\n",
      "Epoch 309/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6805\n",
      "Epoch 310/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5375\n",
      "Epoch 311/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5906\n",
      "Epoch 312/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.9001\n",
      "Epoch 313/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6226\n",
      "Epoch 314/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6184\n",
      "Epoch 315/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5584\n",
      "Epoch 316/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5809\n",
      "Epoch 317/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6064\n",
      "Epoch 318/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7291\n",
      "Epoch 319/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5567\n",
      "Epoch 320/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5988\n",
      "Epoch 321/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7040\n",
      "Epoch 322/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5570\n",
      "Epoch 323/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.9302\n",
      "Epoch 324/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5642\n",
      "Epoch 325/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7532\n",
      "Epoch 326/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5961\n",
      "Epoch 327/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5291\n",
      "Epoch 328/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5284\n",
      "Epoch 329/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5808\n",
      "Epoch 330/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5899\n",
      "Epoch 331/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5202\n",
      "Epoch 332/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5902\n",
      "Epoch 333/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6167\n",
      "Epoch 334/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5374\n",
      "Epoch 335/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5630\n",
      "Epoch 336/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5165\n",
      "Epoch 337/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6283\n",
      "Epoch 338/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.507 - 0s 12us/step - loss: 2.5445\n",
      "Epoch 339/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5345\n",
      "Epoch 340/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6254\n",
      "Epoch 341/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6656\n",
      "Epoch 342/600\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.6684\n",
      "Epoch 343/600\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.5215\n",
      "Epoch 344/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5342\n",
      "Epoch 345/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6266\n",
      "Epoch 346/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5863\n",
      "Epoch 347/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5467\n",
      "Epoch 348/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7212\n",
      "Epoch 349/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5464\n",
      "Epoch 350/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6910\n",
      "Epoch 351/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5528\n",
      "Epoch 352/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5645\n",
      "Epoch 353/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5145\n",
      "Epoch 354/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6526\n",
      "Epoch 355/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4985\n",
      "Epoch 356/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5534\n",
      "Epoch 357/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6575\n",
      "Epoch 358/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6102\n",
      "Epoch 359/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6769\n",
      "Epoch 360/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5052\n",
      "Epoch 361/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5460\n",
      "Epoch 362/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6685\n",
      "Epoch 363/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6094\n",
      "Epoch 364/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5840\n",
      "Epoch 365/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6706\n",
      "Epoch 366/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5149\n",
      "Epoch 367/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4843\n",
      "Epoch 368/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6366\n",
      "Epoch 369/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6326\n",
      "Epoch 370/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6364\n",
      "Epoch 371/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.6050\n",
      "Epoch 372/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5174\n",
      "Epoch 373/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6317\n",
      "Epoch 374/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6146\n",
      "Epoch 375/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5474\n",
      "Epoch 376/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5234\n",
      "Epoch 377/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5353\n",
      "Epoch 378/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5564\n",
      "Epoch 379/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7677\n",
      "Epoch 380/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5062\n",
      "Epoch 381/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5010\n",
      "Epoch 382/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5138\n",
      "Epoch 383/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6763\n",
      "Epoch 384/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5261\n",
      "Epoch 385/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6537\n",
      "Epoch 386/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5037\n",
      "Epoch 387/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5293\n",
      "Epoch 388/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6614\n",
      "Epoch 389/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5528\n",
      "Epoch 390/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6316\n",
      "Epoch 391/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6346\n",
      "Epoch 392/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5338\n",
      "Epoch 393/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4850\n",
      "Epoch 394/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5757\n",
      "Epoch 395/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5759\n",
      "Epoch 396/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5131\n",
      "Epoch 397/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4785\n",
      "Epoch 398/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6957\n",
      "Epoch 399/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4992\n",
      "Epoch 400/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5134\n",
      "Epoch 401/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4986\n",
      "Epoch 402/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4613\n",
      "Epoch 403/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4580\n",
      "Epoch 404/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5459\n",
      "Epoch 405/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6140\n",
      "Epoch 406/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6245\n",
      "Epoch 408/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6483\n",
      "Epoch 409/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5521\n",
      "Epoch 410/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5295\n",
      "Epoch 411/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7200\n",
      "Epoch 412/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4799\n",
      "Epoch 413/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5300\n",
      "Epoch 414/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5607\n",
      "Epoch 415/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6415\n",
      "Epoch 416/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4970\n",
      "Epoch 417/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4701\n",
      "Epoch 418/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5059\n",
      "Epoch 419/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5268\n",
      "Epoch 420/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6083\n",
      "Epoch 421/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4835\n",
      "Epoch 422/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4947\n",
      "Epoch 423/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.7163\n",
      "Epoch 424/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4793\n",
      "Epoch 425/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.5391\n",
      "Epoch 426/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5212\n",
      "Epoch 427/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4627\n",
      "Epoch 428/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5195\n",
      "Epoch 429/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6658\n",
      "Epoch 430/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4842\n",
      "Epoch 431/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4863\n",
      "Epoch 432/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5763\n",
      "Epoch 433/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5989\n",
      "Epoch 434/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4869\n",
      "Epoch 435/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5011\n",
      "Epoch 436/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5221\n",
      "Epoch 437/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4898\n",
      "Epoch 438/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6364\n",
      "Epoch 439/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4639\n",
      "Epoch 440/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4991\n",
      "Epoch 441/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5731\n",
      "Epoch 442/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4742\n",
      "Epoch 443/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4649\n",
      "Epoch 444/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4898\n",
      "Epoch 445/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6279\n",
      "Epoch 446/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5105\n",
      "Epoch 447/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5264\n",
      "Epoch 448/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6160\n",
      "Epoch 449/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4972\n",
      "Epoch 450/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4627\n",
      "Epoch 451/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4501\n",
      "Epoch 452/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4551\n",
      "Epoch 453/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6288\n",
      "Epoch 454/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6134\n",
      "Epoch 455/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4898\n",
      "Epoch 456/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4516\n",
      "Epoch 457/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4611\n",
      "Epoch 458/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5484\n",
      "Epoch 459/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5274\n",
      "Epoch 460/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6143\n",
      "Epoch 461/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4609\n",
      "Epoch 462/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5197\n",
      "Epoch 463/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6126\n",
      "Epoch 464/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4370\n",
      "Epoch 465/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4818\n",
      "Epoch 466/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4418\n",
      "Epoch 467/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4734\n",
      "Epoch 468/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6469\n",
      "Epoch 469/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.607 - 0s 11us/step - loss: 2.5690\n",
      "Epoch 470/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6681\n",
      "Epoch 471/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5266\n",
      "Epoch 472/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4644\n",
      "Epoch 473/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5027\n",
      "Epoch 474/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5582\n",
      "Epoch 475/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6657\n",
      "Epoch 476/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6338\n",
      "Epoch 477/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4520\n",
      "Epoch 478/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6836\n",
      "Epoch 479/600\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.4562\n",
      "Epoch 480/600\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.4657\n",
      "Epoch 481/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4533\n",
      "Epoch 482/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5211\n",
      "Epoch 483/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4249\n",
      "Epoch 484/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5084\n",
      "Epoch 485/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5854\n",
      "Epoch 486/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5278\n",
      "Epoch 487/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5336\n",
      "Epoch 488/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5721\n",
      "Epoch 489/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6733\n",
      "Epoch 490/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4989\n",
      "Epoch 491/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4768\n",
      "Epoch 492/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5707\n",
      "Epoch 493/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5238\n",
      "Epoch 494/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6027\n",
      "Epoch 495/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5211\n",
      "Epoch 496/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4581\n",
      "Epoch 497/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4599\n",
      "Epoch 498/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4186\n",
      "Epoch 499/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5069\n",
      "Epoch 500/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4547\n",
      "Epoch 501/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4926\n",
      "Epoch 502/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5086\n",
      "Epoch 503/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4599\n",
      "Epoch 504/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5779\n",
      "Epoch 505/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5258\n",
      "Epoch 506/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4669\n",
      "Epoch 507/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5360\n",
      "Epoch 508/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4766\n",
      "Epoch 509/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4978\n",
      "Epoch 510/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5878\n",
      "Epoch 511/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5113\n",
      "Epoch 512/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5276\n",
      "Epoch 513/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5464\n",
      "Epoch 514/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4456\n",
      "Epoch 515/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5575\n",
      "Epoch 516/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5948\n",
      "Epoch 517/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4691\n",
      "Epoch 518/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4729\n",
      "Epoch 519/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5652\n",
      "Epoch 520/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4529\n",
      "Epoch 521/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6588\n",
      "Epoch 522/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4318\n",
      "Epoch 523/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5203\n",
      "Epoch 524/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4934\n",
      "Epoch 525/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4758\n",
      "Epoch 526/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8261\n",
      "Epoch 527/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4225\n",
      "Epoch 528/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5166\n",
      "Epoch 529/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4272\n",
      "Epoch 530/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4022\n",
      "Epoch 531/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4047\n",
      "Epoch 532/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4092\n",
      "Epoch 533/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5287\n",
      "Epoch 534/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5502\n",
      "Epoch 535/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4238\n",
      "Epoch 536/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5646\n",
      "Epoch 537/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4047\n",
      "Epoch 538/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4491\n",
      "Epoch 539/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4805\n",
      "Epoch 540/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5708\n",
      "Epoch 541/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6031\n",
      "Epoch 542/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4846\n",
      "Epoch 543/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6867\n",
      "Epoch 544/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4286\n",
      "Epoch 545/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5690\n",
      "Epoch 546/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.5231\n",
      "Epoch 547/600\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4812\n",
      "Epoch 548/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4832\n",
      "Epoch 549/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4979\n",
      "Epoch 550/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4030\n",
      "Epoch 551/600\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.355 - 0s 22us/step - loss: 2.4179\n",
      "Epoch 552/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5087\n",
      "Epoch 553/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4622\n",
      "Epoch 554/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4650\n",
      "Epoch 555/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5608\n",
      "Epoch 556/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4961\n",
      "Epoch 557/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4934\n",
      "Epoch 558/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4438\n",
      "Epoch 559/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4922\n",
      "Epoch 560/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4402\n",
      "Epoch 561/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5219\n",
      "Epoch 562/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4314\n",
      "Epoch 563/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4473\n",
      "Epoch 564/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5803\n",
      "Epoch 565/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4520\n",
      "Epoch 566/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4005\n",
      "Epoch 567/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4265\n",
      "Epoch 568/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4262\n",
      "Epoch 569/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4891\n",
      "Epoch 570/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4223\n",
      "Epoch 571/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4817\n",
      "Epoch 572/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4062\n",
      "Epoch 573/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5688\n",
      "Epoch 574/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4629\n",
      "Epoch 575/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4543\n",
      "Epoch 576/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4569\n",
      "Epoch 577/600\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4278\n",
      "Epoch 578/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4866\n",
      "Epoch 579/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4526\n",
      "Epoch 580/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5420\n",
      "Epoch 581/600\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4860\n",
      "Epoch 582/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4288\n",
      "Epoch 583/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.5722\n",
      "Epoch 584/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4491\n",
      "Epoch 585/600\n",
      "1295/1295 [==============================] - 0s 5us/step - loss: 2.4540\n",
      "Epoch 586/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.3943\n",
      "Epoch 587/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4068\n",
      "Epoch 588/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3662\n",
      "Epoch 589/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5158\n",
      "Epoch 590/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5319\n",
      "Epoch 591/600\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5755\n",
      "Epoch 592/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4115\n",
      "Epoch 593/600\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.4806\n",
      "Epoch 594/600\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3923\n",
      "Epoch 595/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 596/600\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4870\n",
      "Epoch 597/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4487\n",
      "Epoch 598/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.6444\n",
      "Epoch 599/600\n",
      "1295/1295 [==============================] - 0s 6us/step - loss: 2.4447\n",
      "Epoch 600/600\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [57.21454772949219,\n",
       "  50.687165069580075,\n",
       "  37.54488182067871,\n",
       "  21.51938591003418,\n",
       "  16.847016525268554,\n",
       "  16.24608325958252,\n",
       "  15.524640655517578,\n",
       "  14.274700355529784,\n",
       "  12.17860107421875,\n",
       "  9.801820182800293,\n",
       "  8.811099243164062,\n",
       "  8.615861129760741,\n",
       "  8.52487735748291,\n",
       "  8.409569549560548,\n",
       "  8.276469612121582,\n",
       "  8.156628227233886,\n",
       "  7.9920989036560055,\n",
       "  7.813495445251465,\n",
       "  7.614495277404785,\n",
       "  7.364999771118164,\n",
       "  7.1605480194091795,\n",
       "  6.923810863494873,\n",
       "  6.666667175292969,\n",
       "  6.4254988670349125,\n",
       "  6.189587116241455,\n",
       "  5.971698379516601,\n",
       "  5.763789463043213,\n",
       "  5.497400188446045,\n",
       "  5.329218769073487,\n",
       "  5.033742237091064,\n",
       "  4.837559223175049,\n",
       "  4.509328556060791,\n",
       "  4.261570739746094,\n",
       "  4.139410400390625,\n",
       "  3.9709015846252442,\n",
       "  3.860390615463257,\n",
       "  4.366285037994385,\n",
       "  4.519093608856201,\n",
       "  3.8685631275177004,\n",
       "  3.704967212677002,\n",
       "  3.9000033855438234,\n",
       "  4.367709445953369,\n",
       "  3.688839626312256,\n",
       "  3.5196534633636474,\n",
       "  3.7051074504852295,\n",
       "  3.6757468700408937,\n",
       "  3.354991006851196,\n",
       "  3.274302101135254,\n",
       "  3.3170395374298094,\n",
       "  3.6173712730407717,\n",
       "  4.134168243408203,\n",
       "  3.8932435512542725,\n",
       "  3.8354645729064942,\n",
       "  3.357354974746704,\n",
       "  3.413342761993408,\n",
       "  3.357678747177124,\n",
       "  3.274828052520752,\n",
       "  3.162229871749878,\n",
       "  3.2798100471496583,\n",
       "  3.1983200550079345,\n",
       "  3.1113765239715576,\n",
       "  3.5817198753356934,\n",
       "  3.4955037593841554,\n",
       "  3.175920915603638,\n",
       "  3.183470582962036,\n",
       "  3.1354055404663086,\n",
       "  3.0777214050292967,\n",
       "  3.0519155502319335,\n",
       "  3.306360197067261,\n",
       "  3.5163743019104006,\n",
       "  3.185206174850464,\n",
       "  3.2633142471313477,\n",
       "  3.165359878540039,\n",
       "  3.0197215557098387,\n",
       "  2.9728347301483153,\n",
       "  3.1435391426086428,\n",
       "  3.2112184047698973,\n",
       "  3.1903942584991456,\n",
       "  3.5183709144592283,\n",
       "  3.069278860092163,\n",
       "  2.9604976177215576,\n",
       "  3.2007705688476564,\n",
       "  3.0165530681610107,\n",
       "  2.962748098373413,\n",
       "  3.1771544456481933,\n",
       "  3.4531258583068847,\n",
       "  3.059106397628784,\n",
       "  3.060058116912842,\n",
       "  2.9714215278625487,\n",
       "  2.954399299621582,\n",
       "  3.000346612930298,\n",
       "  2.959947443008423,\n",
       "  3.0166409015655518,\n",
       "  3.380245304107666,\n",
       "  2.9881818771362303,\n",
       "  3.0402814388275146,\n",
       "  2.968109893798828,\n",
       "  2.9621390819549562,\n",
       "  2.910461091995239,\n",
       "  3.0950313091278074,\n",
       "  3.0280545711517335,\n",
       "  3.0017754554748537,\n",
       "  3.119375991821289,\n",
       "  2.8948934078216553,\n",
       "  2.940952682495117,\n",
       "  3.127616310119629,\n",
       "  2.900259828567505,\n",
       "  3.0120711803436278,\n",
       "  2.82944803237915,\n",
       "  2.8272653102874754,\n",
       "  2.8968286514282227,\n",
       "  2.8568217754364014,\n",
       "  2.860443925857544,\n",
       "  3.207522487640381,\n",
       "  3.7521377086639403,\n",
       "  2.987202787399292,\n",
       "  2.8379637241363525,\n",
       "  2.7913147449493407,\n",
       "  2.9318235397338865,\n",
       "  2.871771478652954,\n",
       "  3.074945497512817,\n",
       "  3.0576282501220704,\n",
       "  2.8628118991851808,\n",
       "  2.84365029335022,\n",
       "  3.0331258296966555,\n",
       "  2.8305139541625977,\n",
       "  3.064422941207886,\n",
       "  2.9051305294036864,\n",
       "  2.9553067684173584,\n",
       "  2.8083282947540282,\n",
       "  2.8013070583343507,\n",
       "  2.803950309753418,\n",
       "  2.8667590618133545,\n",
       "  2.855555868148804,\n",
       "  2.81970100402832,\n",
       "  2.8622889041900637,\n",
       "  2.832348918914795,\n",
       "  2.7795623302459718,\n",
       "  2.81875319480896,\n",
       "  2.798099994659424,\n",
       "  2.851491355895996,\n",
       "  2.7247769832611084,\n",
       "  2.8973715782165526,\n",
       "  2.939290904998779,\n",
       "  2.787012815475464,\n",
       "  2.7566993713378904,\n",
       "  3.0117908000946043,\n",
       "  2.7652556896209717,\n",
       "  2.9870513439178468,\n",
       "  3.0384739875793456,\n",
       "  2.788803768157959,\n",
       "  2.9499184608459474,\n",
       "  2.812078666687012,\n",
       "  2.806785726547241,\n",
       "  2.7973531246185304,\n",
       "  2.7341700077056883,\n",
       "  2.7573190689086915,\n",
       "  2.854450750350952,\n",
       "  2.7607894420623778,\n",
       "  2.7626920700073243,\n",
       "  2.9151498317718505,\n",
       "  2.931792736053467,\n",
       "  2.948745584487915,\n",
       "  2.697874116897583,\n",
       "  2.7028807163238526,\n",
       "  2.6903459072113036,\n",
       "  2.7610870838165282,\n",
       "  2.738741159439087,\n",
       "  2.6937219142913817,\n",
       "  2.7886382579803466,\n",
       "  2.8593231201171876,\n",
       "  2.7565423488616942,\n",
       "  2.731218957901001,\n",
       "  2.715948724746704,\n",
       "  2.873705434799194,\n",
       "  2.8824395656585695,\n",
       "  2.836450719833374,\n",
       "  2.7744102001190187,\n",
       "  2.6751134395599365,\n",
       "  2.7051909923553468,\n",
       "  2.829745388031006,\n",
       "  2.8830759525299072,\n",
       "  2.7751887798309327,\n",
       "  2.803305959701538,\n",
       "  2.7048184871673584,\n",
       "  2.929511785507202,\n",
       "  2.7102224826812744,\n",
       "  3.055039405822754,\n",
       "  2.7027596950531008,\n",
       "  2.6492101192474364,\n",
       "  2.7532509326934815,\n",
       "  2.793574666976929,\n",
       "  2.6786278247833253,\n",
       "  2.7090587615966797,\n",
       "  2.7528453350067137,\n",
       "  2.916064167022705,\n",
       "  2.734561729431152,\n",
       "  2.7886444568634032,\n",
       "  2.7782451152801513,\n",
       "  2.6200302124023436,\n",
       "  2.6883686542510987,\n",
       "  2.8535069942474367,\n",
       "  2.7782004356384276,\n",
       "  2.7882268905639647,\n",
       "  2.734892654418945,\n",
       "  2.820246171951294,\n",
       "  2.8623711109161376,\n",
       "  2.62115740776062,\n",
       "  2.773115539550781,\n",
       "  2.6717304229736327,\n",
       "  2.7287423610687256,\n",
       "  2.6537779808044433,\n",
       "  2.789725637435913,\n",
       "  2.726212167739868,\n",
       "  2.7628687381744386,\n",
       "  2.68122239112854,\n",
       "  2.657624673843384,\n",
       "  2.744238042831421,\n",
       "  2.7446295261383056,\n",
       "  2.6766912937164307,\n",
       "  2.651298761367798,\n",
       "  2.6796553134918213,\n",
       "  2.7803316593170164,\n",
       "  2.8421252727508546,\n",
       "  2.6327139854431154,\n",
       "  2.5991705417633058,\n",
       "  2.6090453624725343,\n",
       "  2.7514244556427,\n",
       "  2.6085031032562256,\n",
       "  2.6372491359710692,\n",
       "  2.609810495376587,\n",
       "  2.795747709274292,\n",
       "  2.697306203842163,\n",
       "  2.706653022766113,\n",
       "  2.8260963439941404,\n",
       "  2.746039056777954,\n",
       "  2.600348424911499,\n",
       "  2.729938268661499,\n",
       "  2.7943931102752684,\n",
       "  2.789534902572632,\n",
       "  2.647345542907715,\n",
       "  2.665682649612427,\n",
       "  2.6709455966949465,\n",
       "  2.600569486618042,\n",
       "  2.7688482761383058,\n",
       "  2.6991255283355713,\n",
       "  2.6976649284362795,\n",
       "  2.748558282852173,\n",
       "  2.699961805343628,\n",
       "  2.583288621902466,\n",
       "  2.7790050983428953,\n",
       "  2.6774494647979736,\n",
       "  2.693792963027954,\n",
       "  2.6616672039031983,\n",
       "  2.661446380615234,\n",
       "  2.61586537361145,\n",
       "  2.632692241668701,\n",
       "  2.5797598361968994,\n",
       "  2.5699328422546386,\n",
       "  2.5774208545684814,\n",
       "  2.625441312789917,\n",
       "  2.605804777145386,\n",
       "  2.581649160385132,\n",
       "  2.6116074085235597,\n",
       "  2.696325397491455,\n",
       "  2.969168996810913,\n",
       "  2.6157541275024414,\n",
       "  2.67630877494812,\n",
       "  2.6836924076080324,\n",
       "  2.6195910930633546,\n",
       "  2.727127504348755,\n",
       "  2.7055545330047606,\n",
       "  2.629731321334839,\n",
       "  2.8124882221221923,\n",
       "  2.7073519229888916,\n",
       "  2.689346265792847,\n",
       "  2.7634910106658936,\n",
       "  2.69809308052063,\n",
       "  2.5932953357696533,\n",
       "  2.5762050628662108,\n",
       "  2.560819387435913,\n",
       "  2.5377039432525637,\n",
       "  2.5810126781463625,\n",
       "  2.6151068210601807,\n",
       "  2.6086746215820313,\n",
       "  2.6703960418701174,\n",
       "  2.661561441421509,\n",
       "  2.6291659355163572,\n",
       "  2.616768789291382,\n",
       "  2.5605210304260253,\n",
       "  2.7248250007629395,\n",
       "  2.7545098781585695,\n",
       "  2.678377866744995,\n",
       "  2.750530958175659,\n",
       "  2.6650672912597657,\n",
       "  2.6594074726104737,\n",
       "  2.581553506851196,\n",
       "  2.818755054473877,\n",
       "  2.587361192703247,\n",
       "  2.6057380199432374,\n",
       "  2.528196096420288,\n",
       "  2.588554334640503,\n",
       "  2.6915781497955322,\n",
       "  2.676312875747681,\n",
       "  2.599953031539917,\n",
       "  2.5368608951568605,\n",
       "  2.5466118335723875,\n",
       "  2.7361030101776125,\n",
       "  2.680508279800415,\n",
       "  2.5375162601470946,\n",
       "  2.590565490722656,\n",
       "  2.9001367568969725,\n",
       "  2.622628355026245,\n",
       "  2.6183900356292726,\n",
       "  2.558387613296509,\n",
       "  2.5809488773345945,\n",
       "  2.606437063217163,\n",
       "  2.7291072368621827,\n",
       "  2.5567461490631103,\n",
       "  2.598782014846802,\n",
       "  2.7039695262908934,\n",
       "  2.5569722652435303,\n",
       "  2.9302415370941164,\n",
       "  2.564221143722534,\n",
       "  2.7532498359680178,\n",
       "  2.5960944175720213,\n",
       "  2.529143476486206,\n",
       "  2.5283985137939453,\n",
       "  2.580750513076782,\n",
       "  2.589902877807617,\n",
       "  2.520168924331665,\n",
       "  2.5901663303375244,\n",
       "  2.616680908203125,\n",
       "  2.537361192703247,\n",
       "  2.5629961013793947,\n",
       "  2.5165266513824465,\n",
       "  2.628326654434204,\n",
       "  2.5444639682769776,\n",
       "  2.534485864639282,\n",
       "  2.6253735065460204,\n",
       "  2.665601110458374,\n",
       "  2.668359899520874,\n",
       "  2.5214576721191406,\n",
       "  2.534212112426758,\n",
       "  2.6265825748443605,\n",
       "  2.5862546443939207,\n",
       "  2.5467414379119875,\n",
       "  2.721236753463745,\n",
       "  2.5463887214660645,\n",
       "  2.6910030364990236,\n",
       "  2.5527830123901367,\n",
       "  2.5644670963287353,\n",
       "  2.5145076751708983,\n",
       "  2.6526052951812744,\n",
       "  2.4984859466552733,\n",
       "  2.55343337059021,\n",
       "  2.6574902057647707,\n",
       "  2.6102082252502443,\n",
       "  2.676913785934448,\n",
       "  2.5052220821380615,\n",
       "  2.546007680892944,\n",
       "  2.6684500217437743,\n",
       "  2.6093522548675536,\n",
       "  2.5840057373046874,\n",
       "  2.67061767578125,\n",
       "  2.5149084091186524,\n",
       "  2.484325313568115,\n",
       "  2.6366481304168703,\n",
       "  2.632629632949829,\n",
       "  2.6364251136779786,\n",
       "  2.605006742477417,\n",
       "  2.517449951171875,\n",
       "  2.6316742420196535,\n",
       "  2.614634561538696,\n",
       "  2.547426176071167,\n",
       "  2.5234353065490724,\n",
       "  2.535347890853882,\n",
       "  2.556387710571289,\n",
       "  2.7676716327667235,\n",
       "  2.506241798400879,\n",
       "  2.5010118961334227,\n",
       "  2.5137971878051757,\n",
       "  2.6763187408447267,\n",
       "  2.526135206222534,\n",
       "  2.653694486618042,\n",
       "  2.503696250915527,\n",
       "  2.5292969703674317,\n",
       "  2.6613513946533205,\n",
       "  2.5528470516204833,\n",
       "  2.631597471237183,\n",
       "  2.6345558166503906,\n",
       "  2.5337947845458983,\n",
       "  2.4849682807922364,\n",
       "  2.5756882190704347,\n",
       "  2.5758760929107667,\n",
       "  2.513074350357056,\n",
       "  2.47851939201355,\n",
       "  2.6956871509552003,\n",
       "  2.4991751194000242,\n",
       "  2.513428068161011,\n",
       "  2.4986019134521484,\n",
       "  2.4613013744354246,\n",
       "  2.4580006122589113,\n",
       "  2.5459351539611816,\n",
       "  2.613966369628906,\n",
       "  2.5265771389007567,\n",
       "  2.6244555950164794,\n",
       "  2.6483450412750242,\n",
       "  2.5521201610565187,\n",
       "  2.5294801235198974,\n",
       "  2.719969320297241,\n",
       "  2.4799421787261964,\n",
       "  2.5300344944000246,\n",
       "  2.5607061862945555,\n",
       "  2.641468572616577,\n",
       "  2.497039461135864,\n",
       "  2.4701213359832765,\n",
       "  2.505942869186401,\n",
       "  2.5268004894256593,\n",
       "  2.608336591720581,\n",
       "  2.483521270751953,\n",
       "  2.4947240352630615,\n",
       "  2.7163002490997314,\n",
       "  2.479270648956299,\n",
       "  2.5391318798065186,\n",
       "  2.521192693710327,\n",
       "  2.4626842975616454,\n",
       "  2.5195324420928955,\n",
       "  2.6658390045166014,\n",
       "  2.4841880321502687,\n",
       "  2.486284780502319,\n",
       "  2.576330232620239,\n",
       "  2.5988669872283934,\n",
       "  2.486948013305664,\n",
       "  2.501055288314819,\n",
       "  2.5220826625823975,\n",
       "  2.4897600650787353,\n",
       "  2.6364009380340576,\n",
       "  2.4639368534088133,\n",
       "  2.499122667312622,\n",
       "  2.5730659484863283,\n",
       "  2.4741912841796876,\n",
       "  2.464883327484131,\n",
       "  2.489849853515625,\n",
       "  2.6279123783111573,\n",
       "  2.510497236251831,\n",
       "  2.5264141082763674,\n",
       "  2.6159615993499754,\n",
       "  2.4972198963165284,\n",
       "  2.4627317428588866,\n",
       "  2.4501030445098877,\n",
       "  2.455133247375488,\n",
       "  2.6288086414337157,\n",
       "  2.613419198989868,\n",
       "  2.489831829071045,\n",
       "  2.4516032218933104,\n",
       "  2.46111798286438,\n",
       "  2.5484357357025145,\n",
       "  2.5273932933807375,\n",
       "  2.6143375873565673,\n",
       "  2.460943651199341,\n",
       "  2.5196929454803465,\n",
       "  2.6126262187957763,\n",
       "  2.4369701385498046,\n",
       "  2.4818442821502686,\n",
       "  2.441776704788208,\n",
       "  2.4733821392059325,\n",
       "  2.646935224533081,\n",
       "  2.569041109085083,\n",
       "  2.6680683135986327,\n",
       "  2.526646137237549,\n",
       "  2.464377927780151,\n",
       "  2.5026714324951174,\n",
       "  2.5582001209259033,\n",
       "  2.665655565261841,\n",
       "  2.633825731277466,\n",
       "  2.4520083904266357,\n",
       "  2.6835933208465574,\n",
       "  2.456202840805054,\n",
       "  2.4657450675964356,\n",
       "  2.453336763381958,\n",
       "  2.5210889339447022,\n",
       "  2.424936056137085,\n",
       "  2.5084428787231445,\n",
       "  2.5853559017181396,\n",
       "  2.5277791023254395,\n",
       "  2.533572816848755,\n",
       "  2.5720638751983644,\n",
       "  2.673295021057129,\n",
       "  2.4989403247833253,\n",
       "  2.476819658279419,\n",
       "  2.5707409381866455,\n",
       "  2.523815393447876,\n",
       "  2.6026626110076903,\n",
       "  2.5210649967193604,\n",
       "  2.458127737045288,\n",
       "  2.459884786605835,\n",
       "  2.4185599803924562,\n",
       "  2.506855535507202,\n",
       "  2.4547022342681886,\n",
       "  2.4925796508789064,\n",
       "  2.5085581302642823,\n",
       "  2.4599050521850585,\n",
       "  2.577906370162964,\n",
       "  2.525781011581421,\n",
       "  2.4668839454650877,\n",
       "  2.535999059677124,\n",
       "  2.476623868942261,\n",
       "  2.4978121280670167,\n",
       "  2.587769603729248,\n",
       "  2.5113440990447997,\n",
       "  2.5276254653930663,\n",
       "  2.5463788509368896,\n",
       "  2.445605516433716,\n",
       "  2.557498407363892,\n",
       "  2.5948418617248534,\n",
       "  2.469117784500122,\n",
       "  2.472907543182373,\n",
       "  2.565167474746704,\n",
       "  2.4529083728790284,\n",
       "  2.6587533473968508,\n",
       "  2.4318182468414307,\n",
       "  2.5202755451202394,\n",
       "  2.4933969020843505,\n",
       "  2.47578125,\n",
       "  2.8261008739471434,\n",
       "  2.422482204437256,\n",
       "  2.5166250705718993,\n",
       "  2.4272463798522947,\n",
       "  2.4022262573242186,\n",
       "  2.4047091007232666,\n",
       "  2.4091774463653564,\n",
       "  2.5286633014678954,\n",
       "  2.550198221206665,\n",
       "  2.4237709045410156,\n",
       "  2.5645589351654055,\n",
       "  2.4047443866729736,\n",
       "  2.449128818511963,\n",
       "  2.4804843425750733,\n",
       "  2.57076153755188,\n",
       "  2.603074789047241,\n",
       "  2.4845643520355223,\n",
       "  2.6867299556732176,\n",
       "  2.4286306858062745,\n",
       "  2.5690417289733887,\n",
       "  2.5231160640716555,\n",
       "  2.4812158584594726,\n",
       "  2.4832337856292725,\n",
       "  2.4978805065155028,\n",
       "  2.4029987335205076,\n",
       "  2.417943906784058,\n",
       "  2.5086525440216065,\n",
       "  2.4622066020965576,\n",
       "  2.4650009632110597,\n",
       "  2.5608450889587404,\n",
       "  2.496088647842407,\n",
       "  2.49336724281311,\n",
       "  2.4437914371490477,\n",
       "  2.492212152481079,\n",
       "  2.440154695510864,\n",
       "  2.5218643665313722,\n",
       "  2.431440544128418,\n",
       "  2.4473072052001954,\n",
       "  2.580257511138916,\n",
       "  2.4519851207733154,\n",
       "  2.400452661514282,\n",
       "  2.4265204429626466,\n",
       "  2.4261779308319094,\n",
       "  2.489111042022705,\n",
       "  2.422250986099243,\n",
       "  2.4816528797149657,\n",
       "  2.4062172412872314,\n",
       "  2.568836784362793,\n",
       "  2.462924909591675,\n",
       "  2.4543396472930907,\n",
       "  2.4568739891052247,\n",
       "  2.427785539627075,\n",
       "  2.486637020111084,\n",
       "  2.4525946140289308,\n",
       "  2.542024278640747,\n",
       "  2.486000633239746,\n",
       "  2.4287984371185303,\n",
       "  2.5722327709197996,\n",
       "  2.4491116046905517,\n",
       "  2.4540199279785155,\n",
       "  2.394257688522339,\n",
       "  2.406824493408203,\n",
       "  2.366182231903076,\n",
       "  2.5158438682556152,\n",
       "  2.531854820251465,\n",
       "  2.5754839420318603,\n",
       "  2.411492872238159,\n",
       "  2.480615568161011,\n",
       "  2.3922943592071535,\n",
       "  2.4766902923583984,\n",
       "  2.487036180496216,\n",
       "  2.4487078189849854,\n",
       "  2.644370412826538,\n",
       "  2.4447419166564943,\n",
       "  2.452237272262573]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyVal_Low_LR = []\n",
    "historyTr_Low_LR = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_Low_LR()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=1036).history\n",
    "    historyVal_Low_LR.append(history['val_loss'])\n",
    "    historyTr_Low_LR.append(history['loss'])\n",
    "model=create_model_Low_LR()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=600, \n",
    "                      batch_size=1036).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_Low_LR_mean=np.mean(historyVal_Low_LR, axis=0)\n",
    "historyTr_Low_LR_mean=np.mean(historyTr_Low_LR, axis=0)\n",
    "\n",
    "historyVal_Low_LR_sd=np.std(historyVal_Low_LR, axis=0)\n",
    "historyTr_Low_LR_sd=np.std(historyTr_Low_LR, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_Low_LR_mean)),historyTr_Low_LR_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_Low_LR_mean)), historyTr_Low_LR_mean - historyTr_Low_LR_sd, \n",
    "                 historyTr_Low_LR_mean + historyTr_Low_LR_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_Low_LR_mean)), historyVal_Low_LR_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_Low_LR_mean)), historyVal_Low_LR_mean - historyVal_Low_LR_sd, \n",
    "                 historyVal_Low_LR_mean + historyVal_Low_LR_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.title('Learning curves low learning rate', fontsize = 18)\n",
    "plt.legend()\n",
    "plt.ylim(5,80)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 81, +3))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low regularization (lr=0.01) result:\n",
      "MEE on the validation 7.975345325469971 with standard deviation 0.3599013297792157\n",
      "MEE on the training 7.935428237915039 with standard deviation 0.19395151580301634\n"
     ]
    }
   ],
   "source": [
    "print(\"Low regularization (lr=0.01) result:\")\n",
    "print(\"MEE on the validation\",historyVal_Low_LR_mean[-1],\"with standard deviation\",historyVal_Low_LR_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_Low_LR_mean[-1],\"with standard deviation\",historyTr_Low_LR_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_MB():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=0.1, \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 778us/step - loss: 48.0144 - val_loss: 35.7221\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 22.4505 - val_loss: 16.6197\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 14.2318 - val_loss: 11.1148\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 9.4326 - val_loss: 8.6313\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 8.3233 - val_loss: 8.4736\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 7.9449 - val_loss: 7.9329\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 7.4196 - val_loss: 7.3512\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 6.7684 - val_loss: 6.6035\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 6.1395 - val_loss: 5.8583\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 5.4534 - val_loss: 5.2391\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 4.8682 - val_loss: 4.5665\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 4.4010 - val_loss: 4.2399\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 4.0433 - val_loss: 3.9435\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 3.8228 - val_loss: 3.6826\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.6494 - val_loss: 3.6124\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 3.5207 - val_loss: 3.5032\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 3.4361 - val_loss: 3.3402\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.3712 - val_loss: 3.2010\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 3.2991 - val_loss: 3.1817\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 3.2440 - val_loss: 3.3740\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.2108 - val_loss: 3.0763\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 3.1537 - val_loss: 3.1019\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.1102 - val_loss: 2.9799\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.1000 - val_loss: 3.1079\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 3.0888 - val_loss: 3.0135\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 3.0686 - val_loss: 3.0147\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.0585 - val_loss: 3.0101\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.0465 - val_loss: 2.9377\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.0180 - val_loss: 2.9102\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.9967 - val_loss: 3.1337\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.9778 - val_loss: 2.8546\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.9838 - val_loss: 2.8985\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.9944 - val_loss: 2.9200\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.9204 - val_loss: 2.9004\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.9153 - val_loss: 2.8575\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.9503 - val_loss: 3.1207\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.9337 - val_loss: 2.8861\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8688 - val_loss: 2.9230\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.8760 - val_loss: 2.8478\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.8691 - val_loss: 2.9058\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 2.8734 - val_loss: 2.8455\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.8407 - val_loss: 2.8639\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.8430 - val_loss: 2.7987\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.8257 - val_loss: 3.0077\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 2.8587 - val_loss: 2.9939\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.8361 - val_loss: 2.8410\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.8139 - val_loss: 2.7733\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8109 - val_loss: 3.0403\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.8188 - val_loss: 2.7945\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7868 - val_loss: 2.8063\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8383 - val_loss: 2.8285\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7784 - val_loss: 2.8756\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7789 - val_loss: 2.7381\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8011 - val_loss: 2.8254\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.7601 - val_loss: 2.9267\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.7878 - val_loss: 2.7451\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.7470 - val_loss: 2.7214\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.7259 - val_loss: 2.7886\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.7630 - val_loss: 2.7534\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.7394 - val_loss: 2.7863\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.7407 - val_loss: 2.8528\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.7581 - val_loss: 2.9255\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.7668 - val_loss: 2.6753\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.7669 - val_loss: 2.8002\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.7259 - val_loss: 2.7548\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 2.7128 - val_loss: 2.8820\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.7207 - val_loss: 2.7736\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 2.6756 - val_loss: 2.7275\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.7135 - val_loss: 2.8280\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.7434 - val_loss: 2.9172\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.6966 - val_loss: 2.7253\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.6806 - val_loss: 2.6946\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.6781 - val_loss: 2.7387\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.6739 - val_loss: 2.6869\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.6810 - val_loss: 2.6773\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.6450 - val_loss: 2.7431\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.6396 - val_loss: 2.7209\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 2.6739 - val_loss: 2.7830\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.6484 - val_loss: 2.7257\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.6506 - val_loss: 2.7002\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6552 - val_loss: 2.7123\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6566 - val_loss: 2.7644\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.6469 - val_loss: 2.6402\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6354 - val_loss: 2.7480\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6345 - val_loss: 2.8412\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6837 - val_loss: 2.7853\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6311 - val_loss: 2.6826\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6675 - val_loss: 2.6519\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.6501 - val_loss: 2.7626\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6115 - val_loss: 2.6540\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6214 - val_loss: 2.8193\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.6439 - val_loss: 2.7934\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.6336 - val_loss: 2.7532\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6331 - val_loss: 2.6544\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.6237 - val_loss: 2.6424\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.6163 - val_loss: 2.7016\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6411 - val_loss: 2.6842\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.5843 - val_loss: 2.7967\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5977 - val_loss: 2.7342\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6118 - val_loss: 2.7027\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6187 - val_loss: 2.7539\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 2.5880 - val_loss: 2.6712\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 2.5825 - val_loss: 2.7202\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.5820 - val_loss: 2.6682\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.6153 - val_loss: 2.7108\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.5630 - val_loss: 2.7244\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.6013 - val_loss: 2.6302\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5757 - val_loss: 2.6444\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.5630 - val_loss: 2.7874\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5729 - val_loss: 2.6248\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.5822 - val_loss: 2.7229\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5684 - val_loss: 2.7882\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5732 - val_loss: 2.7032\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.5527 - val_loss: 2.6475\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5863 - val_loss: 2.6172\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5427 - val_loss: 2.6454\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5545 - val_loss: 2.6453\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5465 - val_loss: 2.6262\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5429 - val_loss: 2.7016\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5528 - val_loss: 2.5914\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 2.5338 - val_loss: 2.8019\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.5946 - val_loss: 2.6587\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 2.5762 - val_loss: 2.6975\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.5463 - val_loss: 2.6992\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.5389 - val_loss: 2.6262\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 2.5234 - val_loss: 2.7007\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 2.5240 - val_loss: 2.6301\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 2.5661 - val_loss: 2.7360\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5842 - val_loss: 2.6315\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 44us/step - loss: 2.5276 - val_loss: 2.6251\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 2.5299 - val_loss: 2.7544\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 2.5191 - val_loss: 2.6964\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 2.5415 - val_loss: 2.6892\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 2.5253 - val_loss: 2.6366\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.5249 - val_loss: 2.6803\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.5442 - val_loss: 2.6347\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 44us/step - loss: 2.4778 - val_loss: 2.6960\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.4975 - val_loss: 2.6415\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.5328 - val_loss: 2.8856\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5449 - val_loss: 2.6306\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.4923 - val_loss: 2.6463\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.4941 - val_loss: 2.6359\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 2.5366 - val_loss: 2.6049\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.5020 - val_loss: 2.6506\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4994 - val_loss: 2.6147\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4846 - val_loss: 2.6143\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5077 - val_loss: 2.7401\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4902 - val_loss: 2.5855\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4970 - val_loss: 2.6340\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4665 - val_loss: 2.6628\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4838 - val_loss: 2.6419\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5086 - val_loss: 2.6681\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4904 - val_loss: 2.8056\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5070 - val_loss: 2.6324\n",
      "Epoch 155/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4637 - val_loss: 2.7270\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4851 - val_loss: 2.6736\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5208 - val_loss: 2.6099\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4712 - val_loss: 2.6433\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4796 - val_loss: 2.6491\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4954 - val_loss: 2.6300\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.4548 - val_loss: 2.6036\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.4774 - val_loss: 2.6369\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.4913 - val_loss: 2.5917\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.4765 - val_loss: 2.6088\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4589 - val_loss: 2.6152\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4424 - val_loss: 2.6285\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4616 - val_loss: 2.6948\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4689 - val_loss: 2.6559\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4735 - val_loss: 2.6454\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4560 - val_loss: 2.6873\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4413 - val_loss: 2.6661\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4653 - val_loss: 2.6952\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4601 - val_loss: 2.6191\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4760 - val_loss: 2.5786\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4499 - val_loss: 2.7159\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4602 - val_loss: 2.6429\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4692 - val_loss: 2.6485\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4350 - val_loss: 2.6407\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4661 - val_loss: 2.6132\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4318 - val_loss: 2.6350\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4757 - val_loss: 2.7102\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4334 - val_loss: 2.5748\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4561 - val_loss: 2.6315\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4461 - val_loss: 2.6114\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4097 - val_loss: 2.6331\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4242 - val_loss: 2.7111\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4532 - val_loss: 2.7099\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4096 - val_loss: 2.6425\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4211 - val_loss: 2.7041\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4243 - val_loss: 2.6439\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4088 - val_loss: 2.7027\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4513 - val_loss: 2.7712\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4482 - val_loss: 2.6499\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4020 - val_loss: 2.6994\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4102 - val_loss: 2.6088\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4499 - val_loss: 2.6372\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4352 - val_loss: 2.6363\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4054 - val_loss: 2.5846\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4143 - val_loss: 2.6163\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4143 - val_loss: 2.6600\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3940 - val_loss: 2.5751\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4067 - val_loss: 2.7686\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4310 - val_loss: 2.6038\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3869 - val_loss: 2.6567\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4082 - val_loss: 2.7098\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4110 - val_loss: 2.6357\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3855 - val_loss: 2.6606\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3897 - val_loss: 2.7956\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4121 - val_loss: 2.6671\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3987 - val_loss: 2.6498\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4077 - val_loss: 2.6505\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3847 - val_loss: 2.6900\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3913 - val_loss: 2.6418\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4280 - val_loss: 2.7602\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4155 - val_loss: 2.6306\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3762 - val_loss: 2.6529\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3959 - val_loss: 2.6639\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3703 - val_loss: 2.5891\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3762 - val_loss: 2.7406\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3837 - val_loss: 2.6299\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3693 - val_loss: 2.6109\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4000 - val_loss: 2.6224\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4046 - val_loss: 2.6633\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3662 - val_loss: 2.6125\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3782 - val_loss: 2.7223\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3685 - val_loss: 2.5976\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3520 - val_loss: 2.6544\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3593 - val_loss: 2.6034\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3682 - val_loss: 2.7469\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3822 - val_loss: 2.6094\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3651 - val_loss: 2.6465\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3410 - val_loss: 2.6165\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3376 - val_loss: 2.7269\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3606 - val_loss: 2.6291\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.3572 - val_loss: 2.7665\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3776 - val_loss: 2.6508\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3436 - val_loss: 2.6511\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.3513 - val_loss: 2.6469\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3478 - val_loss: 2.6075\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3457 - val_loss: 2.7399\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3509 - val_loss: 2.5826\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3481 - val_loss: 2.6937\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3617 - val_loss: 2.6878\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3378 - val_loss: 2.7262\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3962 - val_loss: 2.6470\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3310 - val_loss: 2.7561\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3738 - val_loss: 2.6858\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3327 - val_loss: 2.6762\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3158 - val_loss: 2.5770\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3550 - val_loss: 2.6515\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3554 - val_loss: 2.7233\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3854 - val_loss: 2.6496\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.3274 - val_loss: 2.6883\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3516 - val_loss: 2.5950\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3168 - val_loss: 2.7417\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3174 - val_loss: 2.6109\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3269 - val_loss: 2.8241\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3559 - val_loss: 2.6253\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3550 - val_loss: 2.6101\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3272 - val_loss: 2.7697\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3211 - val_loss: 2.6065\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3253 - val_loss: 2.6517\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3047 - val_loss: 2.6857\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3163 - val_loss: 2.7370\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3492 - val_loss: 2.8140\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3981 - val_loss: 2.6531\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3349 - val_loss: 2.6115\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3287 - val_loss: 2.6713\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3241 - val_loss: 2.6122\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2991 - val_loss: 2.6196\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2860 - val_loss: 2.6238\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2907 - val_loss: 2.5694\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3058 - val_loss: 2.6262\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.3059 - val_loss: 2.8049\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3035 - val_loss: 2.6772\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3061 - val_loss: 2.7256\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2944 - val_loss: 2.6567\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3376 - val_loss: 2.6494\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3076 - val_loss: 2.6554\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2840 - val_loss: 2.6342\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3153 - val_loss: 2.6602\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3268 - val_loss: 2.6645\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3027 - val_loss: 2.6320\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3108 - val_loss: 2.6359\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2857 - val_loss: 2.5967\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2727 - val_loss: 2.7288\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3293 - val_loss: 2.6018\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2927 - val_loss: 2.6355\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2540 - val_loss: 2.6527\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2923 - val_loss: 2.6818\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2877 - val_loss: 2.6318\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2844 - val_loss: 2.6949\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2799 - val_loss: 2.6407\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2887 - val_loss: 2.6800\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2896 - val_loss: 2.7072\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2767 - val_loss: 2.7245\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2849 - val_loss: 2.6598\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2719 - val_loss: 2.6350\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2649 - val_loss: 2.6732\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2526 - val_loss: 2.6228\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2627 - val_loss: 2.6592\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3283 - val_loss: 2.6498\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2815 - val_loss: 2.6412\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2565 - val_loss: 2.7036\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2961 - val_loss: 2.7543\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2584 - val_loss: 2.6397\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2646 - val_loss: 2.6233\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2807 - val_loss: 2.6248\n",
      "Epoch 309/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2700 - val_loss: 2.6413\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2607 - val_loss: 2.6189\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2539 - val_loss: 2.7820\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2770 - val_loss: 2.7187\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2501 - val_loss: 2.7857\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2937 - val_loss: 2.6120\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2474 - val_loss: 2.6714\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2387 - val_loss: 2.6046\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2645 - val_loss: 2.6030\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2433 - val_loss: 2.5805\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2612 - val_loss: 2.6504\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2444 - val_loss: 2.7546\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2653 - val_loss: 2.6186\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2285 - val_loss: 2.6549\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2510 - val_loss: 2.7884\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2638 - val_loss: 2.7285\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2558 - val_loss: 2.6408\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2350 - val_loss: 2.6474\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2254 - val_loss: 2.7187\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2450 - val_loss: 2.5841\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2390 - val_loss: 2.7600\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2511 - val_loss: 2.6103\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2550 - val_loss: 2.6859\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2552 - val_loss: 2.6365\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2311 - val_loss: 2.5936\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2144 - val_loss: 2.6163\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2173 - val_loss: 2.5844\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2412 - val_loss: 2.6752\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2373 - val_loss: 2.6278\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2117 - val_loss: 2.7052\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2693 - val_loss: 2.6131\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2125 - val_loss: 2.8286\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2177 - val_loss: 2.6479\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1970 - val_loss: 2.6332\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.2342 - val_loss: 2.7003\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2215 - val_loss: 2.6073\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1955 - val_loss: 2.6852\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2282 - val_loss: 2.6261\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2211 - val_loss: 2.6903\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2205 - val_loss: 2.6599\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2204 - val_loss: 2.7197\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2152 - val_loss: 2.7781\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2593 - val_loss: 2.7742\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2467 - val_loss: 2.6130\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1994 - val_loss: 2.6535\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.2371 - val_loss: 2.6992\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2075 - val_loss: 2.6379\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2513 - val_loss: 2.8307\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2668 - val_loss: 2.5909\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1899 - val_loss: 2.6970\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2157 - val_loss: 2.6899\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1947 - val_loss: 2.7547\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2282 - val_loss: 2.6616\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1900 - val_loss: 2.6042\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2022 - val_loss: 2.6572\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2296 - val_loss: 2.6122\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1852 - val_loss: 2.6363\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1927 - val_loss: 2.7130\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2122 - val_loss: 2.7544\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2269 - val_loss: 2.7531\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1927 - val_loss: 2.5881\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2019 - val_loss: 2.7288\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1981 - val_loss: 2.6482\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2001 - val_loss: 2.7532\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1917 - val_loss: 2.7700\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1878 - val_loss: 2.7497\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2092 - val_loss: 2.6950\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1686 - val_loss: 2.8608\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.2167 - val_loss: 2.6911\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 2.1802 - val_loss: 2.7125\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.2019 - val_loss: 2.6739\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.2095 - val_loss: 2.6421\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.2016 - val_loss: 2.6849\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.1851 - val_loss: 2.6595\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.1608 - val_loss: 2.6954\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.1824 - val_loss: 2.7761\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 2.1895 - val_loss: 2.5781\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.1915 - val_loss: 2.6649\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 2.1715 - val_loss: 2.5774\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.2477 - val_loss: 2.7311\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.1759 - val_loss: 2.6684\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2092 - val_loss: 2.7038\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.1855 - val_loss: 2.6169\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2002 - val_loss: 2.6044\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1484 - val_loss: 2.5612\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1949 - val_loss: 2.6098\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1409 - val_loss: 2.6248\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1825 - val_loss: 2.7751\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2011 - val_loss: 2.6184\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.1903 - val_loss: 2.7216\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1704 - val_loss: 2.7774\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.1842 - val_loss: 2.6715\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1461 - val_loss: 2.6271\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1801 - val_loss: 2.6439\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1409 - val_loss: 2.6597\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1842 - val_loss: 2.7872\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1708 - val_loss: 2.5919\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1575 - val_loss: 2.7279\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1779 - val_loss: 2.6488\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1489 - val_loss: 2.6292\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1793 - val_loss: 2.7367\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1473 - val_loss: 2.7138\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.1703 - val_loss: 2.6543\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1835 - val_loss: 2.7402\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1385 - val_loss: 2.5796\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2013 - val_loss: 2.7684\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1833 - val_loss: 2.7647\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1800 - val_loss: 2.6130\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1686 - val_loss: 2.6948\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1255 - val_loss: 2.6721\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1591 - val_loss: 2.6699\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.1525 - val_loss: 2.6604\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1218 - val_loss: 2.7058\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1656 - val_loss: 2.6915\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1501 - val_loss: 2.7098\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1888 - val_loss: 2.7578\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1735 - val_loss: 2.8595\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1544 - val_loss: 2.8031\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1397 - val_loss: 2.6541\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1491 - val_loss: 2.6630\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1257 - val_loss: 2.6926\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1454 - val_loss: 2.7458\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1410 - val_loss: 2.8122\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1241 - val_loss: 2.7937\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1304 - val_loss: 2.6742\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1449 - val_loss: 2.8044\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1337 - val_loss: 2.6537\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1600 - val_loss: 2.7456\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1320 - val_loss: 2.7383\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1061 - val_loss: 2.6802\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1404 - val_loss: 2.7648\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.1242 - val_loss: 2.6165\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1097 - val_loss: 2.7278\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1212 - val_loss: 2.6418\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1547 - val_loss: 2.7120\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1346 - val_loss: 2.7435\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1715 - val_loss: 2.5977\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1462 - val_loss: 2.8497\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1105 - val_loss: 2.7185\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1253 - val_loss: 2.6216\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1319 - val_loss: 2.7585\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1454 - val_loss: 2.6372\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1219 - val_loss: 2.7540\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1128 - val_loss: 2.7162\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1023 - val_loss: 2.6052\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1698 - val_loss: 2.8759\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1507 - val_loss: 2.7585\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1010 - val_loss: 2.6824\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0965 - val_loss: 2.6981\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1101 - val_loss: 2.7986\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0940 - val_loss: 2.7570\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1034 - val_loss: 2.6736\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1116 - val_loss: 2.7166\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0964 - val_loss: 2.7999\n",
      "Epoch 463/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1309 - val_loss: 2.7121\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1403 - val_loss: 2.7506\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1337 - val_loss: 2.7576\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1434 - val_loss: 2.7109\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0951 - val_loss: 2.7592\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0690 - val_loss: 2.7445\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1468 - val_loss: 2.7443\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1139 - val_loss: 2.7760\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1609 - val_loss: 2.7678\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0796 - val_loss: 2.6702\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0645 - val_loss: 2.6511\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.0898 - val_loss: 2.5788\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0992 - val_loss: 2.7025\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0743 - val_loss: 2.6464\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1244 - val_loss: 2.7050\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0958 - val_loss: 2.7970\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0673 - val_loss: 2.6632\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0865 - val_loss: 2.6422\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0632 - val_loss: 2.7174\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0820 - val_loss: 2.6211\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0638 - val_loss: 2.7429\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1103 - val_loss: 2.7364\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0803 - val_loss: 2.7904\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0879 - val_loss: 2.7939\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1141 - val_loss: 2.6552\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0842 - val_loss: 2.6573\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0802 - val_loss: 2.6333\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1059 - val_loss: 2.7281\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0697 - val_loss: 2.7473\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0731 - val_loss: 2.7005\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0719 - val_loss: 2.7116\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0987 - val_loss: 2.8749\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0912 - val_loss: 2.7106\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0712 - val_loss: 2.9067\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1196 - val_loss: 2.7804\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0826 - val_loss: 2.7699\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0686 - val_loss: 2.8058\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.0743 - val_loss: 2.7502\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0563 - val_loss: 2.7032\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0599 - val_loss: 2.6992\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0961 - val_loss: 2.7962\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.0749 - val_loss: 2.6401\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0928 - val_loss: 2.7683\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0566 - val_loss: 2.7058\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0565 - val_loss: 2.7178\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0935 - val_loss: 2.7402\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0648 - val_loss: 2.7842\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0616 - val_loss: 2.7376\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0894 - val_loss: 2.7108\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0488 - val_loss: 2.7419\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0429 - val_loss: 2.6601\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0455 - val_loss: 2.8125\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0563 - val_loss: 2.8276\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0953 - val_loss: 2.6636\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0482 - val_loss: 2.7673\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0512 - val_loss: 2.7562\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.0528 - val_loss: 2.6485\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 2.0234 - val_loss: 2.7080\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 2.0441 - val_loss: 2.7511\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 2.0651 - val_loss: 2.6745\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 2.0322 - val_loss: 2.6896\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 2.0907 - val_loss: 2.6518\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0612 - val_loss: 2.6407\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0456 - val_loss: 2.7993\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0399 - val_loss: 2.8689\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0332 - val_loss: 2.7080\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0421 - val_loss: 2.6699\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0218 - val_loss: 2.6644\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0347 - val_loss: 2.6569\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.0230 - val_loss: 2.6361\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0400 - val_loss: 2.6365\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.0504 - val_loss: 2.6430\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0246 - val_loss: 2.6932\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0254 - val_loss: 2.7365\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0222 - val_loss: 2.7460\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0243 - val_loss: 2.8382\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0308 - val_loss: 2.6753\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.0126 - val_loss: 2.7171\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.0352 - val_loss: 2.7970\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0242 - val_loss: 2.8699\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0919 - val_loss: 2.6937\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.0143 - val_loss: 2.8352\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.0561 - val_loss: 2.7258\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.0421 - val_loss: 2.7401\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0090 - val_loss: 2.8378\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.0208 - val_loss: 2.6974\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0083 - val_loss: 2.8324\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0133 - val_loss: 2.7758\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0118 - val_loss: 2.7304\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0440 - val_loss: 2.6988\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0294 - val_loss: 2.7712\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0070 - val_loss: 2.6936\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0082 - val_loss: 2.8334\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0426 - val_loss: 2.6549\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0236 - val_loss: 2.7149\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0113 - val_loss: 2.7265\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9998 - val_loss: 2.7389\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0087 - val_loss: 2.7903\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0080 - val_loss: 2.6534\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0291 - val_loss: 2.8589\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0554 - val_loss: 2.7679\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.0298 - val_loss: 2.6995\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0123 - val_loss: 2.6342\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0165 - val_loss: 2.7216\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9924 - val_loss: 2.7750\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0363 - val_loss: 2.6499\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0035 - val_loss: 2.6653\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0200 - val_loss: 2.6941\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9901 - val_loss: 2.6517\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0411 - val_loss: 2.6895\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9976 - val_loss: 2.7376\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9858 - val_loss: 2.7468\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9934 - val_loss: 2.7060\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9999 - val_loss: 2.8926\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0192 - val_loss: 2.7976\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0110 - val_loss: 2.8083\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0323 - val_loss: 2.7574\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9955 - val_loss: 2.7056\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9716 - val_loss: 2.8274\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9998 - val_loss: 2.8137\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0017 - val_loss: 2.7040\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9838 - val_loss: 2.6879\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9799 - val_loss: 2.6791\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9955 - val_loss: 2.6738\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9657 - val_loss: 2.8395\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0261 - val_loss: 2.6965\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9830 - val_loss: 2.7857\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9887 - val_loss: 2.7311\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9982 - val_loss: 2.7427\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9778 - val_loss: 2.8411\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9919 - val_loss: 2.7483\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9804 - val_loss: 2.8931\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9805 - val_loss: 2.7035\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9713 - val_loss: 2.8591\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9546 - val_loss: 2.8416\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9751 - val_loss: 3.0241\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0092 - val_loss: 2.6213\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9709 - val_loss: 2.9791\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 788us/step - loss: 50.6842 - val_loss: 35.6078\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 24.7312 - val_loss: 16.0326\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 14.5134 - val_loss: 11.0091\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 9.6177 - val_loss: 8.5881\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 8.4403 - val_loss: 8.4135\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 8.1443 - val_loss: 8.0935\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 7.7611 - val_loss: 7.5980\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 7.2838 - val_loss: 7.1086\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 6.8020 - val_loss: 6.6208\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 6.3305 - val_loss: 6.0960\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 5.7854 - val_loss: 5.6402\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 5.2203 - val_loss: 5.0469\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 4.6434 - val_loss: 4.4132\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 4.2374 - val_loss: 4.0664\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.9556 - val_loss: 3.9139\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.7590 - val_loss: 3.7393\n",
      "Epoch 17/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.6074 - val_loss: 3.7418\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.4585 - val_loss: 3.5388\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.4076 - val_loss: 3.4866\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.2887 - val_loss: 3.4639\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.1980 - val_loss: 3.5107\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.1582 - val_loss: 3.7810\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.1694 - val_loss: 3.4256\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.0553 - val_loss: 3.3117\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.0174 - val_loss: 3.4425\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.0037 - val_loss: 3.2525\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.9548 - val_loss: 3.1915\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.9362 - val_loss: 3.4335\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.9169 - val_loss: 3.3241\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8850 - val_loss: 3.3275\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.9099 - val_loss: 3.3364\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8836 - val_loss: 3.3299\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8829 - val_loss: 3.2923\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8153 - val_loss: 3.1520\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.8280 - val_loss: 3.2070\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8266 - val_loss: 3.2523\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8045 - val_loss: 3.1334\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8440 - val_loss: 3.2183\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7875 - val_loss: 3.2173\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8169 - val_loss: 3.1595\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7761 - val_loss: 3.1108\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7578 - val_loss: 3.1005\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7385 - val_loss: 3.0943\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.7539 - val_loss: 3.0649\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7420 - val_loss: 3.0465\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.7087 - val_loss: 3.0621\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7530 - val_loss: 3.1057\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7063 - val_loss: 3.1248\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.6852 - val_loss: 3.1427\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.7163 - val_loss: 3.1827\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.7042 - val_loss: 3.1004\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.7289 - val_loss: 3.0947\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 2.7121 - val_loss: 3.1760\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7144 - val_loss: 3.1053\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.6796 - val_loss: 3.0307\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.6837 - val_loss: 2.9949\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6782 - val_loss: 3.1161\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6490 - val_loss: 3.0007\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.6783 - val_loss: 3.0442\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.6361 - val_loss: 3.0145\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.6774 - val_loss: 3.0515\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.7019 - val_loss: 3.2170\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.6320 - val_loss: 3.0127\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.6375 - val_loss: 3.0169\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.6250 - val_loss: 3.0803\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.6457 - val_loss: 3.1311\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5988 - val_loss: 2.9750\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5962 - val_loss: 3.0294\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6272 - val_loss: 3.1754\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6157 - val_loss: 3.0266\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5883 - val_loss: 3.1074\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.6224 - val_loss: 3.0316\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 2.5990 - val_loss: 3.0903\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.6344 - val_loss: 3.2751\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 2.6283 - val_loss: 3.1678\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 2.6097 - val_loss: 3.1040\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 2.5916 - val_loss: 3.1661\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 2.6003 - val_loss: 3.2285\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 2.5934 - val_loss: 3.0028\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 2.5712 - val_loss: 3.1397\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 2.5614 - val_loss: 3.1177\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.5627 - val_loss: 3.1058\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.5923 - val_loss: 3.3288\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 2.5965 - val_loss: 3.2619\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 65us/step - loss: 2.6244 - val_loss: 3.0854\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 2.5669 - val_loss: 3.0945\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.5575 - val_loss: 3.0072\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.5477 - val_loss: 3.1711\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5654 - val_loss: 3.0376\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5127 - val_loss: 3.0251\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5869 - val_loss: 3.0375\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5426 - val_loss: 2.9557\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.5034 - val_loss: 3.0230\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5364 - val_loss: 2.9699\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5579 - val_loss: 3.0508\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5487 - val_loss: 3.0772\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5538 - val_loss: 2.9760\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5121 - val_loss: 3.2060\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5468 - val_loss: 3.0979\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5038 - val_loss: 3.0167\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5782 - val_loss: 2.9564\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.5205 - val_loss: 3.0964\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5186 - val_loss: 3.0648\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4882 - val_loss: 3.2191\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5498 - val_loss: 3.0070\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5391 - val_loss: 3.0991\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5328 - val_loss: 2.9669\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4919 - val_loss: 3.0121\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5059 - val_loss: 3.1772\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5492 - val_loss: 3.0331\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5169 - val_loss: 3.0241\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5217 - val_loss: 3.0088\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4949 - val_loss: 2.9928\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4727 - val_loss: 2.9018\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5126 - val_loss: 3.0742\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4853 - val_loss: 3.0556\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.5026 - val_loss: 2.9903\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4966 - val_loss: 3.1076\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4805 - val_loss: 3.0076\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4847 - val_loss: 2.9238\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4774 - val_loss: 2.9894\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4559 - val_loss: 3.0425\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4822 - val_loss: 2.9508\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4554 - val_loss: 3.0548\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.5107 - val_loss: 3.0594\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4996 - val_loss: 3.0127\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4502 - val_loss: 3.0542\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4587 - val_loss: 3.0771\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4879 - val_loss: 3.0806\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.4930 - val_loss: 2.9915\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.4890 - val_loss: 3.1116\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.4514 - val_loss: 2.9747\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4376 - val_loss: 2.9500\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4479 - val_loss: 3.0541\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4390 - val_loss: 2.9886\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.4360 - val_loss: 3.0035\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4301 - val_loss: 2.9544\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4585 - val_loss: 2.9888\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4607 - val_loss: 2.9769\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4669 - val_loss: 3.0798\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4519 - val_loss: 2.9296\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4437 - val_loss: 2.9941\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4432 - val_loss: 3.1051\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4317 - val_loss: 2.9818\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4106 - val_loss: 3.0689\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4532 - val_loss: 3.0886\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4368 - val_loss: 3.0573\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.4115 - val_loss: 2.9762\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4313 - val_loss: 3.1104\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4279 - val_loss: 2.9804\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4304 - val_loss: 2.9842\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4000 - val_loss: 3.2672\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4273 - val_loss: 3.0268\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3958 - val_loss: 2.9621\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4283 - val_loss: 3.0038\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3959 - val_loss: 3.0621\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4246 - val_loss: 2.9518\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 2.4138 - val_loss: 2.9997\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.4000 - val_loss: 3.0158\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.4168 - val_loss: 2.9482\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.4210 - val_loss: 2.9412\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.3891 - val_loss: 3.0111\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.4011 - val_loss: 3.0469\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 2.3760 - val_loss: 3.0552\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.4516 - val_loss: 2.9932\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.3916 - val_loss: 2.9640\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 2.4218 - val_loss: 2.9256\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.3939 - val_loss: 2.9451\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.3981 - val_loss: 2.9227\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.3757 - val_loss: 3.1696\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4105 - val_loss: 2.9860\n",
      "Epoch 172/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3746 - val_loss: 3.0161\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.3896 - val_loss: 2.9960\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.4036 - val_loss: 3.0360\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3903 - val_loss: 2.9902\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3831 - val_loss: 3.0273\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3999 - val_loss: 2.9238\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3905 - val_loss: 2.9557\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3662 - val_loss: 2.9884\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4076 - val_loss: 2.9527\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3735 - val_loss: 3.0978\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3769 - val_loss: 3.0438\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.3707 - val_loss: 2.9833\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3781 - val_loss: 3.0877\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3617 - val_loss: 3.0152\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3727 - val_loss: 2.9606\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3787 - val_loss: 3.1366\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.3621 - val_loss: 3.0046\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3541 - val_loss: 3.0391\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3928 - val_loss: 3.0188\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3745 - val_loss: 3.0034\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.3625 - val_loss: 3.0644\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3562 - val_loss: 2.9448\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3428 - val_loss: 3.0058\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3544 - val_loss: 2.9195\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3558 - val_loss: 2.9997\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.3289 - val_loss: 3.0035\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.3843 - val_loss: 2.9220\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.3486 - val_loss: 2.9941\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 2.3459 - val_loss: 2.9028\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3269 - val_loss: 2.9608\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3253 - val_loss: 3.1869\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4259 - val_loss: 2.9295\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3537 - val_loss: 2.9588\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3318 - val_loss: 2.9769\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3450 - val_loss: 2.9682\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.3486 - val_loss: 2.9454\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3443 - val_loss: 3.0465\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3338 - val_loss: 3.1578\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3720 - val_loss: 2.9695\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.3253 - val_loss: 3.0460\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3282 - val_loss: 2.9942\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.3257 - val_loss: 2.9780\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.3148 - val_loss: 2.9081\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.3372 - val_loss: 2.9318\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.3190 - val_loss: 2.9299\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3289 - val_loss: 3.0189\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.3188 - val_loss: 3.0376\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.3005 - val_loss: 2.9516\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.3266 - val_loss: 2.9653\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3148 - val_loss: 2.9342\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3073 - val_loss: 3.0597\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2914 - val_loss: 2.9577\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3137 - val_loss: 3.0293\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3008 - val_loss: 2.9485\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3072 - val_loss: 2.9442\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2835 - val_loss: 2.9187\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2813 - val_loss: 2.9678\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3042 - val_loss: 2.9932\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2821 - val_loss: 3.0221\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2813 - val_loss: 2.9788\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2929 - val_loss: 2.9404\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3016 - val_loss: 3.0691\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3226 - val_loss: 2.9439\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2778 - val_loss: 2.9117\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2796 - val_loss: 3.0097\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3070 - val_loss: 2.9958\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2830 - val_loss: 2.9272\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2629 - val_loss: 2.9659\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.2790 - val_loss: 2.8776\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2685 - val_loss: 3.0168\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.2953 - val_loss: 2.9298\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3305 - val_loss: 3.0362\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2911 - val_loss: 2.9889\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2837 - val_loss: 3.0199\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2769 - val_loss: 3.0045\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2710 - val_loss: 3.0271\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2872 - val_loss: 2.8945\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2440 - val_loss: 2.9117\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2630 - val_loss: 2.9595\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2487 - val_loss: 2.9364\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2467 - val_loss: 2.9305\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2478 - val_loss: 3.0470\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2547 - val_loss: 3.0082\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2305 - val_loss: 2.9966\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2627 - val_loss: 3.0037\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2720 - val_loss: 2.9902\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2834 - val_loss: 2.9185\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2895 - val_loss: 2.9211\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2728 - val_loss: 3.0922\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2391 - val_loss: 3.0318\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2601 - val_loss: 3.0224\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2871 - val_loss: 2.9207\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2424 - val_loss: 2.9553\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2612 - val_loss: 2.8771\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2203 - val_loss: 2.9818\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2177 - val_loss: 2.9139\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2187 - val_loss: 2.9055\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2606 - val_loss: 2.9753\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2315 - val_loss: 3.0473\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2480 - val_loss: 2.9459\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2008 - val_loss: 2.9228\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2261 - val_loss: 2.9470\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2172 - val_loss: 2.9398\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2528 - val_loss: 2.9200\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2188 - val_loss: 2.9902\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2333 - val_loss: 3.0894\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2118 - val_loss: 2.9601\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2243 - val_loss: 2.9599\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2374 - val_loss: 2.8861\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2171 - val_loss: 2.9861\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2809 - val_loss: 2.8913\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2284 - val_loss: 3.0203\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2502 - val_loss: 2.9771\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2198 - val_loss: 3.0149\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2386 - val_loss: 2.9629\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2089 - val_loss: 2.9898\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2023 - val_loss: 2.8997\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1986 - val_loss: 3.0756\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2453 - val_loss: 2.9280\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2180 - val_loss: 2.9105\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2283 - val_loss: 2.9209\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1794 - val_loss: 3.1005\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2670 - val_loss: 2.9661\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1888 - val_loss: 2.9464\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1886 - val_loss: 3.0880\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2034 - val_loss: 2.9091\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1986 - val_loss: 2.9468\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2076 - val_loss: 2.9851\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1704 - val_loss: 2.9361\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1976 - val_loss: 2.9328\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1767 - val_loss: 2.9642\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2001 - val_loss: 2.8897\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1861 - val_loss: 2.9167\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1747 - val_loss: 2.9358\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2270 - val_loss: 3.0058\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1845 - val_loss: 3.1229\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1890 - val_loss: 2.9411\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1823 - val_loss: 2.9734\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1673 - val_loss: 2.9382\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1778 - val_loss: 2.9490\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1846 - val_loss: 2.9053\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.1821 - val_loss: 3.1188\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1908 - val_loss: 2.9630\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1839 - val_loss: 3.0249\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1815 - val_loss: 2.9456\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1636 - val_loss: 2.9262\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1578 - val_loss: 2.9468\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1739 - val_loss: 2.9208\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1703 - val_loss: 2.9087\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1453 - val_loss: 3.0800\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1975 - val_loss: 3.0578\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1675 - val_loss: 3.0018\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1595 - val_loss: 2.8933\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1634 - val_loss: 2.9294\n",
      "Epoch 326/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1332 - val_loss: 3.0353\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1553 - val_loss: 3.0164\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1662 - val_loss: 3.0858\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1677 - val_loss: 3.0192\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1577 - val_loss: 2.9115\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1584 - val_loss: 2.9530\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1349 - val_loss: 2.9160\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1546 - val_loss: 2.9833\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1584 - val_loss: 2.9427\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1340 - val_loss: 2.9367\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1391 - val_loss: 3.0125\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1689 - val_loss: 3.0026\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1384 - val_loss: 2.9223\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1468 - val_loss: 2.9144\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1231 - val_loss: 3.0364\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1627 - val_loss: 3.0544\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1609 - val_loss: 2.9898\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1221 - val_loss: 2.9187\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1268 - val_loss: 3.0045\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1244 - val_loss: 2.9200\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1246 - val_loss: 2.9791\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1545 - val_loss: 3.0205\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1570 - val_loss: 3.0884\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1287 - val_loss: 2.9264\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1319 - val_loss: 3.0388\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1273 - val_loss: 3.0021\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1315 - val_loss: 3.0763\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1696 - val_loss: 3.0444\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1266 - val_loss: 2.9879\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1464 - val_loss: 2.9655\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1197 - val_loss: 3.0603\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1386 - val_loss: 3.1953\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1763 - val_loss: 3.0457\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1172 - val_loss: 3.0047\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1150 - val_loss: 3.0126\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1764 - val_loss: 3.0238\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1341 - val_loss: 2.9790\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0982 - val_loss: 2.9520\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1187 - val_loss: 2.9392\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1353 - val_loss: 2.9976\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0919 - val_loss: 2.9887\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1137 - val_loss: 2.9546\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0835 - val_loss: 3.0044\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.1087 - val_loss: 2.9889\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.1225 - val_loss: 2.9263\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 2.1148 - val_loss: 2.9345\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.1141 - val_loss: 3.0204\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.1247 - val_loss: 2.9139\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.1155 - val_loss: 2.9149\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1255 - val_loss: 3.0098\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0942 - val_loss: 3.0074\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0776 - val_loss: 2.9602\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0970 - val_loss: 3.0327\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1096 - val_loss: 3.0186\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1105 - val_loss: 2.9840\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0791 - val_loss: 2.9632\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.1045 - val_loss: 3.0532\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0776 - val_loss: 2.9891\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0873 - val_loss: 3.0199\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1651 - val_loss: 2.9783\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.0697 - val_loss: 3.0652\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0707 - val_loss: 3.0497\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.0992 - val_loss: 2.9309\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.0743 - val_loss: 3.0290\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.1243 - val_loss: 3.0098\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0752 - val_loss: 3.1634\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1230 - val_loss: 2.9338\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0977 - val_loss: 3.0150\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0985 - val_loss: 2.9597\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0753 - val_loss: 3.0099\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0565 - val_loss: 3.0324\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.1045 - val_loss: 3.0299\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0914 - val_loss: 2.9860\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0534 - val_loss: 3.0357\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0819 - val_loss: 3.0119\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1032 - val_loss: 2.9543\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0612 - val_loss: 3.0331\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0771 - val_loss: 2.9435\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.0733 - val_loss: 2.9856\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0663 - val_loss: 3.0109\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0687 - val_loss: 3.0729\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0521 - val_loss: 2.9423\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0429 - val_loss: 2.9326\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0501 - val_loss: 3.1013\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0785 - val_loss: 2.9657\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0962 - val_loss: 3.0700\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0739 - val_loss: 3.0100\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0626 - val_loss: 3.0569\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0719 - val_loss: 3.1231\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0739 - val_loss: 3.0518\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0518 - val_loss: 2.9685\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0700 - val_loss: 3.0047\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0363 - val_loss: 3.0015\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0459 - val_loss: 3.0196\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0555 - val_loss: 2.9442\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0489 - val_loss: 3.0571\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0560 - val_loss: 3.0736\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0424 - val_loss: 3.0696\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0484 - val_loss: 2.9849\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0286 - val_loss: 3.0517\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0213 - val_loss: 3.0000\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0437 - val_loss: 2.9581\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0516 - val_loss: 3.0516\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0144 - val_loss: 3.0439\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0525 - val_loss: 2.9582\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0447 - val_loss: 3.0198\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0440 - val_loss: 3.0789\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0519 - val_loss: 3.1216\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0634 - val_loss: 2.9542\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0405 - val_loss: 2.9885\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0599 - val_loss: 3.0085\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0138 - val_loss: 3.0795\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0212 - val_loss: 3.0423\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0182 - val_loss: 2.9727\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 2.0344 - val_loss: 3.0751\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0293 - val_loss: 2.9793\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0400 - val_loss: 3.1263\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0247 - val_loss: 2.9958\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0528 - val_loss: 3.0052\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0135 - val_loss: 2.9961\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0258 - val_loss: 2.9429\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0768 - val_loss: 3.0181\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0058 - val_loss: 3.0354\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0619 - val_loss: 3.0740\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0162 - val_loss: 2.9511\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0163 - val_loss: 3.0395\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0074 - val_loss: 3.0880\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0319 - val_loss: 3.1297\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0613 - val_loss: 3.0171\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0084 - val_loss: 2.9709\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0119 - val_loss: 3.1681\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0028 - val_loss: 3.0006\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0252 - val_loss: 3.0856\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0191 - val_loss: 2.9667\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0085 - val_loss: 2.9957\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9842 - val_loss: 3.0339\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9808 - val_loss: 3.0387\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9998 - val_loss: 2.9618\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0148 - val_loss: 3.0332\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9940 - val_loss: 3.0867\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9918 - val_loss: 3.1175\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0091 - val_loss: 3.0283\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0252 - val_loss: 3.1671\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9821 - val_loss: 3.1191\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0026 - val_loss: 2.9923\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0237 - val_loss: 3.0069\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9858 - val_loss: 3.0105\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9798 - val_loss: 2.9930\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0538 - val_loss: 3.0145\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9787 - val_loss: 3.0517\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9983 - val_loss: 3.0369\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9851 - val_loss: 3.0316\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9851 - val_loss: 2.9981\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9989 - val_loss: 3.0686\n",
      "Epoch 480/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9803 - val_loss: 3.0373\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9997 - val_loss: 2.9749\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9864 - val_loss: 3.0585\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9908 - val_loss: 3.2164\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0056 - val_loss: 3.1118\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9998 - val_loss: 3.1182\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0247 - val_loss: 3.0106\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9866 - val_loss: 3.0204\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9930 - val_loss: 3.0605\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9689 - val_loss: 3.0321\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9676 - val_loss: 2.9934\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0044 - val_loss: 3.0397\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0013 - val_loss: 3.0436\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 1.9787 - val_loss: 3.0376\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9899 - val_loss: 3.0822\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9733 - val_loss: 3.0768\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9766 - val_loss: 2.9856\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 1.9886 - val_loss: 3.0251\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9829 - val_loss: 3.1307\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9608 - val_loss: 3.2019\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0234 - val_loss: 3.1334\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9575 - val_loss: 3.0132\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9452 - val_loss: 3.1768\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9843 - val_loss: 2.9904\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9592 - val_loss: 3.1885\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9951 - val_loss: 3.0912\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9550 - val_loss: 3.0627\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9444 - val_loss: 3.0039\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9490 - val_loss: 3.0681\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9750 - val_loss: 3.0896\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9632 - val_loss: 3.0407\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9891 - val_loss: 3.0357\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 1.9721 - val_loss: 3.1069\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 1.9747 - val_loss: 3.1768\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9721 - val_loss: 3.1044\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9526 - val_loss: 3.0377\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 1.9360 - val_loss: 3.0758\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 1.9419 - val_loss: 3.1043\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9416 - val_loss: 2.9936\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9359 - val_loss: 3.1929\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 1.9520 - val_loss: 3.0230\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9432 - val_loss: 3.0497\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 1.9714 - val_loss: 3.0049\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9390 - val_loss: 3.0067\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9634 - val_loss: 3.1514\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9313 - val_loss: 3.0336\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9162 - val_loss: 3.0609\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9672 - val_loss: 3.1059\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9474 - val_loss: 3.0752\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9222 - val_loss: 3.0463\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 1.9405 - val_loss: 2.9940\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9886 - val_loss: 3.1312\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8959 - val_loss: 3.0607\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9329 - val_loss: 3.0645\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9500 - val_loss: 3.1134\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9566 - val_loss: 3.0376\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9270 - val_loss: 3.0810\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9224 - val_loss: 3.0056\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9402 - val_loss: 3.0305\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9172 - val_loss: 3.0141\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9311 - val_loss: 3.0249\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9165 - val_loss: 3.1909\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 1.9590 - val_loss: 3.0336\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9328 - val_loss: 3.0627\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9306 - val_loss: 3.0653\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9409 - val_loss: 3.0699\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9050 - val_loss: 3.0572\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 1.9113 - val_loss: 3.0745\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8943 - val_loss: 3.1614\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9437 - val_loss: 3.1090\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 1.9567 - val_loss: 3.1275\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 1.9142 - val_loss: 3.1172\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9162 - val_loss: 3.1204\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 1.9499 - val_loss: 3.1771\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 1.9065 - val_loss: 3.0603\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9294 - val_loss: 3.1628\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 1.9145 - val_loss: 3.0306\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 1.8918 - val_loss: 3.0457\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 1.9450 - val_loss: 3.0600\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 1.9154 - val_loss: 3.0601\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.8931 - val_loss: 3.2270\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9150 - val_loss: 3.0803\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9176 - val_loss: 3.0519\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.8757 - val_loss: 3.0109\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 1.9424 - val_loss: 3.0213\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.8843 - val_loss: 3.1105\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9096 - val_loss: 3.0931\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.8910 - val_loss: 3.1255\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.8957 - val_loss: 3.0747\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9094 - val_loss: 3.1691\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9007 - val_loss: 3.0433\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.8948 - val_loss: 3.1595\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9277 - val_loss: 3.0416\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9259 - val_loss: 3.0186\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.8798 - val_loss: 3.0999\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.8961 - val_loss: 3.0268\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.8865 - val_loss: 3.0941\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8715 - val_loss: 3.0488\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.8832 - val_loss: 3.0813\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8968 - val_loss: 3.1187\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.8898 - val_loss: 3.0314\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9107 - val_loss: 3.1167\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8791 - val_loss: 3.0778\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8769 - val_loss: 3.1329\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8769 - val_loss: 3.0826\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.8895 - val_loss: 3.0493\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8825 - val_loss: 3.1532\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8880 - val_loss: 3.1934\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9036 - val_loss: 3.0955\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9105 - val_loss: 3.1022\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.8717 - val_loss: 3.1712\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9543 - val_loss: 3.1084\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9060 - val_loss: 3.0891\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8958 - val_loss: 3.1014\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9330 - val_loss: 3.0942\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.8805 - val_loss: 3.0612\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8756 - val_loss: 3.0571\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8602 - val_loss: 3.0874\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8532 - val_loss: 3.1107\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8596 - val_loss: 3.1040\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8800 - val_loss: 3.1793\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 882us/step - loss: 49.0807 - val_loss: 32.9186\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 24.5326 - val_loss: 17.3453\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 15.3852 - val_loss: 12.2713\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 10.5817 - val_loss: 8.4577\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 8.7533 - val_loss: 8.1176\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 8.4369 - val_loss: 7.7602\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 7.9751 - val_loss: 7.2810\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 7.5180 - val_loss: 6.8831\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 7.0709 - val_loss: 6.4761\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 6.6212 - val_loss: 6.1409\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 6.1697 - val_loss: 5.6517\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 5.7077 - val_loss: 5.1836\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 5.2047 - val_loss: 4.6778\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 4.6835 - val_loss: 4.3174\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 4.2770 - val_loss: 3.8636\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.9577 - val_loss: 3.6139\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.7564 - val_loss: 3.4789\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 3.5718 - val_loss: 3.4112\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.5317 - val_loss: 3.2218\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.3592 - val_loss: 3.3047\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.3557 - val_loss: 3.2406\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.2609 - val_loss: 3.2184\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.1999 - val_loss: 3.2073\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.1683 - val_loss: 3.1269\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.1389 - val_loss: 3.2116\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.1271 - val_loss: 3.1102\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.1223 - val_loss: 3.0746\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.0919 - val_loss: 2.9241\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.0675 - val_loss: 3.0048\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.0523 - val_loss: 2.9442\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.0383 - val_loss: 2.8900\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.9745 - val_loss: 3.2027\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 3.0008 - val_loss: 3.1554\n",
      "Epoch 34/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.9691 - val_loss: 2.8836\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.9624 - val_loss: 2.8330\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.9782 - val_loss: 2.8060\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.9265 - val_loss: 2.8228\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8937 - val_loss: 2.8006\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.8949 - val_loss: 2.8649\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.8931 - val_loss: 2.9915\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.8775 - val_loss: 2.8716\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 2.8750 - val_loss: 2.8813\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.8695 - val_loss: 2.7553\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 72us/step - loss: 2.8328 - val_loss: 2.8630\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.8469 - val_loss: 2.9711\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.8393 - val_loss: 2.7436\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.8025 - val_loss: 2.7652\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8583 - val_loss: 2.7986\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8083 - val_loss: 2.8287\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.8304 - val_loss: 2.7619\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8098 - val_loss: 2.8270\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.7756 - val_loss: 2.7597\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.8179 - val_loss: 2.7399\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7993 - val_loss: 2.8663\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.8036 - val_loss: 2.6900\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7317 - val_loss: 2.6973\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7706 - val_loss: 2.7502\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7495 - val_loss: 2.6761\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7359 - val_loss: 2.7022\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.7555 - val_loss: 2.7383\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.7266 - val_loss: 2.7567\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.7312 - val_loss: 2.6691\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.7471 - val_loss: 2.6651\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.7011 - val_loss: 2.6264\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.7248 - val_loss: 2.6454\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.7440 - val_loss: 2.9760\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.7507 - val_loss: 2.6137\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.6871 - val_loss: 2.7232\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6889 - val_loss: 2.7070\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6935 - val_loss: 2.7400\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7304 - val_loss: 2.6234\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.6936 - val_loss: 2.6987\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.6767 - val_loss: 2.6331\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6837 - val_loss: 2.6117\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.6867 - val_loss: 2.6166\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7074 - val_loss: 2.6363\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.6629 - val_loss: 2.7491\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6705 - val_loss: 2.6825\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.6941 - val_loss: 2.7079\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.6824 - val_loss: 2.6872\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.6865 - val_loss: 2.7341\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6375 - val_loss: 2.9116\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6678 - val_loss: 2.6884\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6228 - val_loss: 2.6792\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6296 - val_loss: 2.7272\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6495 - val_loss: 2.6305\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6235 - val_loss: 2.6121\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6527 - val_loss: 2.6331\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6237 - val_loss: 2.6098\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6503 - val_loss: 2.6651\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6560 - val_loss: 2.6179\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6022 - val_loss: 2.6652\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6369 - val_loss: 2.5881\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5818 - val_loss: 2.6894\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6037 - val_loss: 2.6198\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6346 - val_loss: 2.6940\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6033 - val_loss: 2.6439\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6406 - val_loss: 2.7055\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6463 - val_loss: 2.6162\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5928 - val_loss: 2.6038\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5999 - val_loss: 2.8972\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6367 - val_loss: 2.6568\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5868 - val_loss: 2.6387\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.5838 - val_loss: 2.6266\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5955 - val_loss: 2.6324\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.5816 - val_loss: 2.7819\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.6507 - val_loss: 2.6178\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5884 - val_loss: 2.5919\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5499 - val_loss: 2.6121\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5594 - val_loss: 2.5976\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.5786 - val_loss: 2.6323\n",
      "Epoch 112/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5982 - val_loss: 2.6256\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5886 - val_loss: 2.5664\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.5793 - val_loss: 2.6955\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5784 - val_loss: 2.5852\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5607 - val_loss: 2.5921\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5503 - val_loss: 2.5625\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.5468 - val_loss: 2.6184\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5989 - val_loss: 2.5621\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5560 - val_loss: 2.5765\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5652 - val_loss: 2.6171\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.5521 - val_loss: 2.6364\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5486 - val_loss: 2.6695\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5540 - val_loss: 2.5794\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5382 - val_loss: 2.6163\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5512 - val_loss: 2.5853\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.5425 - val_loss: 2.6268\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5286 - val_loss: 2.7293\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5578 - val_loss: 2.6029\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5497 - val_loss: 2.6738\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5312 - val_loss: 2.5605\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5014 - val_loss: 2.6210\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5361 - val_loss: 2.6405\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5259 - val_loss: 2.5818\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4922 - val_loss: 2.5812\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5039 - val_loss: 2.6035\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5090 - val_loss: 2.6104\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4957 - val_loss: 2.5992\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5433 - val_loss: 2.6910\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5094 - val_loss: 2.6550\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5121 - val_loss: 2.6396\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5098 - val_loss: 2.5972\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5186 - val_loss: 2.6352\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5062 - val_loss: 2.5956\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5026 - val_loss: 2.5956\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4751 - val_loss: 2.5859\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4858 - val_loss: 2.5742\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4818 - val_loss: 2.5820\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4701 - val_loss: 2.5833\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4704 - val_loss: 2.6419\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4680 - val_loss: 2.6882\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5141 - val_loss: 2.6121\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.4635 - val_loss: 2.5676\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4626 - val_loss: 2.5991\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4533 - val_loss: 2.6057\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4665 - val_loss: 2.5917\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4547 - val_loss: 2.7294\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4774 - val_loss: 2.7454\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5017 - val_loss: 2.6104\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4731 - val_loss: 2.5777\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4663 - val_loss: 2.5822\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4693 - val_loss: 2.6571\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4775 - val_loss: 2.7697\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4682 - val_loss: 2.7159\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4550 - val_loss: 2.5958\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4661 - val_loss: 2.5788\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4584 - val_loss: 2.5736\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4467 - val_loss: 2.6691\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4507 - val_loss: 2.5673\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4764 - val_loss: 2.6105\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4521 - val_loss: 2.6176\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4586 - val_loss: 2.5844\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4284 - val_loss: 2.5574\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4384 - val_loss: 2.5898\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4383 - val_loss: 2.6580\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4407 - val_loss: 2.5715\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4213 - val_loss: 2.5689\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4416 - val_loss: 2.6109\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4336 - val_loss: 2.5863\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4271 - val_loss: 2.5734\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4230 - val_loss: 2.5817\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4444 - val_loss: 2.5621\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4072 - val_loss: 2.5627\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4076 - val_loss: 2.5695\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4072 - val_loss: 2.6046\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4147 - val_loss: 2.5888\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4313 - val_loss: 2.5853\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4013 - val_loss: 2.5485\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4455 - val_loss: 2.5807\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4269 - val_loss: 2.5609\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4145 - val_loss: 2.6778\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4506 - val_loss: 2.6061\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 44us/step - loss: 2.3946 - val_loss: 2.5957\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 2.4318 - val_loss: 2.6428\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.3860 - val_loss: 2.5558\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4289 - val_loss: 2.7177\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4156 - val_loss: 2.5618\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4202 - val_loss: 2.5669\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3961 - val_loss: 2.5590\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3984 - val_loss: 2.6381\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3986 - val_loss: 2.5931\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.4082 - val_loss: 2.6180\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4221 - val_loss: 2.5839\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3993 - val_loss: 2.5966\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3726 - val_loss: 2.6754\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3970 - val_loss: 2.5682\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4072 - val_loss: 2.6017\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4061 - val_loss: 2.6163\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3943 - val_loss: 2.6148\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3714 - val_loss: 2.5504\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3853 - val_loss: 2.6656\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3964 - val_loss: 2.5599\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4009 - val_loss: 2.5552\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3750 - val_loss: 2.6044\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4121 - val_loss: 2.5764\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4069 - val_loss: 2.5793\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3986 - val_loss: 2.6181\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3639 - val_loss: 2.5560\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3618 - val_loss: 2.6087\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3653 - val_loss: 2.5477\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3553 - val_loss: 2.5519\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3485 - val_loss: 2.5879\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3761 - val_loss: 2.5490\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3806 - val_loss: 2.5844\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3451 - val_loss: 2.5415\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3377 - val_loss: 2.5703\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3491 - val_loss: 2.5585\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3403 - val_loss: 2.5940\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3500 - val_loss: 2.5617\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3481 - val_loss: 2.5757\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.3243 - val_loss: 2.5831\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3751 - val_loss: 2.5594\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3446 - val_loss: 2.5550\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3606 - val_loss: 2.6596\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3572 - val_loss: 2.5974\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3519 - val_loss: 2.6234\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3531 - val_loss: 2.5901\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3388 - val_loss: 2.6000\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3777 - val_loss: 2.5928\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3465 - val_loss: 2.6412\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3585 - val_loss: 2.6026\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3230 - val_loss: 2.6102\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3189 - val_loss: 2.5908\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3412 - val_loss: 2.6642\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3261 - val_loss: 2.6439\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3213 - val_loss: 2.5808\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3496 - val_loss: 2.6332\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3269 - val_loss: 2.6102\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3356 - val_loss: 2.5859\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3166 - val_loss: 2.6014\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3421 - val_loss: 2.6120\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3205 - val_loss: 2.6120\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3516 - val_loss: 2.6355\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3160 - val_loss: 2.6657\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3681 - val_loss: 2.5783\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2944 - val_loss: 2.6522\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3289 - val_loss: 2.5742\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2961 - val_loss: 2.6281\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3298 - val_loss: 2.6354\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3213 - val_loss: 2.5496\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3226 - val_loss: 2.5709\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3087 - val_loss: 2.5440\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3093 - val_loss: 2.5850\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3054 - val_loss: 2.6059\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2945 - val_loss: 2.6066\n",
      "Epoch 266/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2958 - val_loss: 2.5988\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2843 - val_loss: 2.6046\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3087 - val_loss: 2.6263\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2844 - val_loss: 2.6508\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3088 - val_loss: 2.5897\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2729 - val_loss: 2.6209\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2925 - val_loss: 2.5821\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3013 - val_loss: 2.5591\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2720 - val_loss: 2.5976\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3000 - val_loss: 2.6191\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2879 - val_loss: 2.6585\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3316 - val_loss: 2.5719\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2798 - val_loss: 2.6424\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2999 - val_loss: 2.5911\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2718 - val_loss: 2.7101\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2872 - val_loss: 2.5735\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2809 - val_loss: 2.6710\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3108 - val_loss: 2.6201\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2728 - val_loss: 2.6422\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2905 - val_loss: 2.6439\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2711 - val_loss: 2.6115\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2630 - val_loss: 2.6660\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3054 - val_loss: 2.6267\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2660 - val_loss: 2.5970\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2548 - val_loss: 2.6443\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2800 - val_loss: 2.6170\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2674 - val_loss: 2.6567\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2711 - val_loss: 2.5804\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2699 - val_loss: 2.6202\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2562 - val_loss: 2.6162\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2672 - val_loss: 2.5672\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2451 - val_loss: 2.6180\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2381 - val_loss: 2.5961\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2742 - val_loss: 2.6463\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2717 - val_loss: 2.5952\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2589 - val_loss: 2.6136\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2817 - val_loss: 2.6272\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2653 - val_loss: 2.7073\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2627 - val_loss: 2.6605\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2589 - val_loss: 2.6609\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2565 - val_loss: 2.6349\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2402 - val_loss: 2.5908\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2563 - val_loss: 2.6023\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2295 - val_loss: 2.6810\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2566 - val_loss: 2.5932\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2247 - val_loss: 2.6779\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2624 - val_loss: 2.6398\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2168 - val_loss: 2.6094\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2167 - val_loss: 2.6787\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2431 - val_loss: 2.6501\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2506 - val_loss: 2.7221\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2549 - val_loss: 2.6395\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2405 - val_loss: 2.5914\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2157 - val_loss: 2.6995\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2560 - val_loss: 2.6451\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2369 - val_loss: 2.6161\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2090 - val_loss: 2.6516\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2449 - val_loss: 2.7738\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2379 - val_loss: 2.6779\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2174 - val_loss: 2.5762\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2419 - val_loss: 2.7164\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2367 - val_loss: 2.6543\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2440 - val_loss: 2.6285\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2261 - val_loss: 2.6867\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2348 - val_loss: 2.7486\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2405 - val_loss: 2.6349\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2143 - val_loss: 2.6877\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2280 - val_loss: 2.6811\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2187 - val_loss: 2.6364\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1987 - val_loss: 2.6844\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2206 - val_loss: 2.6495\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2161 - val_loss: 2.6371\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2125 - val_loss: 2.6760\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2254 - val_loss: 2.6722\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2054 - val_loss: 2.6495\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1858 - val_loss: 2.5804\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1850 - val_loss: 2.6035\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1953 - val_loss: 2.6574\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2086 - val_loss: 2.6588\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2010 - val_loss: 2.7167\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2263 - val_loss: 2.6593\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1905 - val_loss: 2.6516\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1983 - val_loss: 2.6344\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1908 - val_loss: 2.6827\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1938 - val_loss: 2.6827\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2050 - val_loss: 2.6695\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2033 - val_loss: 2.6531\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1742 - val_loss: 2.6512\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1893 - val_loss: 2.7233\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2169 - val_loss: 2.6931\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2121 - val_loss: 2.6878\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1853 - val_loss: 2.6323\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2061 - val_loss: 2.7374\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2085 - val_loss: 2.6825\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1939 - val_loss: 2.6499\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1731 - val_loss: 2.6289\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.1705 - val_loss: 2.6955\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.1720 - val_loss: 2.6701\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1629 - val_loss: 2.6292\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1455 - val_loss: 2.6969\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1752 - val_loss: 2.6303\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1530 - val_loss: 2.7030\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1716 - val_loss: 2.6440\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1980 - val_loss: 2.7296\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1748 - val_loss: 2.6853\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1400 - val_loss: 2.7292\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1505 - val_loss: 2.7280\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1591 - val_loss: 2.7079\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1669 - val_loss: 2.6955\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1692 - val_loss: 2.7490\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1921 - val_loss: 2.6985\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1518 - val_loss: 2.7559\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1590 - val_loss: 2.7224\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1615 - val_loss: 2.6578\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1574 - val_loss: 2.6952\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1391 - val_loss: 2.6829\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1630 - val_loss: 2.6922\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1386 - val_loss: 2.6756\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1197 - val_loss: 2.7003\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1862 - val_loss: 2.6769\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1670 - val_loss: 2.6890\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1391 - val_loss: 2.6585\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1204 - val_loss: 2.6991\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1325 - val_loss: 2.6644\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1287 - val_loss: 2.7155\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1360 - val_loss: 2.7082\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1523 - val_loss: 2.6989\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1503 - val_loss: 2.7411\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1608 - val_loss: 2.7115\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1461 - val_loss: 2.6844\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1217 - val_loss: 2.6860\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1381 - val_loss: 2.7439\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1281 - val_loss: 2.7079\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1303 - val_loss: 2.6669\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1530 - val_loss: 2.7544\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1693 - val_loss: 2.6596\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1141 - val_loss: 2.7487\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1215 - val_loss: 2.6794\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1128 - val_loss: 2.6494\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1167 - val_loss: 2.6774\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1245 - val_loss: 2.7038\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1477 - val_loss: 2.6815\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1009 - val_loss: 2.6700\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1327 - val_loss: 2.7135\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1130 - val_loss: 2.6933\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1244 - val_loss: 2.6848\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0799 - val_loss: 2.7131\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1257 - val_loss: 2.7587\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1467 - val_loss: 2.7138\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1176 - val_loss: 2.7239\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1230 - val_loss: 2.7069\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1150 - val_loss: 2.6417\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0963 - val_loss: 2.7128\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0888 - val_loss: 2.6510\n",
      "Epoch 420/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0859 - val_loss: 2.7144\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1143 - val_loss: 2.6883\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0756 - val_loss: 2.7536\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1002 - val_loss: 2.6703\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1260 - val_loss: 2.7310\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1274 - val_loss: 2.6969\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0889 - val_loss: 2.7032\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0893 - val_loss: 2.6509\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0945 - val_loss: 2.7094\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0978 - val_loss: 2.6940\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1033 - val_loss: 2.7355\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1062 - val_loss: 2.7094\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0734 - val_loss: 2.7083\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1008 - val_loss: 2.7295\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1039 - val_loss: 2.6573\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1002 - val_loss: 2.6875\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1141 - val_loss: 2.7887\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0907 - val_loss: 2.6964\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0755 - val_loss: 2.7747\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0866 - val_loss: 2.7009\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0750 - val_loss: 2.7247\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 2.0700 - val_loss: 2.7341\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0908 - val_loss: 2.7020\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.0798 - val_loss: 2.6917\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0978 - val_loss: 2.7353\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0670 - val_loss: 2.6925\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0718 - val_loss: 2.6665\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0517 - val_loss: 2.6636\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0543 - val_loss: 2.7046\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0797 - val_loss: 2.7141\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0756 - val_loss: 2.7633\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0734 - val_loss: 2.7188\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0567 - val_loss: 2.6799\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0855 - val_loss: 2.6311\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0700 - val_loss: 2.7139\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0763 - val_loss: 2.6690\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0325 - val_loss: 2.7687\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0791 - val_loss: 2.7282\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0455 - val_loss: 2.6719\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0347 - val_loss: 2.6972\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0290 - val_loss: 2.6946\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0474 - val_loss: 2.7906\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0425 - val_loss: 2.7946\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0590 - val_loss: 2.7301\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0464 - val_loss: 2.7201\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0319 - val_loss: 2.7344\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0547 - val_loss: 2.7358\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0560 - val_loss: 2.7056\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0439 - val_loss: 2.7304\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0625 - val_loss: 2.6919\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0329 - val_loss: 2.7184\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0368 - val_loss: 2.7350\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0398 - val_loss: 2.8608\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0740 - val_loss: 2.7946\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0538 - val_loss: 2.7797\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0536 - val_loss: 2.6965\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0451 - val_loss: 2.7065\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0468 - val_loss: 2.7582\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0524 - val_loss: 2.8248\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0676 - val_loss: 2.7515\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0216 - val_loss: 2.7118\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0487 - val_loss: 2.7591\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0495 - val_loss: 2.7514\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0283 - val_loss: 2.6636\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0466 - val_loss: 2.7092\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0139 - val_loss: 2.7149\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0300 - val_loss: 2.7134\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0348 - val_loss: 2.8366\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0566 - val_loss: 2.7447\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0486 - val_loss: 2.7241\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0226 - val_loss: 2.7051\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0344 - val_loss: 2.7148\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0036 - val_loss: 2.7021\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0525 - val_loss: 2.6816\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0141 - val_loss: 2.7130\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9983 - val_loss: 2.7546\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0266 - val_loss: 2.6991\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0298 - val_loss: 2.7133\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0127 - val_loss: 2.7478\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9949 - val_loss: 2.7508\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0020 - val_loss: 2.7336\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0209 - val_loss: 2.7185\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0311 - val_loss: 2.7098\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0097 - val_loss: 2.8212\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0367 - val_loss: 2.7269\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9954 - val_loss: 2.7119\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0214 - val_loss: 2.7085\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9903 - val_loss: 2.7114\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9873 - val_loss: 2.7580\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0049 - val_loss: 2.7328\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0160 - val_loss: 2.7433\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0265 - val_loss: 2.7578\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9980 - val_loss: 2.7350\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9820 - val_loss: 2.7636\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0313 - val_loss: 2.7007\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9735 - val_loss: 2.7798\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9903 - val_loss: 2.7377\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9827 - val_loss: 2.7052\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 1.9783 - val_loss: 2.7506\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9862 - val_loss: 2.7188\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0044 - val_loss: 2.7118\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9873 - val_loss: 2.7605\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9791 - val_loss: 2.7586\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9834 - val_loss: 2.8250\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0108 - val_loss: 2.7964\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9986 - val_loss: 2.7481\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9667 - val_loss: 2.7173\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9726 - val_loss: 2.8210\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9747 - val_loss: 2.7159\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.0052 - val_loss: 2.7210\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9462 - val_loss: 2.7423\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9817 - val_loss: 2.7444\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9589 - val_loss: 2.7509\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0001 - val_loss: 2.7637\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9767 - val_loss: 2.9693\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0336 - val_loss: 2.7664\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0186 - val_loss: 2.7712\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9731 - val_loss: 2.9239\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0146 - val_loss: 2.7854\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9755 - val_loss: 2.7398\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9846 - val_loss: 2.7470\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9623 - val_loss: 2.7580\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9694 - val_loss: 2.7424\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9390 - val_loss: 2.8392\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9784 - val_loss: 2.7541\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9347 - val_loss: 2.8126\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9615 - val_loss: 2.7435\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9595 - val_loss: 2.7818\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9581 - val_loss: 2.7482\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9468 - val_loss: 2.7938\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9510 - val_loss: 2.7856\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9921 - val_loss: 2.7310\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9410 - val_loss: 2.8362\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9883 - val_loss: 2.8318\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9591 - val_loss: 2.7979\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9670 - val_loss: 2.7599\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9502 - val_loss: 2.7846\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9408 - val_loss: 2.7437\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9584 - val_loss: 2.7968\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9550 - val_loss: 2.7847\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9443 - val_loss: 2.8524\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9707 - val_loss: 2.7739\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9343 - val_loss: 2.7222\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9591 - val_loss: 2.7887\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9487 - val_loss: 2.7560\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9167 - val_loss: 2.8024\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9498 - val_loss: 2.7949\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9552 - val_loss: 2.7080\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9422 - val_loss: 2.7372\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9257 - val_loss: 2.7595\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9354 - val_loss: 2.8149\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9530 - val_loss: 2.7344\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9268 - val_loss: 2.7882\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9169 - val_loss: 2.7968\n",
      "Epoch 574/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9117 - val_loss: 2.8232\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9559 - val_loss: 2.7865\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9351 - val_loss: 2.7369\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9167 - val_loss: 2.7308\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9072 - val_loss: 2.7263\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9452 - val_loss: 2.7604\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9047 - val_loss: 2.7266\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8968 - val_loss: 2.7716\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9323 - val_loss: 2.7494\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9390 - val_loss: 2.8218\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9503 - val_loss: 2.7116\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9185 - val_loss: 2.8958\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9600 - val_loss: 2.8010\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9162 - val_loss: 2.8532\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9541 - val_loss: 2.7526\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9123 - val_loss: 2.7393\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9271 - val_loss: 3.0872\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0649 - val_loss: 2.8290\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9359 - val_loss: 2.7441\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9015 - val_loss: 2.7787\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9088 - val_loss: 2.8607\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9503 - val_loss: 2.7239\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9083 - val_loss: 2.7209\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9045 - val_loss: 2.7762\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9039 - val_loss: 2.7301\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.8813 - val_loss: 2.7887\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9156 - val_loss: 2.7819\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 876us/step - loss: 47.3945 - val_loss: 29.2225\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 21.6955 - val_loss: 15.6114\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 13.6383 - val_loss: 9.8282\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 9.0474 - val_loss: 8.4086\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 8.3264 - val_loss: 8.0204\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 7.9161 - val_loss: 7.4871\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 7.3349 - val_loss: 6.7857\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 6.6303 - val_loss: 6.0223\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 5.8843 - val_loss: 5.4320\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 5.1939 - val_loss: 4.7756\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 4.6591 - val_loss: 4.5175\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 44us/step - loss: 4.3004 - val_loss: 4.3481\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 4.0522 - val_loss: 4.0001\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.8650 - val_loss: 3.9312\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.7136 - val_loss: 3.8376\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 3.5799 - val_loss: 3.6310\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.4435 - val_loss: 3.5463\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.3946 - val_loss: 3.6399\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.3313 - val_loss: 3.4151\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.2778 - val_loss: 3.4752\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.2309 - val_loss: 3.4157\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.1600 - val_loss: 3.3717\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.1914 - val_loss: 3.2452\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 3.1042 - val_loss: 3.2745\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 3.0763 - val_loss: 3.2841\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 3.0837 - val_loss: 3.2075\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 3.0693 - val_loss: 3.2210\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 3.0299 - val_loss: 3.1771\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.9906 - val_loss: 3.2013\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.9920 - val_loss: 3.1716\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 3.0311 - val_loss: 3.5676\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 3.0659 - val_loss: 3.0932\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.9565 - val_loss: 3.1656\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.9493 - val_loss: 3.1649\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.9228 - val_loss: 3.1856\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.9517 - val_loss: 3.2582\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.9479 - val_loss: 3.0301\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.9535 - val_loss: 3.1534\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.9496 - val_loss: 3.1049\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.8932 - val_loss: 3.0733\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.9018 - val_loss: 3.0011\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.9045 - val_loss: 3.0986\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8889 - val_loss: 3.1138\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.9322 - val_loss: 2.9722\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.8273 - val_loss: 2.9638\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.9012 - val_loss: 3.0890\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8624 - val_loss: 3.1483\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.8478 - val_loss: 2.9283\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7997 - val_loss: 2.9288\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.8173 - val_loss: 3.0236\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.7855 - val_loss: 2.9248\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.8127 - val_loss: 3.0780\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8208 - val_loss: 2.9610\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7907 - val_loss: 3.0061\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8993 - val_loss: 2.9753\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.7886 - val_loss: 3.0129\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8117 - val_loss: 2.9982\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7861 - val_loss: 2.9095\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7789 - val_loss: 2.8900\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7578 - val_loss: 3.0072\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7457 - val_loss: 2.9295\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7913 - val_loss: 2.9908\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7842 - val_loss: 2.9516\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.7493 - val_loss: 3.0450\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7699 - val_loss: 2.9390\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7310 - val_loss: 2.8996\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7563 - val_loss: 3.1046\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7391 - val_loss: 3.1466\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7291 - val_loss: 2.9257\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.7110 - val_loss: 2.8888\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7168 - val_loss: 2.9192\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7107 - val_loss: 2.9421\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7512 - val_loss: 2.9984\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.7113 - val_loss: 3.0854\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.7793 - val_loss: 2.8727\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.6795 - val_loss: 2.8598\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.7093 - val_loss: 2.8766\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.6685 - val_loss: 2.8599\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.6821 - val_loss: 3.0681\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.6859 - val_loss: 2.8831\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.7291 - val_loss: 2.9139\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.6874 - val_loss: 2.8582\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.6729 - val_loss: 3.0221\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.7629 - val_loss: 3.0609\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.7046 - val_loss: 2.9217\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.6799 - val_loss: 2.8627\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.6893 - val_loss: 3.1099\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.7175 - val_loss: 2.8830\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.6611 - val_loss: 3.0165\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.6853 - val_loss: 2.8484\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.6340 - val_loss: 2.9339\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6334 - val_loss: 2.8530\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6350 - val_loss: 2.9516\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6531 - val_loss: 2.8763\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6335 - val_loss: 2.8838\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.6614 - val_loss: 2.9539\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.6181 - val_loss: 2.8354\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 2.6130 - val_loss: 2.8921\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.6424 - val_loss: 3.0791\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 2.6902 - val_loss: 2.8816\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 2.6369 - val_loss: 2.8565\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.6361 - val_loss: 2.9971\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.6583 - val_loss: 2.8198\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 2.5866 - val_loss: 2.8460\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 2.6337 - val_loss: 2.8216\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 65us/step - loss: 2.6231 - val_loss: 2.8455\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.5844 - val_loss: 2.9016\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.6651 - val_loss: 2.8836\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 2.5887 - val_loss: 2.8332\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.5919 - val_loss: 2.9229\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5908 - val_loss: 2.8577\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5556 - val_loss: 2.8492\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.5886 - val_loss: 2.8273\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5764 - val_loss: 2.9129\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5757 - val_loss: 2.8981\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5994 - val_loss: 2.9687\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5805 - val_loss: 2.8348\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5466 - val_loss: 3.0283\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5955 - val_loss: 2.8459\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5725 - val_loss: 2.8683\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.6088 - val_loss: 3.0069\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5601 - val_loss: 2.8305\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5660 - val_loss: 2.8795\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5846 - val_loss: 2.8397\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5850 - val_loss: 2.8496\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5806 - val_loss: 2.8167\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5315 - val_loss: 2.8101\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5290 - val_loss: 2.8606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5484 - val_loss: 2.8216\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5227 - val_loss: 2.8861\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.5465 - val_loss: 2.8004\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.5425 - val_loss: 2.8273\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 2.5353 - val_loss: 2.8018\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.5383 - val_loss: 2.8297\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5390 - val_loss: 2.8060\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5217 - val_loss: 2.8031\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5395 - val_loss: 2.8501\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5484 - val_loss: 2.9203\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5619 - val_loss: 2.8379\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5586 - val_loss: 2.7908\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5085 - val_loss: 2.8693\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5347 - val_loss: 2.8261\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5064 - val_loss: 2.8080\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5057 - val_loss: 2.9229\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5014 - val_loss: 3.1065\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.5211 - val_loss: 2.7958\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.4931 - val_loss: 2.8136\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 2.4856 - val_loss: 2.9520\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.5378 - val_loss: 3.1794\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5345 - val_loss: 2.8097\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.5080 - val_loss: 2.9960\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5261 - val_loss: 2.7939\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5009 - val_loss: 2.9622\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.5121 - val_loss: 2.8979\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4857 - val_loss: 2.9053\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4652 - val_loss: 2.8229\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.4728 - val_loss: 2.8479\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4763 - val_loss: 2.8039\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.4422 - val_loss: 3.0009\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4875 - val_loss: 2.8293\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4585 - val_loss: 2.8470\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4601 - val_loss: 2.9813\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4842 - val_loss: 2.8283\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4638 - val_loss: 2.7880\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4550 - val_loss: 2.9850\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4829 - val_loss: 2.8722\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4587 - val_loss: 2.8227\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.4549 - val_loss: 2.8800\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4721 - val_loss: 2.8179\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4437 - val_loss: 2.7677\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4300 - val_loss: 2.7975\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4606 - val_loss: 2.8628\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4636 - val_loss: 2.8102\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4385 - val_loss: 2.8116\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 2.4254 - val_loss: 2.7916\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.4428 - val_loss: 2.9012\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4696 - val_loss: 2.7727\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4194 - val_loss: 2.7710\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4652 - val_loss: 2.9487\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4548 - val_loss: 2.8324\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4215 - val_loss: 2.8441\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4402 - val_loss: 3.0068\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4472 - val_loss: 2.7750\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4269 - val_loss: 2.8055\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4385 - val_loss: 2.8059\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3932 - val_loss: 2.7794\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4193 - val_loss: 2.8242\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4289 - val_loss: 2.8210\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4328 - val_loss: 3.0098\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4447 - val_loss: 2.8104\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4209 - val_loss: 2.7942\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4187 - val_loss: 2.8121\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.4158 - val_loss: 2.8488\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4189 - val_loss: 2.8148\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3866 - val_loss: 2.7776\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3983 - val_loss: 2.7903\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4096 - val_loss: 2.7782\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3931 - val_loss: 2.8346\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3778 - val_loss: 2.8245\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4131 - val_loss: 2.8222\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4326 - val_loss: 2.7500\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3916 - val_loss: 2.8193\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3959 - val_loss: 2.7774\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3895 - val_loss: 2.7715\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4014 - val_loss: 2.7956\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3741 - val_loss: 2.8922\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3903 - val_loss: 2.8307\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3963 - val_loss: 2.8131\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.3920 - val_loss: 2.8019\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4243 - val_loss: 2.7992\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3712 - val_loss: 2.7600\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3762 - val_loss: 2.7654\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3688 - val_loss: 2.8535\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3856 - val_loss: 2.8032\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3889 - val_loss: 2.7697\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3645 - val_loss: 2.7754\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3911 - val_loss: 2.7878\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3609 - val_loss: 2.7726\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3568 - val_loss: 2.8913\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4153 - val_loss: 2.8972\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3748 - val_loss: 2.7882\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3565 - val_loss: 2.7753\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3820 - val_loss: 2.7623\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3809 - val_loss: 2.7881\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3577 - val_loss: 2.7948\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3617 - val_loss: 2.7683\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3538 - val_loss: 2.7699\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3563 - val_loss: 2.7731\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3568 - val_loss: 2.7362\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3271 - val_loss: 2.7949\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3371 - val_loss: 2.7610\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3845 - val_loss: 2.8571\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3699 - val_loss: 2.7670\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3632 - val_loss: 2.7719\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3187 - val_loss: 2.8782\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3803 - val_loss: 2.8715\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3448 - val_loss: 2.7677\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3631 - val_loss: 2.7857\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3504 - val_loss: 2.8703\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3301 - val_loss: 2.7917\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3571 - val_loss: 2.7750\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3629 - val_loss: 2.7987\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3303 - val_loss: 2.7727\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3323 - val_loss: 2.8073\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3031 - val_loss: 2.8096\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3560 - val_loss: 2.7697\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3203 - val_loss: 2.7802\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3159 - val_loss: 2.7946\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3337 - val_loss: 2.8368\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3577 - val_loss: 2.8091\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3154 - val_loss: 2.7960\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3087 - val_loss: 2.8703\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3458 - val_loss: 2.8496\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3238 - val_loss: 2.7742\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3256 - val_loss: 2.7776\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3303 - val_loss: 3.0044\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3282 - val_loss: 2.8039\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3088 - val_loss: 2.7718\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3096 - val_loss: 2.7820\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3099 - val_loss: 2.7565\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3028 - val_loss: 2.7796\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3170 - val_loss: 2.7741\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3054 - val_loss: 2.7656\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3051 - val_loss: 2.7872\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3114 - val_loss: 2.9040\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3183 - val_loss: 2.8318\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.3181 - val_loss: 2.7613\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2846 - val_loss: 2.7733\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2915 - val_loss: 2.7572\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2979 - val_loss: 2.8773\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.3008 - val_loss: 2.8560\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.3393 - val_loss: 2.7632\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.3067 - val_loss: 2.7998\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2822 - val_loss: 2.8018\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2884 - val_loss: 2.8147\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2875 - val_loss: 3.0449\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3736 - val_loss: 2.7836\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2912 - val_loss: 2.7631\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3020 - val_loss: 2.8466\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3017 - val_loss: 2.7694\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2757 - val_loss: 3.0073\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3081 - val_loss: 2.7574\n",
      "Epoch 283/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2673 - val_loss: 2.8121\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2824 - val_loss: 2.7449\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2658 - val_loss: 3.0119\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2985 - val_loss: 2.7852\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2792 - val_loss: 2.8685\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2803 - val_loss: 2.7795\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2829 - val_loss: 2.7745\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2576 - val_loss: 2.8187\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2486 - val_loss: 2.8152\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2752 - val_loss: 2.8751\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3038 - val_loss: 2.7619\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2529 - val_loss: 2.9633\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3036 - val_loss: 2.7761\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2678 - val_loss: 2.7657\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3126 - val_loss: 2.8656\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2604 - val_loss: 2.8917\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2651 - val_loss: 2.8373\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2874 - val_loss: 2.7580\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2592 - val_loss: 2.8159\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2638 - val_loss: 2.7883\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2750 - val_loss: 2.8848\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2708 - val_loss: 2.7776\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2570 - val_loss: 2.8133\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2493 - val_loss: 2.8273\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2555 - val_loss: 2.8228\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2449 - val_loss: 2.8622\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2770 - val_loss: 2.7564\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2501 - val_loss: 2.7357\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2575 - val_loss: 2.7726\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2511 - val_loss: 2.8515\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2493 - val_loss: 2.8334\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2413 - val_loss: 2.7730\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2425 - val_loss: 2.7892\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2312 - val_loss: 2.8227\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2323 - val_loss: 2.7709\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2198 - val_loss: 2.8095\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2623 - val_loss: 2.7920\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2148 - val_loss: 2.9629\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2529 - val_loss: 2.7573\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2040 - val_loss: 2.8211\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2670 - val_loss: 2.9113\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2384 - val_loss: 2.7780\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2148 - val_loss: 2.8129\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2188 - val_loss: 2.7804\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2118 - val_loss: 2.7745\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2352 - val_loss: 2.8396\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2150 - val_loss: 2.9134\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2697 - val_loss: 2.8272\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.2435 - val_loss: 2.8650\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2385 - val_loss: 2.8350\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2466 - val_loss: 2.8211\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2293 - val_loss: 2.8323\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2038 - val_loss: 2.8400\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2070 - val_loss: 2.8124\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2460 - val_loss: 2.7988\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2016 - val_loss: 2.8916\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2287 - val_loss: 2.7994\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2078 - val_loss: 2.8494\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2024 - val_loss: 2.8867\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2203 - val_loss: 2.7609\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2366 - val_loss: 2.8727\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.2080 - val_loss: 2.9923\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3016 - val_loss: 2.8264\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2141 - val_loss: 2.8228\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1860 - val_loss: 2.7981\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1965 - val_loss: 2.8176\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1949 - val_loss: 2.8075\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1863 - val_loss: 2.7885\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2130 - val_loss: 2.7903\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2122 - val_loss: 2.9068\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2160 - val_loss: 2.7843\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1955 - val_loss: 2.7794\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2194 - val_loss: 2.7965\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1634 - val_loss: 2.8428\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1902 - val_loss: 2.7823\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1655 - val_loss: 2.8591\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2359 - val_loss: 2.7825\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1877 - val_loss: 2.7590\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1694 - val_loss: 2.8525\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2113 - val_loss: 2.9337\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2318 - val_loss: 2.8057\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1785 - val_loss: 2.8186\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1824 - val_loss: 2.8350\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1892 - val_loss: 2.9153\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2086 - val_loss: 2.8540\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1788 - val_loss: 2.8651\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1912 - val_loss: 2.8060\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1691 - val_loss: 2.8819\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2248 - val_loss: 2.8717\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1806 - val_loss: 2.8234\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1820 - val_loss: 2.8998\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1997 - val_loss: 2.9023\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1719 - val_loss: 2.8130\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1668 - val_loss: 2.8397\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2026 - val_loss: 2.8331\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1957 - val_loss: 2.8034\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1432 - val_loss: 2.8836\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1562 - val_loss: 2.8605\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1698 - val_loss: 2.9458\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1702 - val_loss: 2.8441\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1510 - val_loss: 2.8190\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1448 - val_loss: 2.8338\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1615 - val_loss: 2.8627\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1602 - val_loss: 2.9088\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1547 - val_loss: 2.8967\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1853 - val_loss: 2.8605\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1624 - val_loss: 2.8094\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1861 - val_loss: 2.8629\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1627 - val_loss: 2.8900\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1679 - val_loss: 2.8991\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1308 - val_loss: 2.8303\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1675 - val_loss: 2.8914\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1400 - val_loss: 2.8028\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1576 - val_loss: 3.0616\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1739 - val_loss: 2.8434\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1641 - val_loss: 2.8335\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1423 - val_loss: 2.8174\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1282 - val_loss: 2.9517\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1691 - val_loss: 2.8353\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1387 - val_loss: 2.8326\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1503 - val_loss: 2.8165\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1535 - val_loss: 2.8146\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1388 - val_loss: 2.8464\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1574 - val_loss: 2.8101\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1422 - val_loss: 2.9298\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1810 - val_loss: 2.8780\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1682 - val_loss: 2.8121\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1639 - val_loss: 2.8279\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1231 - val_loss: 2.8352\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0934 - val_loss: 2.8660\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0940 - val_loss: 2.8654\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1722 - val_loss: 3.0195\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2032 - val_loss: 2.9144\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1488 - val_loss: 2.9423\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1577 - val_loss: 2.8362\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1284 - val_loss: 2.8111\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1215 - val_loss: 2.8327\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1092 - val_loss: 2.8531\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1118 - val_loss: 2.8306\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1222 - val_loss: 2.8848\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1587 - val_loss: 2.8742\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1271 - val_loss: 2.9108\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1202 - val_loss: 2.9013\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1305 - val_loss: 2.8986\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1189 - val_loss: 2.8160\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1141 - val_loss: 2.8381\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1204 - val_loss: 2.8300\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0938 - val_loss: 2.8529\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.1686 - val_loss: 2.8116\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1072 - val_loss: 2.8160\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1080 - val_loss: 2.8180\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1021 - val_loss: 2.9679\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.1434 - val_loss: 2.9052\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1237 - val_loss: 2.8554\n",
      "Epoch 437/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1227 - val_loss: 2.8577\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1315 - val_loss: 2.8274\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1116 - val_loss: 2.8742\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1159 - val_loss: 2.8664\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1002 - val_loss: 2.8554\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1104 - val_loss: 2.8984\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1346 - val_loss: 2.8099\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0973 - val_loss: 2.8457\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0830 - val_loss: 2.8531\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1246 - val_loss: 2.8782\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1420 - val_loss: 2.8302\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0797 - val_loss: 2.8123\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0879 - val_loss: 2.8443\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0806 - val_loss: 2.8389\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0855 - val_loss: 2.8341\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0887 - val_loss: 2.8539\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1131 - val_loss: 2.8631\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1157 - val_loss: 2.8898\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0736 - val_loss: 2.8180\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0685 - val_loss: 2.8148\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0925 - val_loss: 2.8245\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0860 - val_loss: 2.9302\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1127 - val_loss: 2.8365\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0960 - val_loss: 2.8561\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1034 - val_loss: 2.9571\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1124 - val_loss: 2.8202\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0654 - val_loss: 2.8912\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0918 - val_loss: 2.9216\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0666 - val_loss: 2.8465\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0964 - val_loss: 2.9307\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0902 - val_loss: 2.8464\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0827 - val_loss: 2.9220\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0976 - val_loss: 2.9164\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0977 - val_loss: 2.9100\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1178 - val_loss: 3.0089\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1142 - val_loss: 2.8552\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0719 - val_loss: 2.8885\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0558 - val_loss: 2.8313\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0758 - val_loss: 2.8304\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0503 - val_loss: 2.9477\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0965 - val_loss: 2.8862\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0606 - val_loss: 2.9178\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0539 - val_loss: 2.8505\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0516 - val_loss: 2.8319\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0520 - val_loss: 2.8453\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0672 - val_loss: 2.8345\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0487 - val_loss: 2.8158\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0324 - val_loss: 2.9224\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1075 - val_loss: 3.0842\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1167 - val_loss: 2.8263\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0431 - val_loss: 2.9226\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0683 - val_loss: 2.8060\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0219 - val_loss: 2.8731\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0546 - val_loss: 3.0086\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1156 - val_loss: 2.9030\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0494 - val_loss: 2.8186\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0155 - val_loss: 2.8093\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0203 - val_loss: 2.8334\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0267 - val_loss: 3.1025\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1014 - val_loss: 2.8423\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0477 - val_loss: 2.9047\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0339 - val_loss: 2.8714\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0865 - val_loss: 2.9335\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0784 - val_loss: 2.8852\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0477 - val_loss: 2.8654\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0253 - val_loss: 2.9038\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0783 - val_loss: 2.9278\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0238 - val_loss: 2.8200\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0039 - val_loss: 2.8565\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0566 - val_loss: 2.8608\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1054 - val_loss: 2.8689\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0472 - val_loss: 2.8545\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0555 - val_loss: 2.8645\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0121 - val_loss: 2.8884\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0299 - val_loss: 2.8131\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0377 - val_loss: 2.8945\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0636 - val_loss: 2.9134\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0315 - val_loss: 2.9024\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0781 - val_loss: 2.9115\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0217 - val_loss: 2.8756\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0367 - val_loss: 2.8272\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0003 - val_loss: 2.8880\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9992 - val_loss: 3.0570\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.1238 - val_loss: 2.9385\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0358 - val_loss: 2.9366\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0280 - val_loss: 2.8648\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0158 - val_loss: 2.8725\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0215 - val_loss: 2.8959\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0409 - val_loss: 2.8346\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0026 - val_loss: 2.9894\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0410 - val_loss: 2.8928\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0246 - val_loss: 2.8681\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0021 - val_loss: 2.8684\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0139 - val_loss: 2.8900\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9978 - val_loss: 2.8824\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9920 - val_loss: 2.9267\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0473 - val_loss: 2.9636\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.0200 - val_loss: 2.8719\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9909 - val_loss: 2.8669\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0048 - val_loss: 2.8568\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9870 - val_loss: 2.8427\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9783 - val_loss: 2.8903\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0088 - val_loss: 3.0087\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0100 - val_loss: 2.8560\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9876 - val_loss: 3.0933\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0450 - val_loss: 2.8591\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9882 - val_loss: 2.8513\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9855 - val_loss: 2.9859\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0577 - val_loss: 2.8635\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0196 - val_loss: 2.8576\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9917 - val_loss: 2.8756\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9812 - val_loss: 2.9249\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9841 - val_loss: 2.8698\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9567 - val_loss: 2.8976\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9854 - val_loss: 2.8989\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9811 - val_loss: 2.8864\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9851 - val_loss: 2.8939\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0256 - val_loss: 2.9821\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0339 - val_loss: 2.9210\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0339 - val_loss: 2.8593\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0112 - val_loss: 2.8429\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9833 - val_loss: 2.8836\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9699 - val_loss: 2.8648\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9672 - val_loss: 2.8444\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9628 - val_loss: 2.8826\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0005 - val_loss: 2.8775\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9764 - val_loss: 2.8789\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9973 - val_loss: 2.8880\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9964 - val_loss: 2.8461\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9663 - val_loss: 2.8545\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9596 - val_loss: 3.0014\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0023 - val_loss: 2.9701\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9456 - val_loss: 2.9408\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0160 - val_loss: 2.8547\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9619 - val_loss: 2.9196\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9571 - val_loss: 2.9317\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9885 - val_loss: 2.8964\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9598 - val_loss: 3.0403\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0275 - val_loss: 2.9250\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9914 - val_loss: 2.9231\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9828 - val_loss: 2.8775\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9497 - val_loss: 2.8709\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9319 - val_loss: 2.9008\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9766 - val_loss: 2.8700\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9626 - val_loss: 2.8770\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9612 - val_loss: 2.8778\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9768 - val_loss: 2.8850\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9356 - val_loss: 2.8978\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9619 - val_loss: 2.9233\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9780 - val_loss: 2.8790\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9611 - val_loss: 2.9034\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9798 - val_loss: 2.8842\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9742 - val_loss: 3.0281\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0038 - val_loss: 2.9284\n",
      "Epoch 591/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9556 - val_loss: 2.9056\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9305 - val_loss: 2.9225\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9862 - val_loss: 3.0164\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0573 - val_loss: 2.8800\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9493 - val_loss: 2.8532\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9559 - val_loss: 2.9418\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9602 - val_loss: 2.8903\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9638 - val_loss: 2.9388\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9280 - val_loss: 2.9130\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9530 - val_loss: 2.9166\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 853us/step - loss: 47.9352 - val_loss: 31.2425\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 21.7507 - val_loss: 16.3705\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 13.2249 - val_loss: 10.9439\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 8.9070 - val_loss: 8.7717\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 8.2747 - val_loss: 8.5301\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 7.8813 - val_loss: 8.0547\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 7.3035 - val_loss: 7.4678\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 6.6393 - val_loss: 6.7083\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 5.9047 - val_loss: 6.1786\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 5.2913 - val_loss: 5.4734\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 4.7282 - val_loss: 4.8868\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 4.3169 - val_loss: 4.4868\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 4.0342 - val_loss: 4.1911\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.7878 - val_loss: 4.0055\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.6948 - val_loss: 3.8140\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.5514 - val_loss: 3.7716\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 3.5274 - val_loss: 3.6398\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.3848 - val_loss: 3.6499\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.3153 - val_loss: 3.4837\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.2374 - val_loss: 3.3572\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.2210 - val_loss: 3.4147\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.1271 - val_loss: 3.3386\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.1472 - val_loss: 3.2532\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.1223 - val_loss: 3.3522\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.1271 - val_loss: 3.2681\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.0472 - val_loss: 3.1711\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.0362 - val_loss: 3.4464\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.0750 - val_loss: 3.2170\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.0040 - val_loss: 3.1279\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.0316 - val_loss: 3.3107\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.9532 - val_loss: 3.1914\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.9610 - val_loss: 3.1060\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.9360 - val_loss: 3.0889\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.9547 - val_loss: 3.0821\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.9552 - val_loss: 3.1476\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.9027 - val_loss: 3.0599\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8872 - val_loss: 3.0297\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.8944 - val_loss: 3.0446\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8377 - val_loss: 3.0600\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.8517 - val_loss: 3.0977\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.8589 - val_loss: 3.0800\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.8547 - val_loss: 3.1486\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8489 - val_loss: 3.0318\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8546 - val_loss: 2.9746\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8479 - val_loss: 3.0320\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8116 - val_loss: 3.2070\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8300 - val_loss: 3.1237\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.8331 - val_loss: 2.9653\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7875 - val_loss: 3.2159\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.8102 - val_loss: 2.9145\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7829 - val_loss: 2.9249\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - ETA: 0s - loss: 2.738 - 0s 27us/step - loss: 2.7688 - val_loss: 2.9814\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.7704 - val_loss: 2.9592\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.7737 - val_loss: 2.9646\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7524 - val_loss: 2.8753\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.7506 - val_loss: 3.0181\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7711 - val_loss: 3.2566\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.7829 - val_loss: 2.9940\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7724 - val_loss: 3.0962\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7420 - val_loss: 2.9471\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7460 - val_loss: 2.8967\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7627 - val_loss: 2.9901\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.7588 - val_loss: 2.9116\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6923 - val_loss: 3.0421\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7510 - val_loss: 2.8588\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7164 - val_loss: 2.9238\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.7040 - val_loss: 2.8359\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7175 - val_loss: 2.9898\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6925 - val_loss: 2.8421\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.6659 - val_loss: 2.9045\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6869 - val_loss: 3.0249\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6510 - val_loss: 2.8328\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6784 - val_loss: 2.8802\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.7239 - val_loss: 2.8898\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.7096 - val_loss: 2.8616\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6326 - val_loss: 2.9617\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.6601 - val_loss: 2.8130\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.6618 - val_loss: 2.9250\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6435 - val_loss: 2.8201\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6328 - val_loss: 2.8439\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6634 - val_loss: 2.8684\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6560 - val_loss: 2.9181\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6348 - val_loss: 2.9687\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6435 - val_loss: 2.9008\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6533 - val_loss: 2.8932\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6150 - val_loss: 2.9350\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6157 - val_loss: 2.8288\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.6232 - val_loss: 2.8268\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6104 - val_loss: 2.7976\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6056 - val_loss: 2.8200\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6108 - val_loss: 2.8366\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5817 - val_loss: 2.8472\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5841 - val_loss: 2.8930\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6034 - val_loss: 2.8314\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5871 - val_loss: 2.8333\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5848 - val_loss: 2.8103\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5696 - val_loss: 2.8395\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.6046 - val_loss: 2.8012\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5858 - val_loss: 3.0042\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5728 - val_loss: 2.8425\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6047 - val_loss: 2.8584\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.5751 - val_loss: 2.8531\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.5526 - val_loss: 2.9954\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.6112 - val_loss: 2.8278\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5689 - val_loss: 2.8625\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5714 - val_loss: 2.9614\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5673 - val_loss: 2.8328\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5536 - val_loss: 2.8579\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5705 - val_loss: 2.8023\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5349 - val_loss: 2.8079\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5198 - val_loss: 2.7683\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5193 - val_loss: 2.8245\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.5245 - val_loss: 2.8303\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5063 - val_loss: 2.8785\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5119 - val_loss: 2.8975\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.5740 - val_loss: 2.7623\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.5453 - val_loss: 2.8312\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.5075 - val_loss: 2.9006\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.5904 - val_loss: 2.8115\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.5194 - val_loss: 2.8216\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.5151 - val_loss: 2.8849\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.5136 - val_loss: 2.7985\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5533 - val_loss: 2.9232\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5576 - val_loss: 2.8392\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5202 - val_loss: 2.8403\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.5050 - val_loss: 2.9486\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.5267 - val_loss: 3.0051\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.5465 - val_loss: 2.7934\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4977 - val_loss: 2.7926\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.5097 - val_loss: 2.9181\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.5311 - val_loss: 2.9506\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 2.5272 - val_loss: 2.7899\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.5200 - val_loss: 2.8280\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.5020 - val_loss: 2.8406\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 2.4878 - val_loss: 2.8914\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.5016 - val_loss: 2.8583\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 2.5035 - val_loss: 2.7958\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.4769 - val_loss: 2.8262\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 2.4743 - val_loss: 2.8097\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.4718 - val_loss: 2.8377\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 2.4732 - val_loss: 2.7652\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 2.4498 - val_loss: 2.7611\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.4640 - val_loss: 2.8302\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.4455 - val_loss: 2.8395\n",
      "Epoch 145/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4811 - val_loss: 2.8020\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.4871 - val_loss: 2.7917\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4505 - val_loss: 2.8434\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.4608 - val_loss: 2.7809\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.4705 - val_loss: 2.7565\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4681 - val_loss: 2.7997\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.4422 - val_loss: 2.7463\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4422 - val_loss: 2.8080\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4779 - val_loss: 2.7994\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.4469 - val_loss: 2.7841\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.4316 - val_loss: 2.8495\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4480 - val_loss: 2.8902\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.4712 - val_loss: 3.1118\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4835 - val_loss: 2.8585\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4524 - val_loss: 2.8162\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4666 - val_loss: 2.9140\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4400 - val_loss: 2.9338\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4789 - val_loss: 2.7715\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4404 - val_loss: 2.7555\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.4589 - val_loss: 2.7756\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.4470 - val_loss: 2.8099\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.4409 - val_loss: 2.7439\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.4149 - val_loss: 2.7676\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.4313 - val_loss: 2.7560\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.4240 - val_loss: 2.8279\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4238 - val_loss: 2.8141\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4445 - val_loss: 2.8094\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4445 - val_loss: 2.7465\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4080 - val_loss: 2.8250\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.4237 - val_loss: 2.8568\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4007 - val_loss: 2.7549\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3931 - val_loss: 2.9605\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4214 - val_loss: 2.7994\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4345 - val_loss: 2.7626\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3885 - val_loss: 2.7460\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3927 - val_loss: 2.7725\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4057 - val_loss: 2.7379\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3722 - val_loss: 2.9799\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.4809 - val_loss: 2.7259\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.3861 - val_loss: 2.7545\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3981 - val_loss: 2.7410\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3833 - val_loss: 2.7817\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3826 - val_loss: 2.7225\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.4121 - val_loss: 2.7243\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3802 - val_loss: 2.7502\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3617 - val_loss: 2.7565\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3826 - val_loss: 2.7236\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3938 - val_loss: 2.7693\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4135 - val_loss: 2.7854\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3936 - val_loss: 2.8330\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.4115 - val_loss: 2.8041\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3730 - val_loss: 3.0234\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3927 - val_loss: 2.8145\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3835 - val_loss: 2.9045\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.4015 - val_loss: 2.7569\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.3651 - val_loss: 3.0541\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.4092 - val_loss: 2.7415\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3699 - val_loss: 2.9073\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3808 - val_loss: 3.0943\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.4150 - val_loss: 2.7394\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3644 - val_loss: 2.8127\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3697 - val_loss: 2.7835\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.3828 - val_loss: 2.8293\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.3741 - val_loss: 2.7170\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3480 - val_loss: 2.8134\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3459 - val_loss: 2.7546\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.3497 - val_loss: 2.7535\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3418 - val_loss: 2.8108\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3700 - val_loss: 2.8736\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.3726 - val_loss: 2.8846\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3411 - val_loss: 2.7720\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3419 - val_loss: 2.8848\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.3398 - val_loss: 2.7348\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.3410 - val_loss: 2.7548\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.3811 - val_loss: 2.8105\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.3418 - val_loss: 2.8548\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3789 - val_loss: 2.8664\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3484 - val_loss: 2.7728\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3263 - val_loss: 2.8716\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3573 - val_loss: 2.7491\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3293 - val_loss: 2.8350\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3452 - val_loss: 2.8032\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 46us/step - loss: 2.3463 - val_loss: 2.7468\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 2.3455 - val_loss: 2.8211\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 2.3568 - val_loss: 2.9079\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.3487 - val_loss: 2.7876\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.3580 - val_loss: 2.7256\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.3405 - val_loss: 2.8273\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.3567 - val_loss: 2.8319\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 2.3320 - val_loss: 2.7595\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.3356 - val_loss: 2.7758\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.3273 - val_loss: 2.7851\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 2.3157 - val_loss: 2.7340\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.3009 - val_loss: 2.7875\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.3612 - val_loss: 2.7838\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 41us/step - loss: 2.3172 - val_loss: 2.7469\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.3164 - val_loss: 2.7411\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 40us/step - loss: 2.3166 - val_loss: 2.8108\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.3210 - val_loss: 2.7453\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2917 - val_loss: 2.7565\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.3242 - val_loss: 2.8831\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3088 - val_loss: 2.7631\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3193 - val_loss: 2.8429\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3404 - val_loss: 2.8271\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2941 - val_loss: 2.7494\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3088 - val_loss: 2.7824\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2907 - val_loss: 2.8351\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3083 - val_loss: 2.7775\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.3023 - val_loss: 2.9466\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.3095 - val_loss: 2.7351\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.3046 - val_loss: 2.7784\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3218 - val_loss: 2.7545\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2919 - val_loss: 2.7478\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3044 - val_loss: 2.8179\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2885 - val_loss: 2.7272\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.3152 - val_loss: 2.7885\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2977 - val_loss: 2.7937\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2887 - val_loss: 2.8032\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2819 - val_loss: 2.8024\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.3069 - val_loss: 2.7270\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2806 - val_loss: 2.7263\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2800 - val_loss: 2.7529\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2760 - val_loss: 2.7471\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2960 - val_loss: 2.7708\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2919 - val_loss: 2.7471\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2655 - val_loss: 2.7473\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2810 - val_loss: 2.7892\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2635 - val_loss: 2.8379\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.2606 - val_loss: 2.7692\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.2676 - val_loss: 2.7511\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.2543 - val_loss: 2.7468\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2670 - val_loss: 2.7995\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.3007 - val_loss: 2.8047\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2675 - val_loss: 2.7486\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2605 - val_loss: 2.8930\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2925 - val_loss: 2.8044\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2679 - val_loss: 2.7398\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2611 - val_loss: 2.7906\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2618 - val_loss: 2.8516\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 2.2530 - val_loss: 2.7627\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 2.2338 - val_loss: 2.7551\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 2.2325 - val_loss: 2.9242\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 2.2585 - val_loss: 2.8273\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2349 - val_loss: 2.8015\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2496 - val_loss: 2.7739\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.2310 - val_loss: 2.7298\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2276 - val_loss: 2.7949\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.2300 - val_loss: 2.7581\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 2.2089 - val_loss: 2.7449\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 2.2418 - val_loss: 2.7815\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.2211 - val_loss: 2.8023\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2459 - val_loss: 2.8564\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2449 - val_loss: 2.8188\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2193 - val_loss: 2.8709\n",
      "Epoch 299/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2380 - val_loss: 2.8788\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2540 - val_loss: 2.7700\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2116 - val_loss: 2.7654\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2633 - val_loss: 2.7856\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2124 - val_loss: 2.7259\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2119 - val_loss: 2.8438\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2284 - val_loss: 2.8572\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2307 - val_loss: 2.8114\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2174 - val_loss: 2.8598\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2150 - val_loss: 3.0565\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.3016 - val_loss: 2.7983\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 2.2055 - val_loss: 2.8730\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.2153 - val_loss: 2.7733\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.2366 - val_loss: 2.7313\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.2129 - val_loss: 2.7934\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.2422 - val_loss: 2.7933\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.2081 - val_loss: 2.8674\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.2385 - val_loss: 2.7850\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1972 - val_loss: 2.8071\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2374 - val_loss: 2.8444\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 2.2494 - val_loss: 2.8297\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.2055 - val_loss: 2.8040\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1997 - val_loss: 2.7705\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 2.2077 - val_loss: 2.7937\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2046 - val_loss: 2.8502\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2089 - val_loss: 3.0249\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2272 - val_loss: 2.8275\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1932 - val_loss: 2.7590\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1989 - val_loss: 2.8097\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1995 - val_loss: 2.7433\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1760 - val_loss: 2.7758\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1829 - val_loss: 2.7434\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2035 - val_loss: 2.9140\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2262 - val_loss: 2.9032\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.2525 - val_loss: 2.7959\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2437 - val_loss: 2.9032\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.2098 - val_loss: 2.8218\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1824 - val_loss: 2.7414\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1516 - val_loss: 2.8644\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1750 - val_loss: 2.7827\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1594 - val_loss: 2.8057\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1684 - val_loss: 2.7610\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1764 - val_loss: 2.8125\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1715 - val_loss: 2.7925\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1638 - val_loss: 2.7569\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1657 - val_loss: 2.9755\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1717 - val_loss: 2.7366\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1804 - val_loss: 2.7813\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1724 - val_loss: 2.7867\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1968 - val_loss: 2.9244\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2187 - val_loss: 2.8285\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1904 - val_loss: 2.7822\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1595 - val_loss: 2.8406\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1694 - val_loss: 2.7705\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1518 - val_loss: 2.8692\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.2345 - val_loss: 2.8262\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1569 - val_loss: 2.8276\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1675 - val_loss: 2.8128\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1390 - val_loss: 2.7399\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1336 - val_loss: 2.8664\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1457 - val_loss: 2.7717\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1848 - val_loss: 2.8048\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1420 - val_loss: 2.7827\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1357 - val_loss: 2.7853\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1373 - val_loss: 2.8696\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1388 - val_loss: 2.9083\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1889 - val_loss: 2.7632\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1342 - val_loss: 2.8134\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.1468 - val_loss: 2.8683\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1743 - val_loss: 2.8237\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1562 - val_loss: 2.8173\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1500 - val_loss: 2.7619\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1189 - val_loss: 2.8028\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1549 - val_loss: 2.7854\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1468 - val_loss: 2.7722\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1501 - val_loss: 2.7935\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1414 - val_loss: 2.7829\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1598 - val_loss: 2.8864\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1334 - val_loss: 2.7919\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1153 - val_loss: 2.7698\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1235 - val_loss: 2.7797\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0950 - val_loss: 2.8048\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1236 - val_loss: 2.8685\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1385 - val_loss: 2.9512\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1534 - val_loss: 2.9086\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1383 - val_loss: 2.7601\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1005 - val_loss: 2.9730\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1275 - val_loss: 2.7920\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1389 - val_loss: 2.8289\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1258 - val_loss: 2.8273\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.1153 - val_loss: 2.7722\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1010 - val_loss: 2.8052\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1128 - val_loss: 2.8073\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1175 - val_loss: 2.8523\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1331 - val_loss: 2.8144\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1277 - val_loss: 2.8673\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1430 - val_loss: 2.8107\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0944 - val_loss: 2.7787\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 2.0948 - val_loss: 2.8306\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 37us/step - loss: 2.0953 - val_loss: 2.8260\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.1328 - val_loss: 2.7901\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0893 - val_loss: 2.8194\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0877 - val_loss: 2.8615\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1199 - val_loss: 2.8374\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0856 - val_loss: 2.8273\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0837 - val_loss: 2.8535\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1185 - val_loss: 2.9014\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0999 - val_loss: 2.8285\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0989 - val_loss: 2.8092\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0886 - val_loss: 2.8250\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1336 - val_loss: 2.8036\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0948 - val_loss: 2.8227\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.1195 - val_loss: 2.9275\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0893 - val_loss: 2.8049\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0802 - val_loss: 2.8942\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0980 - val_loss: 2.8243\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0906 - val_loss: 2.8968\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.1138 - val_loss: 2.8375\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0886 - val_loss: 2.8825\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0969 - val_loss: 2.9225\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0882 - val_loss: 2.9931\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0990 - val_loss: 2.9360\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0782 - val_loss: 2.7919\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0814 - val_loss: 2.9793\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.1070 - val_loss: 2.8365\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0654 - val_loss: 2.8331\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0711 - val_loss: 2.8748\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0566 - val_loss: 2.8238\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0713 - val_loss: 2.9413\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0672 - val_loss: 2.8150\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0661 - val_loss: 3.0087\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0588 - val_loss: 2.8006\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0738 - val_loss: 2.8023\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0522 - val_loss: 2.8965\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0817 - val_loss: 2.8756\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0787 - val_loss: 2.8395\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0641 - val_loss: 2.7962\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0458 - val_loss: 3.1282\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0959 - val_loss: 2.8315\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0476 - val_loss: 2.8354\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0489 - val_loss: 2.9191\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0947 - val_loss: 2.8316\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0531 - val_loss: 2.9262\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0610 - val_loss: 2.8270\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0620 - val_loss: 2.8067\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0480 - val_loss: 2.8048\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0516 - val_loss: 2.8767\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.1110 - val_loss: 2.8305\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0589 - val_loss: 2.9755\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0662 - val_loss: 2.8974\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0372 - val_loss: 2.9119\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0665 - val_loss: 2.8781\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0746 - val_loss: 2.9093\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0528 - val_loss: 2.8254\n",
      "Epoch 453/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0389 - val_loss: 2.8501\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0336 - val_loss: 2.8663\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0533 - val_loss: 2.8621\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0531 - val_loss: 2.8832\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0842 - val_loss: 2.8271\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0302 - val_loss: 2.8591\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0255 - val_loss: 2.8787\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 2.0473 - val_loss: 2.8514\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0333 - val_loss: 2.9243\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - ETA: 0s - loss: 2.058 - 0s 28us/step - loss: 2.0374 - val_loss: 2.9571\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0457 - val_loss: 2.8247\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0364 - val_loss: 2.8833\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0243 - val_loss: 2.8388\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 2.0203 - val_loss: 2.9864\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0265 - val_loss: 2.8494\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0110 - val_loss: 2.9206\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0568 - val_loss: 2.8617\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0278 - val_loss: 2.8772\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0122 - val_loss: 2.9113\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0420 - val_loss: 2.7974\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0088 - val_loss: 2.8992\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 2.0416 - val_loss: 2.8900\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0243 - val_loss: 2.8614\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 2.0167 - val_loss: 2.8800\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0329 - val_loss: 2.8808\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0185 - val_loss: 2.8232\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0346 - val_loss: 2.9931\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0787 - val_loss: 2.9110\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0449 - val_loss: 2.9252\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0258 - val_loss: 2.8598\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0253 - val_loss: 2.8457\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0076 - val_loss: 2.8306\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 2.0153 - val_loss: 2.8581\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9943 - val_loss: 2.9083\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0510 - val_loss: 2.8568\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0202 - val_loss: 2.9156\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9938 - val_loss: 2.8380\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9816 - val_loss: 2.8316\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0168 - val_loss: 2.9057\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9988 - val_loss: 2.8735\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9948 - val_loss: 2.8643\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0330 - val_loss: 2.8574\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9969 - val_loss: 2.9882\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0423 - val_loss: 2.8853\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 1.9882 - val_loss: 2.8524\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0109 - val_loss: 2.8753\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0016 - val_loss: 2.8649\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9838 - val_loss: 2.8767\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0014 - val_loss: 2.8501\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9941 - val_loss: 2.9700\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0366 - val_loss: 2.8495\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9926 - val_loss: 2.9785\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 2.0135 - val_loss: 2.8710\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9939 - val_loss: 2.8791\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9949 - val_loss: 2.8635\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0066 - val_loss: 2.9069\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9963 - val_loss: 2.8640\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9618 - val_loss: 2.9396\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9634 - val_loss: 2.8517\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9823 - val_loss: 2.9499\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 2.0295 - val_loss: 2.9391\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9844 - val_loss: 2.8615\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 1.9555 - val_loss: 2.9476\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 2.0029 - val_loss: 2.8937\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9841 - val_loss: 2.9074\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9798 - val_loss: 2.8671\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9613 - val_loss: 2.8928\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9763 - val_loss: 2.9099\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9674 - val_loss: 2.8749\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9520 - val_loss: 2.8832\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9479 - val_loss: 2.9035\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9535 - val_loss: 2.8511\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9712 - val_loss: 2.8728\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.0152 - val_loss: 2.8713\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9893 - val_loss: 2.8995\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9862 - val_loss: 2.8794\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9842 - val_loss: 2.9919\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9670 - val_loss: 2.8672\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9585 - val_loss: 2.9334\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9844 - val_loss: 3.0559\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 42us/step - loss: 1.9802 - val_loss: 2.8767\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 34us/step - loss: 1.9353 - val_loss: 2.8904\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9355 - val_loss: 2.8522\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 39us/step - loss: 1.9670 - val_loss: 3.1208\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0250 - val_loss: 3.0378\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.0072 - val_loss: 2.8649\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9794 - val_loss: 2.8955\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9555 - val_loss: 2.9540\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9956 - val_loss: 2.9736\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9493 - val_loss: 2.9005\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9769 - val_loss: 2.8898\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9345 - val_loss: 2.8441\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9444 - val_loss: 2.8576\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9816 - val_loss: 2.9143\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9828 - val_loss: 2.9056\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9478 - val_loss: 2.9299\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9607 - val_loss: 2.9581\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9713 - val_loss: 2.8754\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 1.9324 - val_loss: 2.9161\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9386 - val_loss: 2.9388\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9615 - val_loss: 2.8813\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9485 - val_loss: 2.9091\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9616 - val_loss: 2.9683\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9764 - val_loss: 2.8767\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9488 - val_loss: 2.9028\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9270 - val_loss: 2.9432\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9381 - val_loss: 2.9562\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9747 - val_loss: 2.8644\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9523 - val_loss: 2.8709\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9281 - val_loss: 2.9363\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9105 - val_loss: 2.9264\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9169 - val_loss: 2.8834\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9406 - val_loss: 2.9641\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9526 - val_loss: 2.9640\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9543 - val_loss: 2.9493\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9709 - val_loss: 2.8858\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9409 - val_loss: 2.8762\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9202 - val_loss: 2.8750\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9207 - val_loss: 2.9193\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9197 - val_loss: 2.9366\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9437 - val_loss: 2.9033\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9491 - val_loss: 2.8707\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9220 - val_loss: 2.8997\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9142 - val_loss: 2.9697\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9294 - val_loss: 2.8837\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9190 - val_loss: 2.8549\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9036 - val_loss: 2.9139\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 1.9552 - val_loss: 2.8700\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9480 - val_loss: 2.9579\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.9141 - val_loss: 2.9866\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 33us/step - loss: 1.9404 - val_loss: 2.9637\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9419 - val_loss: 2.8677\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9033 - val_loss: 3.0773\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.0045 - val_loss: 2.9272\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9057 - val_loss: 2.8532\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 1.9018 - val_loss: 2.9437\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 1.8991 - val_loss: 2.8888\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 1.9251 - val_loss: 2.9157\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9287 - val_loss: 2.8766\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8980 - val_loss: 2.9435\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.9253 - val_loss: 2.9181\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9115 - val_loss: 2.8814\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.8974 - val_loss: 2.9122\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.8939 - val_loss: 2.9133\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9399 - val_loss: 2.8794\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 1.8973 - val_loss: 2.9370\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 1.9053 - val_loss: 2.8817\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 1.9137 - val_loss: 2.9197\n",
      "Epoch 1/600\n",
      "1295/1295 [==============================] - 1s 488us/step - loss: 47.7182\n",
      "Epoch 2/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 20.7530\n",
      "Epoch 3/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 12.8866\n",
      "Epoch 4/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 8.6787\n",
      "Epoch 5/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 8.1861\n",
      "Epoch 6/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 7.6642\n",
      "Epoch 7/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 6.9898\n",
      "Epoch 8/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 6.2592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 5.5715\n",
      "Epoch 10/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 5.0258\n",
      "Epoch 11/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 4.5912\n",
      "Epoch 12/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 4.2455\n",
      "Epoch 13/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 3.9945\n",
      "Epoch 14/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 3.8027\n",
      "Epoch 15/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 3.6540\n",
      "Epoch 16/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 3.5291\n",
      "Epoch 17/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 3.3942\n",
      "Epoch 18/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 3.3205\n",
      "Epoch 19/600\n",
      "1295/1295 [==============================] - 0s 30us/step - loss: 3.2231\n",
      "Epoch 20/600\n",
      "1295/1295 [==============================] - 0s 31us/step - loss: 3.1717\n",
      "Epoch 21/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 3.1114\n",
      "Epoch 22/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 3.0678\n",
      "Epoch 23/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 3.0400\n",
      "Epoch 24/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 3.0111\n",
      "Epoch 25/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.9797\n",
      "Epoch 26/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.9850\n",
      "Epoch 27/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.9669\n",
      "Epoch 28/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.9210\n",
      "Epoch 29/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.9791\n",
      "Epoch 30/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.9421\n",
      "Epoch 31/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.9206\n",
      "Epoch 32/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.9273\n",
      "Epoch 33/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.8765\n",
      "Epoch 34/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.8849\n",
      "Epoch 35/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.8777\n",
      "Epoch 36/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.8463\n",
      "Epoch 37/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.8674\n",
      "Epoch 38/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.8118\n",
      "Epoch 39/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.8557\n",
      "Epoch 40/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.8139\n",
      "Epoch 41/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7840\n",
      "Epoch 42/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.7828\n",
      "Epoch 43/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.8325\n",
      "Epoch 44/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.7881\n",
      "Epoch 45/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.7809\n",
      "Epoch 46/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7675\n",
      "Epoch 47/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7880\n",
      "Epoch 48/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7941\n",
      "Epoch 49/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.7590\n",
      "Epoch 50/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7482\n",
      "Epoch 51/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.7360\n",
      "Epoch 52/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7132\n",
      "Epoch 53/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.7113\n",
      "Epoch 54/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7330\n",
      "Epoch 55/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7198\n",
      "Epoch 56/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6835\n",
      "Epoch 57/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.7072\n",
      "Epoch 58/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7180\n",
      "Epoch 59/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.7230\n",
      "Epoch 60/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.6880\n",
      "Epoch 61/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.6865\n",
      "Epoch 62/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6947\n",
      "Epoch 63/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.6684\n",
      "Epoch 64/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.6715\n",
      "Epoch 65/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.6864\n",
      "Epoch 66/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.6712\n",
      "Epoch 67/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.6819\n",
      "Epoch 68/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.6527\n",
      "Epoch 69/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6578\n",
      "Epoch 70/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6315\n",
      "Epoch 71/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6446\n",
      "Epoch 72/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6394\n",
      "Epoch 73/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6451\n",
      "Epoch 74/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6303\n",
      "Epoch 75/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.6403\n",
      "Epoch 76/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.6152\n",
      "Epoch 77/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.6284\n",
      "Epoch 78/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.6252\n",
      "Epoch 79/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.6506\n",
      "Epoch 80/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.6506\n",
      "Epoch 81/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.6092\n",
      "Epoch 82/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.6183\n",
      "Epoch 83/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5873\n",
      "Epoch 84/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.5971\n",
      "Epoch 85/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5782\n",
      "Epoch 86/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.5879\n",
      "Epoch 87/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.6172\n",
      "Epoch 88/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.5928\n",
      "Epoch 89/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6376\n",
      "Epoch 90/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.5985\n",
      "Epoch 91/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.5800\n",
      "Epoch 92/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.5841\n",
      "Epoch 93/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5582\n",
      "Epoch 94/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.5775\n",
      "Epoch 95/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5664\n",
      "Epoch 96/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.5947\n",
      "Epoch 97/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5834\n",
      "Epoch 98/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.5682\n",
      "Epoch 99/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.5741\n",
      "Epoch 100/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.5651\n",
      "Epoch 101/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5630\n",
      "Epoch 102/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.5355\n",
      "Epoch 103/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.5592\n",
      "Epoch 104/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.5371\n",
      "Epoch 105/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5389\n",
      "Epoch 106/600\n",
      "1295/1295 [==============================] - 0s 31us/step - loss: 2.5341\n",
      "Epoch 107/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.5137\n",
      "Epoch 108/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5128\n",
      "Epoch 109/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.5225\n",
      "Epoch 110/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.5323\n",
      "Epoch 111/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.5312\n",
      "Epoch 112/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.5121\n",
      "Epoch 113/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.5169\n",
      "Epoch 114/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.5090\n",
      "Epoch 115/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.5461\n",
      "Epoch 116/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.5101\n",
      "Epoch 117/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.5294\n",
      "Epoch 118/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.5207\n",
      "Epoch 119/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.5422\n",
      "Epoch 120/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5160\n",
      "Epoch 121/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4921\n",
      "Epoch 122/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5145\n",
      "Epoch 123/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.5026\n",
      "Epoch 124/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4978\n",
      "Epoch 125/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.4921\n",
      "Epoch 126/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4832\n",
      "Epoch 127/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.5014\n",
      "Epoch 128/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4782\n",
      "Epoch 129/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4899\n",
      "Epoch 130/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4950\n",
      "Epoch 131/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4965\n",
      "Epoch 132/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4752\n",
      "Epoch 133/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4996\n",
      "Epoch 134/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4814\n",
      "Epoch 135/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4871\n",
      "Epoch 136/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4831\n",
      "Epoch 137/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.4574\n",
      "Epoch 138/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.4859\n",
      "Epoch 139/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4770\n",
      "Epoch 140/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.4607\n",
      "Epoch 141/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4742\n",
      "Epoch 142/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4584\n",
      "Epoch 143/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.4808\n",
      "Epoch 144/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.4595\n",
      "Epoch 145/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.4492\n",
      "Epoch 146/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4594\n",
      "Epoch 147/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4521\n",
      "Epoch 148/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.4566\n",
      "Epoch 149/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.4469\n",
      "Epoch 150/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4593\n",
      "Epoch 151/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4379\n",
      "Epoch 152/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.4356\n",
      "Epoch 153/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4507\n",
      "Epoch 154/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.4335\n",
      "Epoch 155/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4173\n",
      "Epoch 156/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.4342\n",
      "Epoch 157/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4225\n",
      "Epoch 158/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.4228\n",
      "Epoch 159/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.4482\n",
      "Epoch 160/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.4295\n",
      "Epoch 161/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.4404\n",
      "Epoch 162/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.4385\n",
      "Epoch 163/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.4222\n",
      "Epoch 164/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.4434\n",
      "Epoch 165/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.4496\n",
      "Epoch 166/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.4154\n",
      "Epoch 167/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.3984\n",
      "Epoch 168/600\n",
      "1295/1295 [==============================] - 0s 30us/step - loss: 2.4000\n",
      "Epoch 169/600\n",
      "1295/1295 [==============================] - 0s 31us/step - loss: 2.4186\n",
      "Epoch 170/600\n",
      "1295/1295 [==============================] - 0s 32us/step - loss: 2.4386\n",
      "Epoch 171/600\n",
      "1295/1295 [==============================] - 0s 30us/step - loss: 2.4181\n",
      "Epoch 172/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.4141\n",
      "Epoch 173/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.3867\n",
      "Epoch 174/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.4136\n",
      "Epoch 175/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.4311\n",
      "Epoch 176/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3943\n",
      "Epoch 177/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.4116\n",
      "Epoch 178/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.4008\n",
      "Epoch 179/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.4301\n",
      "Epoch 180/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3997\n",
      "Epoch 181/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.4030\n",
      "Epoch 182/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3847\n",
      "Epoch 183/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3984\n",
      "Epoch 184/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.4088\n",
      "Epoch 185/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.3934\n",
      "Epoch 186/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.3753\n",
      "Epoch 187/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3713\n",
      "Epoch 188/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.3744\n",
      "Epoch 189/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.3676\n",
      "Epoch 190/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3959\n",
      "Epoch 191/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.4078\n",
      "Epoch 192/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3984\n",
      "Epoch 193/600\n",
      "1295/1295 [==============================] - 0s 31us/step - loss: 2.3718\n",
      "Epoch 194/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.3901\n",
      "Epoch 195/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.3678\n",
      "Epoch 196/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3556\n",
      "Epoch 197/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3805\n",
      "Epoch 198/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3788\n",
      "Epoch 199/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.3726\n",
      "Epoch 200/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3690\n",
      "Epoch 201/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.3688\n",
      "Epoch 202/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.3431\n",
      "Epoch 203/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3487\n",
      "Epoch 204/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3442\n",
      "Epoch 205/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3482\n",
      "Epoch 206/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3519\n",
      "Epoch 207/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3422\n",
      "Epoch 208/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.3413\n",
      "Epoch 209/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3796\n",
      "Epoch 210/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.3457\n",
      "Epoch 211/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.3573\n",
      "Epoch 212/600\n",
      "1295/1295 [==============================] - 0s 33us/step - loss: 2.3779\n",
      "Epoch 213/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.3435\n",
      "Epoch 214/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3807\n",
      "Epoch 215/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3579\n",
      "Epoch 216/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.3411\n",
      "Epoch 217/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.3400\n",
      "Epoch 218/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3379\n",
      "Epoch 219/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.3929\n",
      "Epoch 220/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3324\n",
      "Epoch 221/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.3643\n",
      "Epoch 222/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3412\n",
      "Epoch 223/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3578\n",
      "Epoch 224/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3018\n",
      "Epoch 225/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3122\n",
      "Epoch 226/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3183\n",
      "Epoch 227/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3339\n",
      "Epoch 228/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3451\n",
      "Epoch 229/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3406\n",
      "Epoch 230/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3291\n",
      "Epoch 231/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3177\n",
      "Epoch 232/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.3396\n",
      "Epoch 233/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.3196\n",
      "Epoch 234/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.3089\n",
      "Epoch 235/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2972\n",
      "Epoch 236/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.3079\n",
      "Epoch 237/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3222\n",
      "Epoch 238/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3003\n",
      "Epoch 239/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3394\n",
      "Epoch 240/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.3265\n",
      "Epoch 241/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3214\n",
      "Epoch 242/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.3085\n",
      "Epoch 243/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.2931\n",
      "Epoch 244/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3411\n",
      "Epoch 245/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3011\n",
      "Epoch 246/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3025\n",
      "Epoch 247/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2936\n",
      "Epoch 248/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.3061\n",
      "Epoch 249/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.3079\n",
      "Epoch 250/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3054\n",
      "Epoch 251/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2784\n",
      "Epoch 252/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2901\n",
      "Epoch 253/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3144\n",
      "Epoch 254/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2883\n",
      "Epoch 255/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.3257\n",
      "Epoch 256/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2804\n",
      "Epoch 257/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.3060\n",
      "Epoch 258/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2807\n",
      "Epoch 259/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2867\n",
      "Epoch 260/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.2662\n",
      "Epoch 261/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.2649\n",
      "Epoch 262/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2890\n",
      "Epoch 263/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2863\n",
      "Epoch 264/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2632\n",
      "Epoch 265/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2814\n",
      "Epoch 266/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2921\n",
      "Epoch 267/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2815\n",
      "Epoch 268/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2873\n",
      "Epoch 269/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2851\n",
      "Epoch 270/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2874\n",
      "Epoch 271/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.2731\n",
      "Epoch 272/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2958\n",
      "Epoch 273/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2701\n",
      "Epoch 274/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2457\n",
      "Epoch 275/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2703\n",
      "Epoch 276/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2661\n",
      "Epoch 277/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2682\n",
      "Epoch 278/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2351\n",
      "Epoch 279/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2501\n",
      "Epoch 280/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2695\n",
      "Epoch 281/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2566\n",
      "Epoch 282/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2426\n",
      "Epoch 283/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2419\n",
      "Epoch 284/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2623\n",
      "Epoch 285/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2297\n",
      "Epoch 286/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2541\n",
      "Epoch 287/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.2642\n",
      "Epoch 288/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2462\n",
      "Epoch 289/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.2557\n",
      "Epoch 290/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.2421\n",
      "Epoch 291/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2619\n",
      "Epoch 292/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.2320\n",
      "Epoch 293/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.2179\n",
      "Epoch 294/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2578\n",
      "Epoch 295/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2568\n",
      "Epoch 296/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1984\n",
      "Epoch 297/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2626\n",
      "Epoch 298/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.2385\n",
      "Epoch 299/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2357\n",
      "Epoch 300/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2289\n",
      "Epoch 301/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2328\n",
      "Epoch 302/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2325\n",
      "Epoch 303/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2089\n",
      "Epoch 304/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.2191\n",
      "Epoch 305/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2075\n",
      "Epoch 306/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.2034\n",
      "Epoch 307/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.2301\n",
      "Epoch 308/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.2066\n",
      "Epoch 309/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2221\n",
      "Epoch 310/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2229\n",
      "Epoch 311/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2038\n",
      "Epoch 312/600\n",
      "1295/1295 [==============================] - 0s 32us/step - loss: 2.2389\n",
      "Epoch 313/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.2159\n",
      "Epoch 314/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2263\n",
      "Epoch 315/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2003\n",
      "Epoch 316/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.2053\n",
      "Epoch 317/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2240\n",
      "Epoch 318/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2029\n",
      "Epoch 319/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2053\n",
      "Epoch 320/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2160\n",
      "Epoch 321/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1967\n",
      "Epoch 322/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1926\n",
      "Epoch 323/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2011\n",
      "Epoch 324/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1945\n",
      "Epoch 325/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2024\n",
      "Epoch 326/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2091\n",
      "Epoch 327/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1895\n",
      "Epoch 328/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2021\n",
      "Epoch 329/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2269\n",
      "Epoch 330/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1731\n",
      "Epoch 331/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1778\n",
      "Epoch 332/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1815\n",
      "Epoch 333/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.2041\n",
      "Epoch 334/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.2023\n",
      "Epoch 335/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1783\n",
      "Epoch 336/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1981\n",
      "Epoch 337/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.2056\n",
      "Epoch 338/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2031\n",
      "Epoch 339/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1844\n",
      "Epoch 340/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1550\n",
      "Epoch 341/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1672\n",
      "Epoch 342/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1771\n",
      "Epoch 343/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1787\n",
      "Epoch 344/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.2035\n",
      "Epoch 345/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1764\n",
      "Epoch 346/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1716\n",
      "Epoch 347/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.1745\n",
      "Epoch 348/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.2101\n",
      "Epoch 349/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1627\n",
      "Epoch 350/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1544\n",
      "Epoch 351/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1660\n",
      "Epoch 352/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.1706\n",
      "Epoch 353/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1486\n",
      "Epoch 354/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1787\n",
      "Epoch 355/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.1589\n",
      "Epoch 356/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1583\n",
      "Epoch 357/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1571\n",
      "Epoch 358/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1843\n",
      "Epoch 359/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1617\n",
      "Epoch 360/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.1859\n",
      "Epoch 361/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1585\n",
      "Epoch 362/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.2251\n",
      "Epoch 363/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1963\n",
      "Epoch 364/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1489\n",
      "Epoch 365/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1640\n",
      "Epoch 366/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.1655\n",
      "Epoch 367/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1739\n",
      "Epoch 368/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1536\n",
      "Epoch 369/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1381\n",
      "Epoch 370/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.1757\n",
      "Epoch 371/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.1365\n",
      "Epoch 372/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1354\n",
      "Epoch 373/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1413\n",
      "Epoch 374/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.1347\n",
      "Epoch 375/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.1292\n",
      "Epoch 376/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1505\n",
      "Epoch 377/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1512\n",
      "Epoch 378/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1583\n",
      "Epoch 379/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1493\n",
      "Epoch 380/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.1442\n",
      "Epoch 381/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.1421\n",
      "Epoch 382/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1572\n",
      "Epoch 383/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.1537\n",
      "Epoch 384/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.1225\n",
      "Epoch 385/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.1446\n",
      "Epoch 386/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.1505\n",
      "Epoch 387/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.1324\n",
      "Epoch 388/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.1196\n",
      "Epoch 389/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.1347\n",
      "Epoch 390/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.1434\n",
      "Epoch 391/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.1085\n",
      "Epoch 392/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1155\n",
      "Epoch 393/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0995\n",
      "Epoch 394/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.1112\n",
      "Epoch 395/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1261\n",
      "Epoch 396/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0967\n",
      "Epoch 397/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1211\n",
      "Epoch 398/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1435\n",
      "Epoch 399/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1315\n",
      "Epoch 400/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1106\n",
      "Epoch 401/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.1244\n",
      "Epoch 402/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.1244\n",
      "Epoch 403/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.1282\n",
      "Epoch 404/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1294\n",
      "Epoch 405/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0940\n",
      "Epoch 406/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1159\n",
      "Epoch 407/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.1039\n",
      "Epoch 408/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1380\n",
      "Epoch 409/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1316\n",
      "Epoch 410/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1290\n",
      "Epoch 411/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0979\n",
      "Epoch 412/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1493\n",
      "Epoch 413/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1080\n",
      "Epoch 414/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0867\n",
      "Epoch 415/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0968\n",
      "Epoch 416/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1158\n",
      "Epoch 417/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0867\n",
      "Epoch 418/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0822\n",
      "Epoch 419/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.1000\n",
      "Epoch 420/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1012\n",
      "Epoch 421/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.1011\n",
      "Epoch 422/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1085\n",
      "Epoch 423/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0826\n",
      "Epoch 424/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0918\n",
      "Epoch 425/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.1194\n",
      "Epoch 426/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0948\n",
      "Epoch 427/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0864\n",
      "Epoch 428/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0673\n",
      "Epoch 429/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0641\n",
      "Epoch 430/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.1052\n",
      "Epoch 431/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0519\n",
      "Epoch 432/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.0913\n",
      "Epoch 433/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0692\n",
      "Epoch 434/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0663\n",
      "Epoch 435/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0668\n",
      "Epoch 436/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0664\n",
      "Epoch 437/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0920\n",
      "Epoch 438/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0890\n",
      "Epoch 439/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.0834\n",
      "Epoch 440/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.0862\n",
      "Epoch 441/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0980\n",
      "Epoch 442/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0500\n",
      "Epoch 443/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0736\n",
      "Epoch 444/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0569\n",
      "Epoch 445/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.0935\n",
      "Epoch 446/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0514\n",
      "Epoch 447/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0816\n",
      "Epoch 448/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0530\n",
      "Epoch 449/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0663\n",
      "Epoch 450/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0670\n",
      "Epoch 451/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0837\n",
      "Epoch 452/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0389\n",
      "Epoch 453/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0661\n",
      "Epoch 454/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0474\n",
      "Epoch 455/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0589\n",
      "Epoch 456/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0601\n",
      "Epoch 457/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0562\n",
      "Epoch 458/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0771\n",
      "Epoch 459/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0475\n",
      "Epoch 460/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.0522\n",
      "Epoch 461/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0766\n",
      "Epoch 462/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0833\n",
      "Epoch 463/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0895\n",
      "Epoch 464/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.0489\n",
      "Epoch 465/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0526\n",
      "Epoch 466/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0620\n",
      "Epoch 467/600\n",
      "1295/1295 [==============================] - 0s 30us/step - loss: 2.0331\n",
      "Epoch 468/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.0944\n",
      "Epoch 469/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0342\n",
      "Epoch 470/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.0382\n",
      "Epoch 471/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0617\n",
      "Epoch 472/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0660\n",
      "Epoch 473/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.0532\n",
      "Epoch 474/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0581\n",
      "Epoch 475/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0543\n",
      "Epoch 476/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0372\n",
      "Epoch 477/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0426\n",
      "Epoch 478/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0688\n",
      "Epoch 479/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0516\n",
      "Epoch 480/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0397\n",
      "Epoch 481/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0643\n",
      "Epoch 482/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0749\n",
      "Epoch 483/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0447\n",
      "Epoch 484/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0195\n",
      "Epoch 485/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.0700\n",
      "Epoch 486/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0729\n",
      "Epoch 487/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0297\n",
      "Epoch 488/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0290\n",
      "Epoch 489/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0102\n",
      "Epoch 490/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0285\n",
      "Epoch 491/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0415\n",
      "Epoch 492/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0356\n",
      "Epoch 493/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0354\n",
      "Epoch 494/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0378\n",
      "Epoch 495/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0147\n",
      "Epoch 496/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0206\n",
      "Epoch 497/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0087\n",
      "Epoch 498/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0221\n",
      "Epoch 499/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0139\n",
      "Epoch 500/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0309\n",
      "Epoch 501/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0449\n",
      "Epoch 502/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0029\n",
      "Epoch 503/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0321\n",
      "Epoch 504/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0022\n",
      "Epoch 505/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.0344\n",
      "Epoch 506/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0599\n",
      "Epoch 507/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0292\n",
      "Epoch 508/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 1.9976\n",
      "Epoch 509/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0168\n",
      "Epoch 510/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0412\n",
      "Epoch 511/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0158\n",
      "Epoch 512/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0006\n",
      "Epoch 513/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0483\n",
      "Epoch 514/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 1.9752\n",
      "Epoch 515/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0217\n",
      "Epoch 516/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0242\n",
      "Epoch 517/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0204\n",
      "Epoch 518/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.0035\n",
      "Epoch 519/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.0213\n",
      "Epoch 520/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0016\n",
      "Epoch 521/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9963\n",
      "Epoch 522/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 1.9771\n",
      "Epoch 523/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0173\n",
      "Epoch 524/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0013\n",
      "Epoch 525/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0091\n",
      "Epoch 526/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9967\n",
      "Epoch 527/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0121\n",
      "Epoch 528/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0013\n",
      "Epoch 529/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.0141\n",
      "Epoch 530/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9640\n",
      "Epoch 531/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9995\n",
      "Epoch 532/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.0095\n",
      "Epoch 533/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 1.9866\n",
      "Epoch 534/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.0016\n",
      "Epoch 535/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 1.9691\n",
      "Epoch 536/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 1.9870\n",
      "Epoch 537/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 1.9801\n",
      "Epoch 538/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9818\n",
      "Epoch 539/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9673\n",
      "Epoch 540/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.0199\n",
      "Epoch 541/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.0143\n",
      "Epoch 542/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 1.9708\n",
      "Epoch 543/600\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 1.9810\n",
      "Epoch 544/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 1.9740\n",
      "Epoch 545/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 1.9476\n",
      "Epoch 546/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9877\n",
      "Epoch 547/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 1.9831\n",
      "Epoch 548/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9618\n",
      "Epoch 549/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9857\n",
      "Epoch 550/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.0008\n",
      "Epoch 551/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0015\n",
      "Epoch 552/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9780\n",
      "Epoch 553/600\n",
      "1295/1295 [==============================] - 0s 31us/step - loss: 1.9823\n",
      "Epoch 554/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 1.9344\n",
      "Epoch 555/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9464\n",
      "Epoch 556/600\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 1.9597\n",
      "Epoch 557/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 2.0034\n",
      "Epoch 558/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9673\n",
      "Epoch 559/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 1.9537\n",
      "Epoch 560/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9431\n",
      "Epoch 561/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9563\n",
      "Epoch 562/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9936\n",
      "Epoch 563/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.0282\n",
      "Epoch 564/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9554\n",
      "Epoch 565/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 1.9309\n",
      "Epoch 566/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.0270\n",
      "Epoch 567/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9739\n",
      "Epoch 568/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 1.9553\n",
      "Epoch 569/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9377\n",
      "Epoch 570/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9711\n",
      "Epoch 571/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 1.9657\n",
      "Epoch 572/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9465\n",
      "Epoch 573/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 1.9392\n",
      "Epoch 574/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9466\n",
      "Epoch 575/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9496\n",
      "Epoch 576/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.0122\n",
      "Epoch 577/600\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 1.9629\n",
      "Epoch 578/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9502\n",
      "Epoch 579/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9354\n",
      "Epoch 580/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9657\n",
      "Epoch 581/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9448\n",
      "Epoch 582/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 1.9618\n",
      "Epoch 583/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9471\n",
      "Epoch 584/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9462\n",
      "Epoch 585/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9612\n",
      "Epoch 586/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9658\n",
      "Epoch 587/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9597\n",
      "Epoch 588/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9362\n",
      "Epoch 589/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9454\n",
      "Epoch 590/600\n",
      "1295/1295 [==============================] - 0s 28us/step - loss: 1.9417\n",
      "Epoch 591/600\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 1.9124\n",
      "Epoch 592/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9580\n",
      "Epoch 593/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9472\n",
      "Epoch 594/600\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 1.9146\n",
      "Epoch 595/600\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 1.9547\n",
      "Epoch 596/600\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 1.9078\n",
      "Epoch 597/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9433\n",
      "Epoch 598/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9468\n",
      "Epoch 599/600\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 1.9643\n",
      "Epoch 600/600\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 1.9077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [47.71822115629336,\n",
       "  20.752952634597836,\n",
       "  12.886550038017361,\n",
       "  8.678712553959556,\n",
       "  8.186094571264555,\n",
       "  7.664249768128267,\n",
       "  6.98977111573385,\n",
       "  6.259163906215241,\n",
       "  5.571497241502563,\n",
       "  5.025753621429089,\n",
       "  4.591214375146107,\n",
       "  4.245512371357804,\n",
       "  3.9944746733632326,\n",
       "  3.80270427902693,\n",
       "  3.653968800909271,\n",
       "  3.529076778750622,\n",
       "  3.394178450337708,\n",
       "  3.320511351220856,\n",
       "  3.2231376097469258,\n",
       "  3.1716814188423306,\n",
       "  3.111392694090324,\n",
       "  3.0677951339589122,\n",
       "  3.0400381060640784,\n",
       "  3.0111432185964695,\n",
       "  2.9796638359894625,\n",
       "  2.9850314556401667,\n",
       "  2.96694633859465,\n",
       "  2.921040579158827,\n",
       "  2.9790591889826947,\n",
       "  2.942086970944202,\n",
       "  2.920601294307635,\n",
       "  2.927325275413778,\n",
       "  2.876489597858149,\n",
       "  2.8848930758398934,\n",
       "  2.877695959047001,\n",
       "  2.8463409812293916,\n",
       "  2.867390535052679,\n",
       "  2.8118120546966905,\n",
       "  2.855732456597582,\n",
       "  2.813886986736165,\n",
       "  2.7839913009216426,\n",
       "  2.782824843086331,\n",
       "  2.8324673010115458,\n",
       "  2.7880777410558752,\n",
       "  2.7808937161125273,\n",
       "  2.767457884711188,\n",
       "  2.7879894912012757,\n",
       "  2.794132324719521,\n",
       "  2.7589699918238813,\n",
       "  2.748210404370282,\n",
       "  2.736038593712,\n",
       "  2.713178792975584,\n",
       "  2.7113033316770574,\n",
       "  2.7329929040665792,\n",
       "  2.7197525887875944,\n",
       "  2.683522504268926,\n",
       "  2.7071981466875115,\n",
       "  2.718044647378811,\n",
       "  2.723044453440486,\n",
       "  2.6880112679308446,\n",
       "  2.6864515713282993,\n",
       "  2.6946789141327256,\n",
       "  2.6684184497847983,\n",
       "  2.6715081133897702,\n",
       "  2.6863561836448877,\n",
       "  2.671228030473569,\n",
       "  2.681865223586329,\n",
       "  2.652722563062395,\n",
       "  2.6577913540210503,\n",
       "  2.631508516989159,\n",
       "  2.644600812072459,\n",
       "  2.639399002877902,\n",
       "  2.6450833003953615,\n",
       "  2.6302974932902567,\n",
       "  2.6403407801992644,\n",
       "  2.6151529904958366,\n",
       "  2.628377021509708,\n",
       "  2.6252403461795057,\n",
       "  2.6506126184721253,\n",
       "  2.6505985260009766,\n",
       "  2.6092225373021423,\n",
       "  2.618276629208598,\n",
       "  2.587327580654483,\n",
       "  2.5970598058811025,\n",
       "  2.5782122326633643,\n",
       "  2.587861836186707,\n",
       "  2.617209764060827,\n",
       "  2.592750202274691,\n",
       "  2.637610237570803,\n",
       "  2.5984805891412566,\n",
       "  2.579971232469478,\n",
       "  2.584139825754644,\n",
       "  2.5581859051030573,\n",
       "  2.577541343033544,\n",
       "  2.5663956583236636,\n",
       "  2.5946721634809573,\n",
       "  2.5833538959385347,\n",
       "  2.5681880928834895,\n",
       "  2.5740906492623585,\n",
       "  2.565134201270733,\n",
       "  2.5629654814377716,\n",
       "  2.535452858361498,\n",
       "  2.559212420437787,\n",
       "  2.5370751587120264,\n",
       "  2.5388569123036153,\n",
       "  2.534115138661447,\n",
       "  2.5136804746384787,\n",
       "  2.5127966293496975,\n",
       "  2.522536175591605,\n",
       "  2.532349443803883,\n",
       "  2.53120485696093,\n",
       "  2.5121141446603312,\n",
       "  2.516858453456039,\n",
       "  2.509013576397104,\n",
       "  2.5460565633295125,\n",
       "  2.510101542049393,\n",
       "  2.529363729778864,\n",
       "  2.520739494603573,\n",
       "  2.5422243004139786,\n",
       "  2.5160071656510636,\n",
       "  2.492114577974592,\n",
       "  2.514496813409577,\n",
       "  2.502646138769319,\n",
       "  2.497837850025722,\n",
       "  2.492097129232635,\n",
       "  2.4831945997407536,\n",
       "  2.5013590982061555,\n",
       "  2.4782199657101427,\n",
       "  2.489891290664673,\n",
       "  2.494984882678765,\n",
       "  2.4965289338675243,\n",
       "  2.475208381888489,\n",
       "  2.4995713804679487,\n",
       "  2.481384007626979,\n",
       "  2.4870952584108332,\n",
       "  2.4831349232942443,\n",
       "  2.4573922718813983,\n",
       "  2.485913910921016,\n",
       "  2.4769710942124767,\n",
       "  2.4606514409702256,\n",
       "  2.4741842378520595,\n",
       "  2.458377673358991,\n",
       "  2.4807987185518714,\n",
       "  2.4594756406246465,\n",
       "  2.4491742283220916,\n",
       "  2.4594224502681303,\n",
       "  2.452100724327058,\n",
       "  2.4566037295868037,\n",
       "  2.446909494841881,\n",
       "  2.4592991142199305,\n",
       "  2.4379268999725694,\n",
       "  2.4355825751904816,\n",
       "  2.4506536579500295,\n",
       "  2.4335079220731286,\n",
       "  2.4172692400148015,\n",
       "  2.4341755309160154,\n",
       "  2.4225007419880753,\n",
       "  2.422789779869286,\n",
       "  2.448249151347687,\n",
       "  2.4294806198723986,\n",
       "  2.4404449315605015,\n",
       "  2.438512131974504,\n",
       "  2.4222455273263703,\n",
       "  2.4434068267409867,\n",
       "  2.449599140859479,\n",
       "  2.4153786506431905,\n",
       "  2.398412324286796,\n",
       "  2.399951015199934,\n",
       "  2.4185787443948987,\n",
       "  2.4386266732307935,\n",
       "  2.4181016338377845,\n",
       "  2.414116701104006,\n",
       "  2.3866797220752964,\n",
       "  2.4136185931422998,\n",
       "  2.431131054535796,\n",
       "  2.394261742190505,\n",
       "  2.411594646777886,\n",
       "  2.400804414712324,\n",
       "  2.4301068161445234,\n",
       "  2.3997137196735987,\n",
       "  2.402989962846616,\n",
       "  2.3847250864772724,\n",
       "  2.39839971479762,\n",
       "  2.408778614058918,\n",
       "  2.393402082119209,\n",
       "  2.3752910538529797,\n",
       "  2.3712588120611477,\n",
       "  2.374435346559208,\n",
       "  2.367584348185182,\n",
       "  2.3959493710727764,\n",
       "  2.4077805045948986,\n",
       "  2.398430234216815,\n",
       "  2.371815620702206,\n",
       "  2.390145951716596,\n",
       "  2.3677821895790836,\n",
       "  2.355566844977007,\n",
       "  2.3804576498200993,\n",
       "  2.37879880791005,\n",
       "  2.3725857007457485,\n",
       "  2.368985484465669,\n",
       "  2.3688273908548836,\n",
       "  2.3431317732601094,\n",
       "  2.348686271652752,\n",
       "  2.344162386816901,\n",
       "  2.3482071460444036,\n",
       "  2.351858440973584,\n",
       "  2.3421691467402983,\n",
       "  2.341272158972545,\n",
       "  2.3795563254117047,\n",
       "  2.345689581167744,\n",
       "  2.3572607914913575,\n",
       "  2.3779256896162586,\n",
       "  2.343453323518908,\n",
       "  2.380737509966817,\n",
       "  2.3579391779586616,\n",
       "  2.3411370106185263,\n",
       "  2.3399918981500574,\n",
       "  2.337914754064847,\n",
       "  2.392868536319512,\n",
       "  2.332416007877777,\n",
       "  2.3643107533915164,\n",
       "  2.341185357119586,\n",
       "  2.3577632545044063,\n",
       "  2.301780263429443,\n",
       "  2.312209569349252,\n",
       "  2.3182644153653884,\n",
       "  2.333878927709513,\n",
       "  2.345065694057803,\n",
       "  2.3405608615359745,\n",
       "  2.3290715134742177,\n",
       "  2.3176714471868567,\n",
       "  2.3395552902148036,\n",
       "  2.3196274562231824,\n",
       "  2.308883542719955,\n",
       "  2.2971880445148956,\n",
       "  2.3079016954282077,\n",
       "  2.3222367901599545,\n",
       "  2.3003415868089006,\n",
       "  2.3394249525769797,\n",
       "  2.3264726441799444,\n",
       "  2.3213721076494016,\n",
       "  2.308480964204059,\n",
       "  2.2930670769518406,\n",
       "  2.3410627685458505,\n",
       "  2.3010740270946015,\n",
       "  2.302536443393663,\n",
       "  2.2936079529721765,\n",
       "  2.30606747225905,\n",
       "  2.3079278515112445,\n",
       "  2.305365324940921,\n",
       "  2.2784176913007346,\n",
       "  2.290075128142898,\n",
       "  2.3144133873427695,\n",
       "  2.28834745552549,\n",
       "  2.3256650210347414,\n",
       "  2.280446687720457,\n",
       "  2.3060033790853494,\n",
       "  2.280744114437619,\n",
       "  2.286699155122617,\n",
       "  2.2662096078791674,\n",
       "  2.2649073296992475,\n",
       "  2.288998323978144,\n",
       "  2.286263824890019,\n",
       "  2.2632441447048115,\n",
       "  2.281434593053398,\n",
       "  2.292139292683841,\n",
       "  2.2814912087208516,\n",
       "  2.287324052059512,\n",
       "  2.2851314282325244,\n",
       "  2.287391253880092,\n",
       "  2.2731378998995746,\n",
       "  2.295824373090589,\n",
       "  2.270141290421652,\n",
       "  2.245701405072304,\n",
       "  2.2703211307525635,\n",
       "  2.266097799691454,\n",
       "  2.2682372667614557,\n",
       "  2.235107422795535,\n",
       "  2.250056203267749,\n",
       "  2.269465754851411,\n",
       "  2.2565942933660676,\n",
       "  2.2426046082411952,\n",
       "  2.2418788335498236,\n",
       "  2.2622971396648746,\n",
       "  2.2296615245259406,\n",
       "  2.2541231519927387,\n",
       "  2.2642391243496456,\n",
       "  2.2461537845346458,\n",
       "  2.2557060120188592,\n",
       "  2.242134178927506,\n",
       "  2.26186390088792,\n",
       "  2.2320252900878437,\n",
       "  2.217850343601124,\n",
       "  2.2577516747257422,\n",
       "  2.256789664043883,\n",
       "  2.1984219569497125,\n",
       "  2.2626199482950926,\n",
       "  2.2384507738945567,\n",
       "  2.235709891816364,\n",
       "  2.2289143372686673,\n",
       "  2.2328204749633906,\n",
       "  2.2324773556477315,\n",
       "  2.2089322616694975,\n",
       "  2.219104407837032,\n",
       "  2.207509094223553,\n",
       "  2.2033813754564084,\n",
       "  2.230136621872891,\n",
       "  2.2066342701783053,\n",
       "  2.2221311929143073,\n",
       "  2.2228855875007896,\n",
       "  2.2038461029759704,\n",
       "  2.2389459812503065,\n",
       "  2.215879333525551,\n",
       "  2.2262564331408172,\n",
       "  2.2003401006971086,\n",
       "  2.205344142140569,\n",
       "  2.2239829527365194,\n",
       "  2.2029003609101285,\n",
       "  2.2052887305329665,\n",
       "  2.2159702197925464,\n",
       "  2.1966790029901335,\n",
       "  2.1925506260404255,\n",
       "  2.2010546991723845,\n",
       "  2.194529662261138,\n",
       "  2.202352372835962,\n",
       "  2.2091414394525946,\n",
       "  2.1894871709889885,\n",
       "  2.2020750533659945,\n",
       "  2.2269294455244735,\n",
       "  2.173139376990123,\n",
       "  2.177803394877312,\n",
       "  2.181524256481627,\n",
       "  2.204083233726531,\n",
       "  2.202315369628111,\n",
       "  2.1782699708312636,\n",
       "  2.1980656831881253,\n",
       "  2.205571326509866,\n",
       "  2.2031039372374193,\n",
       "  2.184372699398792,\n",
       "  2.1550145278105863,\n",
       "  2.1671896523950642,\n",
       "  2.1771123004235817,\n",
       "  2.1786928342576193,\n",
       "  2.2035109306394363,\n",
       "  2.17642609032885,\n",
       "  2.171612975219962,\n",
       "  2.174454942173019,\n",
       "  2.210147154377234,\n",
       "  2.1627460142820496,\n",
       "  2.1544009147923884,\n",
       "  2.1659860353212097,\n",
       "  2.1706445686605447,\n",
       "  2.1485924987719325,\n",
       "  2.1787448426471254,\n",
       "  2.158888422844493,\n",
       "  2.158322858994532,\n",
       "  2.1571138425223157,\n",
       "  2.1843486145196276,\n",
       "  2.161696818804649,\n",
       "  2.185919734041663,\n",
       "  2.158503382362454,\n",
       "  2.225090589302387,\n",
       "  2.1963003804784944,\n",
       "  2.1488795229827113,\n",
       "  2.164003047243509,\n",
       "  2.165524825166091,\n",
       "  2.173896225262793,\n",
       "  2.1536124037959863,\n",
       "  2.138059141092779,\n",
       "  2.1757218220979553,\n",
       "  2.1365377166556576,\n",
       "  2.1353678546817147,\n",
       "  2.141279899935925,\n",
       "  2.1347149035185,\n",
       "  2.1292381654835117,\n",
       "  2.150480887604496,\n",
       "  2.151249867148381,\n",
       "  2.158297519426088,\n",
       "  2.149317470756737,\n",
       "  2.144162277457337,\n",
       "  2.1420905737342983,\n",
       "  2.1572418360176235,\n",
       "  2.153690155868825,\n",
       "  2.1224583125022387,\n",
       "  2.14463108478826,\n",
       "  2.1505273308993305,\n",
       "  2.1324321947502813,\n",
       "  2.119581273163608,\n",
       "  2.1346659328946735,\n",
       "  2.143416354094693,\n",
       "  2.108542092518457,\n",
       "  2.115509110988337,\n",
       "  2.099548034226112,\n",
       "  2.111191921712809,\n",
       "  2.126106560460389,\n",
       "  2.096673112118106,\n",
       "  2.121052705182992,\n",
       "  2.1434997996768437,\n",
       "  2.1315002828030973,\n",
       "  2.110589125902036,\n",
       "  2.1243999381783385,\n",
       "  2.1244004702476,\n",
       "  2.1281558362673607,\n",
       "  2.1294446723341482,\n",
       "  2.093964198381284,\n",
       "  2.115871509529909,\n",
       "  2.103895940375604,\n",
       "  2.138025836134509,\n",
       "  2.1316434002290823,\n",
       "  2.129018186602353,\n",
       "  2.09787409922331,\n",
       "  2.149251039423998,\n",
       "  2.108048102110049,\n",
       "  2.0867292314882904,\n",
       "  2.0967738126695847,\n",
       "  2.115828905326519,\n",
       "  2.0867177027072685,\n",
       "  2.0821962227692477,\n",
       "  2.1000033594006275,\n",
       "  2.1012422329670675,\n",
       "  2.1011351158259917,\n",
       "  2.1084534109329165,\n",
       "  2.0826460238129014,\n",
       "  2.091762494396519,\n",
       "  2.119407271786546,\n",
       "  2.0947739240285514,\n",
       "  2.0863865802647066,\n",
       "  2.0673083154391136,\n",
       "  2.0640735409894964,\n",
       "  2.1052332513580914,\n",
       "  2.051898974709529,\n",
       "  2.0912615479649723,\n",
       "  2.069218177132625,\n",
       "  2.0662831298172706,\n",
       "  2.0668078186889414,\n",
       "  2.0664498474607136,\n",
       "  2.09198471570107,\n",
       "  2.088996284716838,\n",
       "  2.0833939327696576,\n",
       "  2.0862152272669965,\n",
       "  2.097980124609811,\n",
       "  2.0500400940884034,\n",
       "  2.0735520708975184,\n",
       "  2.056904206404815,\n",
       "  2.0934778364468727,\n",
       "  2.0513709143782215,\n",
       "  2.0816357826173997,\n",
       "  2.052997435381974,\n",
       "  2.0663430469836968,\n",
       "  2.067034236252538,\n",
       "  2.0836682724676536,\n",
       "  2.0389034278604514,\n",
       "  2.066079604579675,\n",
       "  2.047364939133633,\n",
       "  2.0588568306337454,\n",
       "  2.060137885877985,\n",
       "  2.0561585122554,\n",
       "  2.0770695402815536,\n",
       "  2.047450100593125,\n",
       "  2.0521963208798737,\n",
       "  2.0765787054673126,\n",
       "  2.0833303624598676,\n",
       "  2.0894561421456945,\n",
       "  2.048925596774775,\n",
       "  2.052624489810016,\n",
       "  2.061987251848788,\n",
       "  2.03307200674845,\n",
       "  2.094448112612986,\n",
       "  2.0341633415590383,\n",
       "  2.0382011875682817,\n",
       "  2.061694672669223,\n",
       "  2.0660000169599377,\n",
       "  2.053156770334281,\n",
       "  2.058057102457437,\n",
       "  2.054287659155356,\n",
       "  2.0371914858063214,\n",
       "  2.0426302112667716,\n",
       "  2.068754970336973,\n",
       "  2.0516138500228354,\n",
       "  2.039696214281914,\n",
       "  2.0642694386736307,\n",
       "  2.0749102789462763,\n",
       "  2.044737517603576,\n",
       "  2.0195163902628837,\n",
       "  2.069954250770186,\n",
       "  2.07285805190392,\n",
       "  2.029664543604759,\n",
       "  2.029039006435733,\n",
       "  2.0101523647897492,\n",
       "  2.0285441755788205,\n",
       "  2.041501906847862,\n",
       "  2.0355937526953265,\n",
       "  2.035371236359291,\n",
       "  2.037770294314646,\n",
       "  2.0147126977508134,\n",
       "  2.0205835127002025,\n",
       "  2.0087252795466126,\n",
       "  2.022104719891051,\n",
       "  2.0139324628248176,\n",
       "  2.030878164593317,\n",
       "  2.044870391315475,\n",
       "  2.002905192522469,\n",
       "  2.032086643012794,\n",
       "  2.0021834115724304,\n",
       "  2.034409343966186,\n",
       "  2.0599442964355,\n",
       "  2.029244937491693,\n",
       "  1.997559506920774,\n",
       "  2.0167802029134685,\n",
       "  2.0412200339512476,\n",
       "  2.015792699394079,\n",
       "  2.0006268806899374,\n",
       "  2.048291117988498,\n",
       "  1.9752270254849467,\n",
       "  2.021721865679767,\n",
       "  2.024199903701723,\n",
       "  2.020442278688939,\n",
       "  2.003547321415316,\n",
       "  2.021276294494688,\n",
       "  2.0016199391781133,\n",
       "  1.9962850933369523,\n",
       "  1.9770788953110978,\n",
       "  2.017282210261665,\n",
       "  2.0013457927924785,\n",
       "  2.0091365557379706,\n",
       "  1.996696440869777,\n",
       "  2.0120997005447916,\n",
       "  2.001286440374308,\n",
       "  2.0141158067121467,\n",
       "  1.964032846988398,\n",
       "  1.9995494744952582,\n",
       "  2.009457101232757,\n",
       "  1.9866268054859058,\n",
       "  2.001576296611182,\n",
       "  1.9690949309286463,\n",
       "  1.9870050165183757,\n",
       "  1.9800742754144558,\n",
       "  1.9818464117160635,\n",
       "  1.9673273673849216,\n",
       "  2.0198558568954468,\n",
       "  2.0143172248450023,\n",
       "  1.9708179387346658,\n",
       "  1.9810046376408756,\n",
       "  1.9740492197537514,\n",
       "  1.9475860963917146,\n",
       "  1.9877173997260429,\n",
       "  1.9831202536476165,\n",
       "  1.9618318679249884,\n",
       "  1.9856768657802155,\n",
       "  2.0008409672262126,\n",
       "  2.0015429134074325,\n",
       "  1.9779589825155193,\n",
       "  1.9823459934543919,\n",
       "  1.9343822149696497,\n",
       "  1.946398707430335,\n",
       "  1.9596774752996142,\n",
       "  2.003366549963196,\n",
       "  1.967259484828669,\n",
       "  1.9536969910257111,\n",
       "  1.9430970142246673,\n",
       "  1.9562600643938572,\n",
       "  1.993551438379472,\n",
       "  2.028236635863551,\n",
       "  1.9554168823603038,\n",
       "  1.9309461802589387,\n",
       "  2.0269557923423736,\n",
       "  1.9738642156814517,\n",
       "  1.9553209078357947,\n",
       "  1.9377415999482497,\n",
       "  1.9711244778283314,\n",
       "  1.9657196082663813,\n",
       "  1.946490366486509,\n",
       "  1.9391714141175553,\n",
       "  1.9465939556769882,\n",
       "  1.949568798183014,\n",
       "  2.01215681974492,\n",
       "  1.9629313149507441,\n",
       "  1.9502412849411541,\n",
       "  1.9353984276760499,\n",
       "  1.9656581234287571,\n",
       "  1.9447729601362957,\n",
       "  1.9617816766716798,\n",
       "  1.947070467886317,\n",
       "  1.946208404758262,\n",
       "  1.9612232211934093,\n",
       "  1.9657741992169826,\n",
       "  1.9596775434192084,\n",
       "  1.936209512493325,\n",
       "  1.9453500615123616,\n",
       "  1.941722688527641,\n",
       "  1.9124063446715072,\n",
       "  1.958042792832069,\n",
       "  1.9472411580987878,\n",
       "  1.914603575316175,\n",
       "  1.9547091782323183,\n",
       "  1.9078017638917135,\n",
       "  1.9433299270836082,\n",
       "  1.9467607520261787,\n",
       "  1.9643296776591121,\n",
       "  1.9076814430560844]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyVal_MB = []\n",
    "historyTr_MB = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_MB()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=100).history\n",
    "    historyVal_MB.append(history['val_loss'])\n",
    "    historyTr_MB.append(history['loss'])\n",
    "model=create_model_MB()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=600, \n",
    "                      batch_size=100).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_MB_mean=np.mean(historyVal_MB, axis=0)\n",
    "historyTr_MB_mean=np.mean(historyTr_MB, axis=0)\n",
    "\n",
    "historyVal_MB_sd=np.std(historyVal_MB, axis=0)\n",
    "historyTr_MB_sd=np.std(historyTr_MB, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_MB_mean)),historyTr_MB_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_MB_mean)), historyTr_MB_mean - historyTr_MB_sd, \n",
    "                 historyTr_MB_mean + historyTr_MB_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_MB_mean)), historyVal_MB_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_MB_mean)), historyVal_MB_mean - historyVal_MB_sd, \n",
    "                 historyVal_MB_mean + historyVal_MB_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.title('Learning curves batch=100', fontsize = 18)\n",
    "plt.legend()\n",
    "plt.ylim(5,20)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 21, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batch (mb=100) result:\n",
      "MEE on the validation 2.9553233279224527 with standard deviation 0.1293055552691295\n",
      "MEE on the training 1.9266514322472355 with standard deviation 0.032017323828517234\n"
     ]
    }
   ],
   "source": [
    "print(\"Mini batch (mb=100) result:\")\n",
    "print(\"MEE on the validation\",historyVal_MB_mean[-1],\"with standard deviation\",historyVal_MB_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_MB_mean[-1],\"with standard deviation\",historyTr_MB_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_OL():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=0.01, \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 4s 4ms/step - loss: 13.8126 - val_loss: 4.9088\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 4.2563 - val_loss: 3.3821\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.4978 - val_loss: 3.4666\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.3586 - val_loss: 2.9561\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.2141 - val_loss: 3.0558\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1323 - val_loss: 3.2348\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1121 - val_loss: 2.7659\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0413 - val_loss: 3.0975\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9821 - val_loss: 3.0786\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9486 - val_loss: 2.8665\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9104 - val_loss: 2.7230\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8423 - val_loss: 2.6688\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8620 - val_loss: 2.7739\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8154 - val_loss: 2.8342\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8651 - val_loss: 2.9224\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7789 - val_loss: 2.7834\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7597 - val_loss: 2.7753\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7849 - val_loss: 2.7319\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7573 - val_loss: 2.6207\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6881 - val_loss: 2.6737\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6979 - val_loss: 2.5961\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6959 - val_loss: 2.6911\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6895 - val_loss: 2.7302\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6712 - val_loss: 2.6717\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6252 - val_loss: 2.5687\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6349 - val_loss: 2.5769\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6361 - val_loss: 2.6157\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6090 - val_loss: 2.7012\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5706 - val_loss: 2.7068\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5612 - val_loss: 2.8871\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5604 - val_loss: 2.6614\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5693 - val_loss: 2.8234\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5535 - val_loss: 2.7160\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5384 - val_loss: 2.6577\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5344 - val_loss: 2.6251\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5301 - val_loss: 2.7155\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5006 - val_loss: 2.6073\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5039 - val_loss: 2.6946\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4704 - val_loss: 2.5822\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4536 - val_loss: 2.6743\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4599 - val_loss: 2.6957\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4515 - val_loss: 2.6399\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4650 - val_loss: 2.8213\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4398 - val_loss: 2.6674\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4408 - val_loss: 2.7321\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4367 - val_loss: 2.6708\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4093 - val_loss: 2.7865\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3914 - val_loss: 2.8579\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4311 - val_loss: 2.7731\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3818 - val_loss: 2.7220\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4004 - val_loss: 2.6720\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3959 - val_loss: 2.6949\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3577 - val_loss: 2.6535\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3663 - val_loss: 2.8914\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3575 - val_loss: 2.6867\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3177 - val_loss: 2.6467\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3531 - val_loss: 2.7272\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3559 - val_loss: 2.6217\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3384 - val_loss: 2.6670\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3208 - val_loss: 2.6499\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2951 - val_loss: 2.6391\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3191 - val_loss: 2.6673\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2992 - val_loss: 2.6180\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3005 - val_loss: 2.6173\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2795 - val_loss: 2.7174\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2780 - val_loss: 2.7149\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2668 - val_loss: 2.6556\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2776 - val_loss: 2.6844\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2512 - val_loss: 2.6449\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2867 - val_loss: 2.8030\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2412 - val_loss: 2.8054\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2331 - val_loss: 2.7050\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2486 - val_loss: 2.6627\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2092 - val_loss: 2.6314\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2228 - val_loss: 2.7942\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2233 - val_loss: 2.6261\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2116 - val_loss: 2.6315\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2240 - val_loss: 2.7022\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2117 - val_loss: 2.7102\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2110 - val_loss: 2.7701\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1810 - val_loss: 2.6631\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1929 - val_loss: 2.6903\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1911 - val_loss: 2.8921\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1932 - val_loss: 2.7313\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1741 - val_loss: 2.5940\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1632 - val_loss: 2.6836\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1410 - val_loss: 2.6869\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1280 - val_loss: 2.6376\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1559 - val_loss: 2.7450\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1330 - val_loss: 2.7195\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1561 - val_loss: 2.8327\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1163 - val_loss: 2.6434\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1041 - val_loss: 2.6071\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1369 - val_loss: 2.8098\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1081 - val_loss: 2.7409\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1302 - val_loss: 2.9206\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0997 - val_loss: 2.8540\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1096 - val_loss: 2.7054\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0859 - val_loss: 2.7249\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0833 - val_loss: 2.7351\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 13.8092 - val_loss: 6.3017\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 4.5198 - val_loss: 3.6965\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.6096 - val_loss: 3.1444\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.3881 - val_loss: 3.2361\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.2441 - val_loss: 2.8640\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1757 - val_loss: 2.8953\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0937 - val_loss: 2.9227\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0864 - val_loss: 2.8555\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0179 - val_loss: 2.9262\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9841 - val_loss: 2.8858\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9449 - val_loss: 2.7889\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9458 - val_loss: 2.9687\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9201 - val_loss: 3.0083\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9088 - val_loss: 3.2061\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8526 - val_loss: 2.8797\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8645 - val_loss: 2.8002\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8220 - val_loss: 2.8089\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8178 - val_loss: 3.0761\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8181 - val_loss: 3.0435\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7885 - val_loss: 2.7866\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7363 - val_loss: 2.7179\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7617 - val_loss: 3.1236\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7408 - val_loss: 2.8043\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7494 - val_loss: 2.7303\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6936 - val_loss: 2.7566\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6787 - val_loss: 2.7693\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6891 - val_loss: 2.8860\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6565 - val_loss: 2.8101\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7001 - val_loss: 2.8129\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6357 - val_loss: 3.0263\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6697 - val_loss: 2.8826\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6135 - val_loss: 2.8256\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6141 - val_loss: 2.8534\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6067 - val_loss: 3.1218\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6349 - val_loss: 2.8215\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5967 - val_loss: 2.6021\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6015 - val_loss: 2.9665\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5714 - val_loss: 2.8804\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5743 - val_loss: 2.5940\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5859 - val_loss: 2.7199\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5660 - val_loss: 2.7339\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5508 - val_loss: 2.9096\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5254 - val_loss: 2.6902\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5367 - val_loss: 2.9486\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5395 - val_loss: 2.7887\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4908 - val_loss: 2.7697\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5109 - val_loss: 2.7141\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5051 - val_loss: 2.7061\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5094 - val_loss: 2.7252\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4724 - val_loss: 2.7081\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4901 - val_loss: 2.8143\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4529 - val_loss: 2.6977\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4533 - val_loss: 2.7458\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4576 - val_loss: 2.9579\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4517 - val_loss: 2.6845\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4372 - val_loss: 2.7517\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4479 - val_loss: 2.6188\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4402 - val_loss: 2.7328\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4254 - val_loss: 2.6890\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4175 - val_loss: 2.6676\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4293 - val_loss: 2.6759\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3835 - val_loss: 2.6619\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3994 - val_loss: 2.7072\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3813 - val_loss: 2.6673\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3629 - val_loss: 3.0680\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3800 - val_loss: 2.6451\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4069 - val_loss: 2.7277\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3637 - val_loss: 2.6864\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3326 - val_loss: 2.7041\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3640 - val_loss: 2.7107\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3376 - val_loss: 2.6956\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3355 - val_loss: 2.7259\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3212 - val_loss: 2.7869\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3213 - val_loss: 2.7133\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3422 - val_loss: 2.7108\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3257 - val_loss: 2.7320\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3258 - val_loss: 2.6466\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3011 - val_loss: 2.6762\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3127 - val_loss: 2.7420\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2883 - val_loss: 2.7355\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2811 - val_loss: 2.7986\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2710 - val_loss: 2.7150\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2486 - val_loss: 2.6636\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2452 - val_loss: 2.6892\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2675 - val_loss: 2.6343\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2492 - val_loss: 2.6322\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2601 - val_loss: 2.7777\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2403 - val_loss: 2.5667\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2532 - val_loss: 2.7289\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2112 - val_loss: 2.6037\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2342 - val_loss: 2.7682\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2352 - val_loss: 2.7282\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1886 - val_loss: 2.6696\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2073 - val_loss: 2.6727\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2058 - val_loss: 2.6646\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2015 - val_loss: 2.6300\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1795 - val_loss: 2.7093\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1830 - val_loss: 2.7730\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1690 - val_loss: 2.7847\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1925 - val_loss: 2.8907\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 14.3420 - val_loss: 6.4486\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 4.7441 - val_loss: 3.9734\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.6593 - val_loss: 3.6996\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.3292 - val_loss: 3.5532\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.2379 - val_loss: 3.3820\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1310 - val_loss: 3.3868\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9972 - val_loss: 3.4805\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0276 - val_loss: 3.2415\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9076 - val_loss: 3.1116\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9068 - val_loss: 3.0788\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8540 - val_loss: 3.0629\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8122 - val_loss: 3.3508\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8027 - val_loss: 3.1742\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7808 - val_loss: 3.0604\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7585 - val_loss: 3.1496\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7639 - val_loss: 3.3715\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7171 - val_loss: 2.9010\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6691 - val_loss: 3.1654\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6056 - val_loss: 2.9498\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6831 - val_loss: 2.9578\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6384 - val_loss: 2.9679\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5926 - val_loss: 3.1354\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6146 - val_loss: 3.1657\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6152 - val_loss: 3.2898\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5748 - val_loss: 2.9146\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5565 - val_loss: 3.0640\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5364 - val_loss: 3.1264\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5528 - val_loss: 3.1499\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5191 - val_loss: 2.9772\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5226 - val_loss: 3.0951\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5095 - val_loss: 3.0848\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4814 - val_loss: 3.1996\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5212 - val_loss: 3.0443\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4335 - val_loss: 3.0243\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4324 - val_loss: 3.0409\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4675 - val_loss: 3.0881\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4410 - val_loss: 3.0205\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4426 - val_loss: 2.9198\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4318 - val_loss: 2.9776\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4264 - val_loss: 2.9110\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4116 - val_loss: 2.9301\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 4s 4ms/step - loss: 2.4116 - val_loss: 2.9952\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3734 - val_loss: 2.8535\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3938 - val_loss: 2.9828\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3606 - val_loss: 3.1242\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3672 - val_loss: 2.9408\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3756 - val_loss: 2.9501\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 5s 5ms/step - loss: 2.3254 - val_loss: 3.1837\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 5s 5ms/step - loss: 2.3535 - val_loss: 2.9931\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3622 - val_loss: 2.9134\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 5s 5ms/step - loss: 2.3062 - val_loss: 2.8703\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 5s 5ms/step - loss: 2.3176 - val_loss: 2.9452\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 5s 5ms/step - loss: 2.3256 - val_loss: 2.9681\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3105 - val_loss: 3.1177\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2924 - val_loss: 3.1495\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2859 - val_loss: 3.0518\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2786 - val_loss: 2.9437\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2576 - val_loss: 3.2189\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2802 - val_loss: 3.1204\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2640 - val_loss: 2.9124\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2585 - val_loss: 2.9763\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2330 - val_loss: 3.0721\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2466 - val_loss: 3.0916\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2367 - val_loss: 2.9857\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2044 - val_loss: 2.9998\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2297 - val_loss: 3.0287\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2261 - val_loss: 3.0305\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1968 - val_loss: 2.8762\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2019 - val_loss: 3.0197\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1769 - val_loss: 3.0371\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1916 - val_loss: 2.9610\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1729 - val_loss: 3.0102\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1654 - val_loss: 3.0640\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1563 - val_loss: 3.0362\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1534 - val_loss: 3.1571\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1684 - val_loss: 3.0970\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1377 - val_loss: 2.9391\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1427 - val_loss: 2.9999\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1406 - val_loss: 2.9265\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1153 - val_loss: 2.9285\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1142 - val_loss: 3.0657\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1214 - val_loss: 2.9407\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1089 - val_loss: 2.9651\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0967 - val_loss: 3.1372\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0810 - val_loss: 2.9652\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0886 - val_loss: 2.9973\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0976 - val_loss: 3.0950\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0660 - val_loss: 3.0368\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0786 - val_loss: 2.8658\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0773 - val_loss: 2.9481\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0805 - val_loss: 3.0096\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0578 - val_loss: 3.0892\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0558 - val_loss: 2.9591\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0429 - val_loss: 2.9039\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0337 - val_loss: 2.9539\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0144 - val_loss: 2.9596\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0710 - val_loss: 3.0053\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0104 - val_loss: 3.0893\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0342 - val_loss: 3.0290\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0369 - val_loss: 2.9924\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 13.5248 - val_loss: 6.3706\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 4.4533 - val_loss: 3.7633\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.5307 - val_loss: 3.1633\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.3661 - val_loss: 3.5234\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1852 - val_loss: 3.7881\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0885 - val_loss: 3.3438\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0443 - val_loss: 3.0534\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0279 - val_loss: 3.0633\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9632 - val_loss: 3.0363\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9550 - val_loss: 2.8827\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9275 - val_loss: 2.8858\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9057 - val_loss: 2.9697\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9096 - val_loss: 2.8889\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8453 - val_loss: 2.7390\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8231 - val_loss: 2.7694\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7858 - val_loss: 3.1161\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7696 - val_loss: 2.8142\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7766 - val_loss: 2.7968\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7484 - val_loss: 2.8546\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7279 - val_loss: 2.6720\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7473 - val_loss: 2.7943\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6899 - val_loss: 2.8241\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6961 - val_loss: 2.7827\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6784 - val_loss: 2.8433\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6941 - val_loss: 2.8941\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6677 - val_loss: 2.7805\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6569 - val_loss: 2.8490\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6498 - val_loss: 2.6091\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6511 - val_loss: 2.8634\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6187 - val_loss: 2.7135\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6395 - val_loss: 2.7848\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6314 - val_loss: 2.7093\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5604 - val_loss: 2.6536\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5795 - val_loss: 2.7122\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6101 - val_loss: 2.7573\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5508 - val_loss: 2.6531\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5813 - val_loss: 2.6994\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5568 - val_loss: 2.7259\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5100 - val_loss: 2.7724\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5409 - val_loss: 2.6926\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5480 - val_loss: 2.6983\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5145 - val_loss: 2.6605\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5206 - val_loss: 2.6891\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5040 - val_loss: 2.6296\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4943 - val_loss: 2.7847\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4784 - val_loss: 2.7065\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4960 - val_loss: 2.7411\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4744 - val_loss: 2.8056\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4665 - val_loss: 2.8769\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4692 - val_loss: 2.7011\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4825 - val_loss: 2.6816\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4543 - val_loss: 2.7017\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4668 - val_loss: 2.7426\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4183 - val_loss: 2.8035\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4350 - val_loss: 2.6894\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4405 - val_loss: 2.6849\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4256 - val_loss: 2.7957\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4290 - val_loss: 2.7801\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4169 - val_loss: 2.6600\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3869 - val_loss: 2.7597\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4159 - val_loss: 2.9571\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4222 - val_loss: 2.6785\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3958 - val_loss: 2.6524\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3764 - val_loss: 2.7674\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3833 - val_loss: 2.7975\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3554 - val_loss: 2.7162\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3932 - val_loss: 2.7944\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3814 - val_loss: 2.7162\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3214 - val_loss: 2.7176\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3452 - val_loss: 2.9045\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3403 - val_loss: 2.7369\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3445 - val_loss: 2.6997\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3263 - val_loss: 2.6980\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3446 - val_loss: 2.7465\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3199 - val_loss: 2.6439\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3100 - val_loss: 2.7191\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2948 - val_loss: 2.8300\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3019 - val_loss: 2.7828\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2925 - val_loss: 2.8830\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2934 - val_loss: 2.7798\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2766 - val_loss: 2.9625\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3127 - val_loss: 2.7281\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2820 - val_loss: 2.7260\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2615 - val_loss: 2.8202\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2390 - val_loss: 2.6302\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2469 - val_loss: 2.7113\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2582 - val_loss: 2.9156\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2628 - val_loss: 2.6940\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2594 - val_loss: 2.7031\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2606 - val_loss: 2.8681\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2610 - val_loss: 2.6809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2615 - val_loss: 2.7380\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2285 - val_loss: 2.6795\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2146 - val_loss: 2.6717\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2512 - val_loss: 3.0509\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2332 - val_loss: 2.9374\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2091 - val_loss: 2.8069\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2053 - val_loss: 2.7354\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2080 - val_loss: 2.8373\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1903 - val_loss: 2.7172\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 13.3557 - val_loss: 5.2875\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 4.2870 - val_loss: 4.0029\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.4843 - val_loss: 3.4815\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.2841 - val_loss: 3.2927\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1519 - val_loss: 3.1551\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0757 - val_loss: 3.3075\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0041 - val_loss: 3.1482\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9451 - val_loss: 3.2236\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9167 - val_loss: 3.2421\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9207 - val_loss: 3.1185\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8084 - val_loss: 3.0618\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7847 - val_loss: 3.0583\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7744 - val_loss: 3.1292\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7838 - val_loss: 2.9656\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7287 - val_loss: 3.0443\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6924 - val_loss: 2.9657\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6963 - val_loss: 3.1335\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6518 - val_loss: 3.1014\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6523 - val_loss: 3.0998\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6266 - val_loss: 3.0996\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5983 - val_loss: 3.0305\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6096 - val_loss: 2.9458\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5510 - val_loss: 3.3327\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5932 - val_loss: 3.2024\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5749 - val_loss: 3.0261\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5444 - val_loss: 3.0265\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5434 - val_loss: 3.0332\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5573 - val_loss: 2.9035\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5074 - val_loss: 3.0277\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4935 - val_loss: 2.9723\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4831 - val_loss: 2.9885\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5190 - val_loss: 2.9840\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4811 - val_loss: 2.9815\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4637 - val_loss: 3.1225\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4430 - val_loss: 2.8901\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4255 - val_loss: 2.9315\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4311 - val_loss: 3.0813\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4438 - val_loss: 3.0731\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4016 - val_loss: 3.0710\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4067 - val_loss: 3.2447\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3995 - val_loss: 2.9750\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3721 - val_loss: 3.0718\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3744 - val_loss: 3.0182\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3696 - val_loss: 3.1849\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3704 - val_loss: 2.9278\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3521 - val_loss: 2.9872\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3696 - val_loss: 2.9383\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3255 - val_loss: 2.9750\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3617 - val_loss: 3.0532\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3202 - val_loss: 3.0624\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2977 - val_loss: 3.3893\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3320 - val_loss: 3.0110\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3012 - val_loss: 2.9960\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3112 - val_loss: 3.0195\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2937 - val_loss: 3.1203\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3011 - val_loss: 3.0166\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2927 - val_loss: 3.0530\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2692 - val_loss: 3.0302\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2581 - val_loss: 2.9607\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2568 - val_loss: 2.9899\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2837 - val_loss: 2.9686\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2703 - val_loss: 3.1167\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2543 - val_loss: 3.0801\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2286 - val_loss: 3.0118\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2597 - val_loss: 3.0365\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2316 - val_loss: 3.2095\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2164 - val_loss: 3.0985\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2180 - val_loss: 3.1587\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2225 - val_loss: 2.9353\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1855 - val_loss: 3.2072\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2048 - val_loss: 3.1409\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1697 - val_loss: 3.0610\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1951 - val_loss: 3.0853\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1912 - val_loss: 2.9332\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1899 - val_loss: 3.0620\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1711 - val_loss: 3.1762\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1463 - val_loss: 3.2826\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1256 - val_loss: 3.2142\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1542 - val_loss: 2.9941\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1231 - val_loss: 3.1320\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1454 - val_loss: 3.0969\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1119 - val_loss: 3.1299\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1327 - val_loss: 3.1767\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1085 - val_loss: 3.1973\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0929 - val_loss: 3.0483\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1135 - val_loss: 3.0999\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0734 - val_loss: 3.0947\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1013 - val_loss: 3.1636\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0850 - val_loss: 3.0970\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0990 - val_loss: 3.2414\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0874 - val_loss: 3.1280\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0629 - val_loss: 3.1201\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0598 - val_loss: 3.2923\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0803 - val_loss: 3.1143\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0595 - val_loss: 3.1563\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0623 - val_loss: 3.0719\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0563 - val_loss: 3.0659\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0469 - val_loss: 3.1128\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0047 - val_loss: 3.1747\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0209 - val_loss: 3.1059\n"
     ]
    }
   ],
   "source": [
    "historyVal_OL = []\n",
    "historyTr_OL = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_OL()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=100, batch_size=1).history\n",
    "    historyVal_OL.append(history['val_loss'])\n",
    "    historyTr_OL.append(history['loss'])\n",
    "#model=create_model_OL()\n",
    "#model.add_loss(MEE_k)\n",
    "#model.fit(X_dev, y_dev, epochs=100, \n",
    "                    #  batch_size=1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_OL_mean=np.mean(historyVal_OL, axis=0)\n",
    "historyTr_OL_mean=np.mean(historyTr_OL, axis=0)\n",
    "\n",
    "historyVal_OL_sd=np.std(historyVal_OL, axis=0)\n",
    "historyTr_OL_sd=np.std(historyTr_OL, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_OL_mean)),historyTr_OL_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_OL_mean)), historyTr_OL_mean - historyTr_OL_sd, \n",
    "                 historyTr_OL_mean + historyTr_OL_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_OL_mean)), historyVal_OL_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_OL_mean)), historyVal_OL_mean - historyVal_OL_sd, \n",
    "                 historyVal_OL_mean + historyVal_OL_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.title('Learning curves OnLine batch', fontsize = 18)\n",
    "plt.legend()\n",
    "plt.ylim(5,14)\n",
    "plt.xlim(-5,100)\n",
    "plt.yticks(np.arange(0, 14, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnLine batch result:\n",
      "MEE on the validation 2.8882639716433283 with standard deviation 0.1489354257559105\n",
      "MEE on the training 2.104787623708255 with standard deviation 0.07363679605818553\n"
     ]
    }
   ],
   "source": [
    "print(\"OnLine batch result:\")\n",
    "print(\"MEE on the validation\",historyVal_OL_mean[-1],\"with standard deviation\",historyVal_OL_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_OL_mean[-1],\"with standard deviation\",historyTr_OL_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_LM():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=mlpr.best_params_['lr'], \n",
    "                                                            momentum=0.01))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 56.7479 - val_loss: 54.3446\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 54.0587 - val_loss: 51.5404\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 51.2613 - val_loss: 48.3691\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 48.0955 - val_loss: 44.6562\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 44.3867 - val_loss: 40.3805\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 40.1149 - val_loss: 35.6602\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 35.4019 - val_loss: 30.7534\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 30.5079 - val_loss: 26.1873\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 25.9444 - val_loss: 22.5889\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 22.3445 - val_loss: 20.3840\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 20.1336 - val_loss: 18.9653\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 18.6825 - val_loss: 17.8987\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 17.5783 - val_loss: 17.0429\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 16.6951 - val_loss: 16.3045\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 15.9363 - val_loss: 15.5948\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 15.2129 - val_loss: 14.8204\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 14.4335 - val_loss: 13.8913\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 13.5073 - val_loss: 12.8572\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 12.4692 - val_loss: 11.8999\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 11.4961 - val_loss: 11.0262\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 10.6067 - val_loss: 10.2495\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.8249 - val_loss: 9.6494\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 9.2342 - val_loss: 9.2725\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.8570 - val_loss: 9.0600\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.6424 - val_loss: 8.9294\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.5174 - val_loss: 8.8454\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 8.4381 - val_loss: 8.7800\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.3801 - val_loss: 8.7257\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 8.3309 - val_loss: 8.6755\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.2853 - val_loss: 8.6269\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.2405 - val_loss: 8.5799\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1957 - val_loss: 8.5319\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 8.1500 - val_loss: 8.4837\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.1028 - val_loss: 8.4334\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.0535 - val_loss: 8.3811\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.0018 - val_loss: 8.3256\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.9473 - val_loss: 8.2671\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.8899 - val_loss: 8.2052\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.8295 - val_loss: 8.1400\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.7663 - val_loss: 8.0714\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.7006 - val_loss: 8.0000\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.6324 - val_loss: 7.9258\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 7.5621 - val_loss: 7.8493\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 7.4897 - val_loss: 7.7702\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 7.4154 - val_loss: 7.6889\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.3392 - val_loss: 7.6051\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 7.2611 - val_loss: 7.5193\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.1812 - val_loss: 7.4313\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 7.0995 - val_loss: 7.3412\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.0160 - val_loss: 7.2490\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.9309 - val_loss: 7.1547\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.8442 - val_loss: 7.0583\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.7559 - val_loss: 6.9602\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 6.6663 - val_loss: 6.8605\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 6.5756 - val_loss: 6.7599\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 6.4842 - val_loss: 6.6592\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.3924 - val_loss: 6.5570\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 6.3000 - val_loss: 6.4551\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.2071 - val_loss: 6.3520\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 6.1139 - val_loss: 6.2504\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 6.0207 - val_loss: 6.1478\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 5.9278 - val_loss: 6.0483\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 5.8356 - val_loss: 5.9447\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 5.7445 - val_loss: 5.8515\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.6551 - val_loss: 5.7455\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 5.5685 - val_loss: 5.6756\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.4898 - val_loss: 5.5779\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 5.4372 - val_loss: 5.6426\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.4542 - val_loss: 5.7357\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 5.6723 - val_loss: 6.3542\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.1624 - val_loss: 6.6093\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 6.6084 - val_loss: 6.5405\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.3377 - val_loss: 6.1295\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.1786 - val_loss: 6.1161\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 5.9222 - val_loss: 5.8523\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.9036 - val_loss: 5.8944\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.7302 - val_loss: 5.6950\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.7582 - val_loss: 5.7165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.5793 - val_loss: 5.5225\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 5.5973 - val_loss: 5.5441\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 5.4342 - val_loss: 5.3731\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.4587 - val_loss: 5.3914\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.3076 - val_loss: 5.2431\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 5.3394 - val_loss: 5.2530\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.1941 - val_loss: 5.1243\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.2303 - val_loss: 5.1260\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.0908 - val_loss: 5.0132\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.1280 - val_loss: 5.0096\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.9964 - val_loss: 4.9117\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.0340 - val_loss: 4.9044\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.9113 - val_loss: 4.8184\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.9464 - val_loss: 4.8106\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.8359 - val_loss: 4.7250\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 4.8578 - val_loss: 4.7235\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.7656 - val_loss: 4.6533\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.7904 - val_loss: 4.6323\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 4.6895 - val_loss: 4.5848\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 4.7239 - val_loss: 4.5630\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.6318 - val_loss: 4.5306\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 4.6703 - val_loss: 4.5047\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.5822 - val_loss: 4.4839\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.6229 - val_loss: 4.4560\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 4.5409 - val_loss: 4.4434\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.5814 - val_loss: 4.4089\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.4994 - val_loss: 4.4037\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 4.5407 - val_loss: 4.3642\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.4597 - val_loss: 4.3666\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.5026 - val_loss: 4.3195\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.4190 - val_loss: 4.3312\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.4652 - val_loss: 4.2779\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.3805 - val_loss: 4.2985\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.4296 - val_loss: 4.2397\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.3448 - val_loss: 4.2680\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3961 - val_loss: 4.2010\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.3086 - val_loss: 4.2378\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.3630 - val_loss: 4.1648\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.2747 - val_loss: 4.2094\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 4.3315 - val_loss: 4.1323\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.2438 - val_loss: 4.1832\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3024 - val_loss: 4.1031\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2156 - val_loss: 4.1592\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.2756 - val_loss: 4.0785\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.1916 - val_loss: 4.1384\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.2522 - val_loss: 4.0531\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.1662 - val_loss: 4.1172\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.2286 - val_loss: 4.0300\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.1424 - val_loss: 4.0974\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.2065 - val_loss: 4.0109\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 4.1232 - val_loss: 4.0817\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.1886 - val_loss: 3.9902\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.1013 - val_loss: 4.0642\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 4.1686 - val_loss: 3.9699\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 4.0795 - val_loss: 4.0480\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.1493 - val_loss: 3.9509\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.0590 - val_loss: 4.0340\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 4.1317 - val_loss: 3.9326\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.0390 - val_loss: 4.0206\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.1145 - val_loss: 3.9148\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.0193 - val_loss: 4.0063\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.0964 - val_loss: 3.8973\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.9998 - val_loss: 3.9912\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.0772 - val_loss: 3.8801\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.9807 - val_loss: 3.9760\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.0580 - val_loss: 3.8635\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.9621 - val_loss: 3.9613\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0393 - val_loss: 3.8473\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9440 - val_loss: 3.9470\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0210 - val_loss: 3.8314\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9260 - val_loss: 3.9326\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0027 - val_loss: 3.8156\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9080 - val_loss: 3.9180\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.9843 - val_loss: 3.8000\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8902 - val_loss: 3.9032\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9657 - val_loss: 3.7796\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.8661 - val_loss: 3.8818\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.9404 - val_loss: 3.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.8475 - val_loss: 3.8656\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9205 - val_loss: 3.7508\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8327 - val_loss: 3.8520\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.9034 - val_loss: 3.7391\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8193 - val_loss: 3.8391\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.8873 - val_loss: 3.7279\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.8066 - val_loss: 3.8265\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8716 - val_loss: 3.7172\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7944 - val_loss: 3.8140\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.8560 - val_loss: 3.7071\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.7828 - val_loss: 3.8015\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8407 - val_loss: 3.6974\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.7715 - val_loss: 3.7893\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8254 - val_loss: 3.6880\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7605 - val_loss: 3.7773\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8104 - val_loss: 3.6789\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7498 - val_loss: 3.7657\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7957 - val_loss: 3.6701\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.7394 - val_loss: 3.7543\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7813 - val_loss: 3.6601\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7280 - val_loss: 3.7430\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7669 - val_loss: 3.6498\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7163 - val_loss: 3.7318\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7525 - val_loss: 3.6412\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.7061 - val_loss: 3.7211\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7388 - val_loss: 3.6334\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6968 - val_loss: 3.7108\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7256 - val_loss: 3.6259\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.6877 - val_loss: 3.7006\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7127 - val_loss: 3.6182\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.6785 - val_loss: 3.6906\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7000 - val_loss: 3.6103\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.6689 - val_loss: 3.6807\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6874 - val_loss: 3.6019\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.6590 - val_loss: 3.6708\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6750 - val_loss: 3.5936\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.6490 - val_loss: 3.6612\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.6629 - val_loss: 3.5855\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6395 - val_loss: 3.6519\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6512 - val_loss: 3.5780\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.6306 - val_loss: 3.6430\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6400 - val_loss: 3.5708\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.6221 - val_loss: 3.6344\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.6292 - val_loss: 3.5640\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.6141 - val_loss: 3.6260\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.6188 - val_loss: 3.5575\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.6063 - val_loss: 3.6180\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6087 - val_loss: 3.5511\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.5988 - val_loss: 3.6101\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.5990 - val_loss: 3.5450\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.5914 - val_loss: 3.6024\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.5894 - val_loss: 3.5388\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.5841 - val_loss: 3.5947\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.5800 - val_loss: 3.5327\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.5767 - val_loss: 3.5871\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5707 - val_loss: 3.5265\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5692 - val_loss: 3.5793\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5613 - val_loss: 3.5202\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5616 - val_loss: 3.5712\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5515 - val_loss: 3.5137\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5536 - val_loss: 3.5627\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5413 - val_loss: 3.5069\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.5454 - val_loss: 3.5539\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5308 - val_loss: 3.5000\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5370 - val_loss: 3.5452\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5204 - val_loss: 3.4934\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5289 - val_loss: 3.5370\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5108 - val_loss: 3.4871\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5213 - val_loss: 3.5294\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5020 - val_loss: 3.4813\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5142 - val_loss: 3.5223\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4937 - val_loss: 3.4757\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5073 - val_loss: 3.5187\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4886 - val_loss: 3.4717\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5020 - val_loss: 3.5133\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4822 - val_loss: 3.4670\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.4962 - val_loss: 3.5079\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4760 - val_loss: 3.4624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4905 - val_loss: 3.5026\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4701 - val_loss: 3.4579\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4850 - val_loss: 3.4974\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.4644 - val_loss: 3.4537\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4797 - val_loss: 3.4922\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4589 - val_loss: 3.4495\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 35us/step - loss: 3.4746 - val_loss: 3.4871\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.4536 - val_loss: 3.4453\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.4695 - val_loss: 3.4821\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4484 - val_loss: 3.4412\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.4644 - val_loss: 3.4770\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4433 - val_loss: 3.4372\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.4594 - val_loss: 3.4719\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4382 - val_loss: 3.4331\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4544 - val_loss: 3.4668\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4331 - val_loss: 3.4291\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4494 - val_loss: 3.4617\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4282 - val_loss: 3.4251\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4445 - val_loss: 3.4566\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4232 - val_loss: 3.4211\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4396 - val_loss: 3.4515\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4184 - val_loss: 3.4171\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.4347 - val_loss: 3.4464\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4135 - val_loss: 3.4131\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4297 - val_loss: 3.4413\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.4088 - val_loss: 3.4091\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4247 - val_loss: 3.4362\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4040 - val_loss: 3.4050\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4197 - val_loss: 3.4311\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3994 - val_loss: 3.4009\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4146 - val_loss: 3.4261\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.3948 - val_loss: 3.3968\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4095 - val_loss: 3.4212\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3902 - val_loss: 3.3927\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4045 - val_loss: 3.4163\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.3857 - val_loss: 3.3886\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3995 - val_loss: 3.4114\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3812 - val_loss: 3.3846\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3945 - val_loss: 3.4065\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3766 - val_loss: 3.3805\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3893 - val_loss: 3.4016\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3719 - val_loss: 3.3762\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3841 - val_loss: 3.3966\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3672 - val_loss: 3.3719\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3788 - val_loss: 3.3916\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.3623 - val_loss: 3.3673\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3732 - val_loss: 3.3865\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3573 - val_loss: 3.3626\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3675 - val_loss: 3.3813\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3521 - val_loss: 3.3578\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3616 - val_loss: 3.3761\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3469 - val_loss: 3.3529\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3557 - val_loss: 3.3710\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3417 - val_loss: 3.3481\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3499 - val_loss: 3.3659\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3367 - val_loss: 3.3436\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3444 - val_loss: 3.3611\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3318 - val_loss: 3.3393\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3391 - val_loss: 3.3564\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3271 - val_loss: 3.3353\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3341 - val_loss: 3.3520\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3227 - val_loss: 3.3315\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3293 - val_loss: 3.3478\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3186 - val_loss: 3.3279\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3248 - val_loss: 3.3438\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3147 - val_loss: 3.3245\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3204 - val_loss: 3.3399\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3110 - val_loss: 3.3212\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3162 - val_loss: 3.3362\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3074 - val_loss: 3.3181\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3122 - val_loss: 3.3326\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3041 - val_loss: 3.3152\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3084 - val_loss: 3.3292\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3009 - val_loss: 3.3125\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3048 - val_loss: 3.3260\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2979 - val_loss: 3.3101\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3015 - val_loss: 3.3228\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2950 - val_loss: 3.3078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2984 - val_loss: 3.3199\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2922 - val_loss: 3.3058\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2955 - val_loss: 3.3170\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2897 - val_loss: 3.3040\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2929 - val_loss: 3.3143\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2873 - val_loss: 3.3024\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2905 - val_loss: 3.3118\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2851 - val_loss: 3.3011\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2884 - val_loss: 3.3094\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2832 - val_loss: 3.3001\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2867 - val_loss: 3.3073\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2817 - val_loss: 3.2995\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2853 - val_loss: 3.3055\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2806 - val_loss: 3.2993\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2843 - val_loss: 3.3040\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.2800 - val_loss: 3.2993\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2836 - val_loss: 3.3029\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2800 - val_loss: 3.2997\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.2832 - val_loss: 3.3020\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2806 - val_loss: 3.3007\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2833 - val_loss: 3.3015\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2819 - val_loss: 3.3025\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2843 - val_loss: 3.3015\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2839 - val_loss: 3.3056\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2864 - val_loss: 3.3023\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2870 - val_loss: 3.3101\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2898 - val_loss: 3.3042\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2916 - val_loss: 3.3163\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2949 - val_loss: 3.3084\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2990 - val_loss: 3.3242\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3016 - val_loss: 3.3156\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3104 - val_loss: 3.3338\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3093 - val_loss: 3.3236\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3226 - val_loss: 3.3442\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.3171 - val_loss: 3.3314\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3345 - val_loss: 3.3550\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3248 - val_loss: 3.3392\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3462 - val_loss: 3.3661\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3327 - val_loss: 3.3458\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3564 - val_loss: 3.3757\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3390 - val_loss: 3.3491\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3626 - val_loss: 3.3821\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3418 - val_loss: 3.3510\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3667 - val_loss: 3.3831\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3390 - val_loss: 3.3529\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3699 - val_loss: 3.3854\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3356 - val_loss: 3.3564\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3757 - val_loss: 3.3885\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3312 - val_loss: 3.3609\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3844 - val_loss: 3.4049\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3429 - val_loss: 3.3910\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4154 - val_loss: 3.3948\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3286 - val_loss: 3.3574\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3744 - val_loss: 3.3429\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2740 - val_loss: 3.3030\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.3137 - val_loss: 3.2977\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2288 - val_loss: 3.2585\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2643 - val_loss: 3.2566\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1898 - val_loss: 3.2259\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2283 - val_loss: 3.2235\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1579 - val_loss: 3.2023\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2014 - val_loss: 3.2065\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1399 - val_loss: 3.1894\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1854 - val_loss: 3.1978\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1298 - val_loss: 3.1814\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1746 - val_loss: 3.1917\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1227 - val_loss: 3.1729\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1634 - val_loss: 3.1836\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1144 - val_loss: 3.1634\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1514 - val_loss: 3.1747\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1050 - val_loss: 3.1540\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1396 - val_loss: 3.1683\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0978 - val_loss: 3.1499\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1331 - val_loss: 3.1643\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0930 - val_loss: 3.1445\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1257 - val_loss: 3.1599\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0879 - val_loss: 3.1395\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1189 - val_loss: 3.1557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0831 - val_loss: 3.1358\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1133 - val_loss: 3.1521\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0790 - val_loss: 3.1327\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1086 - val_loss: 3.1489\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0754 - val_loss: 3.1301\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1044 - val_loss: 3.1459\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.0721 - val_loss: 3.1276\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1005 - val_loss: 3.1431\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.0688 - val_loss: 3.1252\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0969 - val_loss: 3.1403\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0658 - val_loss: 3.1230\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0934 - val_loss: 3.1377\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0629 - val_loss: 3.1210\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0903 - val_loss: 3.1356\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0605 - val_loss: 3.1194\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0875 - val_loss: 3.1342\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0587 - val_loss: 3.1184\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0853 - val_loss: 3.1335\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0576 - val_loss: 3.1179\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0836 - val_loss: 3.1330\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.0566 - val_loss: 3.1176\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0821 - val_loss: 3.1323\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.0554 - val_loss: 3.1172\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0805 - val_loss: 3.1313\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.0540 - val_loss: 3.1168\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0789 - val_loss: 3.1302\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0524 - val_loss: 3.1165\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0775 - val_loss: 3.1298\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0515 - val_loss: 3.1171\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0767 - val_loss: 3.1304\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0517 - val_loss: 3.1188\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0772 - val_loss: 3.1322\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0532 - val_loss: 3.1225\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0795 - val_loss: 3.1372\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0585 - val_loss: 3.1294\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0851 - val_loss: 3.1449\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0681 - val_loss: 3.1406\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0949 - val_loss: 3.1565\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0839 - val_loss: 3.1589\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1116 - val_loss: 3.1738\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1076 - val_loss: 3.1872\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1380 - val_loss: 3.1934\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1348 - val_loss: 3.2148\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1631 - val_loss: 3.2113\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1614 - val_loss: 3.2379\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1839 - val_loss: 3.2264\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1849 - val_loss: 3.2574\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2015 - val_loss: 3.2383\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2039 - val_loss: 3.2721\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2145 - val_loss: 3.2451\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 3.2160 - val_loss: 3.2803\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2212 - val_loss: 3.2479\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2226 - val_loss: 3.2849\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2247 - val_loss: 3.2482\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.2255 - val_loss: 3.2867\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2256 - val_loss: 3.2468\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2257 - val_loss: 3.2858\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.2240 - val_loss: 3.2440\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2237 - val_loss: 3.2827\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2204 - val_loss: 3.2401\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2199 - val_loss: 3.2781\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2154 - val_loss: 3.2355\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2148 - val_loss: 3.2722\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2093 - val_loss: 3.2303\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2087 - val_loss: 3.2652\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.2021 - val_loss: 3.2248\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2018 - val_loss: 3.2574\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.1942 - val_loss: 3.2191\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1944 - val_loss: 3.2492\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1859 - val_loss: 3.2135\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.1870 - val_loss: 3.2414\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1779 - val_loss: 3.2082\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.1798 - val_loss: 3.2342\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1706 - val_loss: 3.2034\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1730 - val_loss: 3.2279\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1641 - val_loss: 3.1991\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1669 - val_loss: 3.2224\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1584 - val_loss: 3.1953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1613 - val_loss: 3.2177\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1534 - val_loss: 3.1918\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1564 - val_loss: 3.2135\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1490 - val_loss: 3.1887\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1519 - val_loss: 3.2099\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1451 - val_loss: 3.1860\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1478 - val_loss: 3.2067\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1416 - val_loss: 3.1834\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1442 - val_loss: 3.2039\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1384 - val_loss: 3.1811\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1409 - val_loss: 3.2013\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1354 - val_loss: 3.1789\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1378 - val_loss: 3.1990\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1327 - val_loss: 3.1769\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1350 - val_loss: 3.1969\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1301 - val_loss: 3.1751\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1324 - val_loss: 3.1949\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1277 - val_loss: 3.1733\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1300 - val_loss: 3.1930\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1254 - val_loss: 3.1716\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1276 - val_loss: 3.1912\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1232 - val_loss: 3.1699\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1254 - val_loss: 3.1895\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1210 - val_loss: 3.1683\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1233 - val_loss: 3.1879\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1190 - val_loss: 3.1668\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1213 - val_loss: 3.1863\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1170 - val_loss: 3.1653\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1194 - val_loss: 3.1848\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1151 - val_loss: 3.1639\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1176 - val_loss: 3.1834\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1132 - val_loss: 3.1625\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1158 - val_loss: 3.1821\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1114 - val_loss: 3.1611\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.1141 - val_loss: 3.1808\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1097 - val_loss: 3.1598\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1124 - val_loss: 3.1796\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1081 - val_loss: 3.1585\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1108 - val_loss: 3.1784\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.1065 - val_loss: 3.1573\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1093 - val_loss: 3.1773\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1050 - val_loss: 3.1562\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1079 - val_loss: 3.1762\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1035 - val_loss: 3.1550\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1065 - val_loss: 3.1752\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1021 - val_loss: 3.1539\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1052 - val_loss: 3.1742\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1007 - val_loss: 3.1528\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1038 - val_loss: 3.1733\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0993 - val_loss: 3.1514\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1023 - val_loss: 3.1722\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0979 - val_loss: 3.1497\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1005 - val_loss: 3.1709\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0962 - val_loss: 3.1480\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0988 - val_loss: 3.1697\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0946 - val_loss: 3.1466\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0973 - val_loss: 3.1687\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0933 - val_loss: 3.1453\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0960 - val_loss: 3.1678\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0921 - val_loss: 3.1442\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0948 - val_loss: 3.1670\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0909 - val_loss: 3.1431\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0936 - val_loss: 3.1661\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0898 - val_loss: 3.1420\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0923 - val_loss: 3.1652\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0886 - val_loss: 3.1408\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0909 - val_loss: 3.1642\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0873 - val_loss: 3.1396\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0895 - val_loss: 3.1632\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.0860 - val_loss: 3.1384\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0880 - val_loss: 3.1621\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0847 - val_loss: 3.1372\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0864 - val_loss: 3.1610\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0833 - val_loss: 3.1359\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0848 - val_loss: 3.1598\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0819 - val_loss: 3.1347\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0832 - val_loss: 3.1585\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0804 - val_loss: 3.1334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0815 - val_loss: 3.1573\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0789 - val_loss: 3.1320\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0797 - val_loss: 3.1560\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0774 - val_loss: 3.1307\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0779 - val_loss: 3.1547\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0759 - val_loss: 3.1294\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0762 - val_loss: 3.1533\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0743 - val_loss: 3.1281\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0744 - val_loss: 3.1520\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0728 - val_loss: 3.1268\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0726 - val_loss: 3.1507\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0713 - val_loss: 3.1255\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0708 - val_loss: 3.1494\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0698 - val_loss: 3.1242\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0690 - val_loss: 3.1482\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0683 - val_loss: 3.1229\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0672 - val_loss: 3.1469\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0669 - val_loss: 3.1217\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0655 - val_loss: 3.1457\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0655 - val_loss: 3.1205\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0638 - val_loss: 3.1446\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.0641 - val_loss: 3.1193\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0621 - val_loss: 3.1434\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0628 - val_loss: 3.1181\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0605 - val_loss: 3.1424\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0615 - val_loss: 3.1170\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0589 - val_loss: 3.1413\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0603 - val_loss: 3.1159\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0573 - val_loss: 3.1403\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0590 - val_loss: 3.1148\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0557 - val_loss: 3.1393\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0578 - val_loss: 3.1137\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0542 - val_loss: 3.1383\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0567 - val_loss: 3.1126\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0527 - val_loss: 3.1374\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0555 - val_loss: 3.1116\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0512 - val_loss: 3.1364\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0543 - val_loss: 3.1105\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0497 - val_loss: 3.1354\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0531 - val_loss: 3.1095\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0482 - val_loss: 3.1345\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0520 - val_loss: 3.1085\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0467 - val_loss: 3.1335\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0508 - val_loss: 3.1074\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0453 - val_loss: 3.1325\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0496 - val_loss: 3.1064\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0438 - val_loss: 3.1316\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0484 - val_loss: 3.1054\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0424 - val_loss: 3.1306\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0472 - val_loss: 3.1044\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0409 - val_loss: 3.1297\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0461 - val_loss: 3.1034\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0395 - val_loss: 3.1287\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0449 - val_loss: 3.1024\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 55.9029 - val_loss: 54.1044\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 53.1372 - val_loss: 51.1760\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 50.2045 - val_loss: 47.8854\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 46.9093 - val_loss: 44.1400\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 43.1601 - val_loss: 39.9346\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 38.9549 - val_loss: 35.3323\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 34.3627 - val_loss: 30.5606\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 29.6284 - val_loss: 26.1207\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 25.3223 - val_loss: 22.7118\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 22.2088 - val_loss: 20.5775\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 20.3623 - val_loss: 18.9967\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 18.9823 - val_loss: 17.7244\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 17.8895 - val_loss: 16.6608\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.9786 - val_loss: 15.7430\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.1784 - val_loss: 14.9295\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.4504 - val_loss: 14.1870\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.7676 - val_loss: 13.4757\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 14.0933 - val_loss: 12.7441\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 13.3737 - val_loss: 11.9426\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.5485 - val_loss: 11.0936\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.6249 - val_loss: 10.2884\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 10.7438 - val_loss: 9.6201\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 10.0101 - val_loss: 9.1271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 9.4615 - val_loss: 8.8122\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.0766 - val_loss: 8.6342\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 8.8228 - val_loss: 8.5465\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 8.6651 - val_loss: 8.5156\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.5661 - val_loss: 8.4926\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 8.5003 - val_loss: 8.4782\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4510 - val_loss: 8.4563\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.4087 - val_loss: 8.4341\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.3687 - val_loss: 8.4039\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3287 - val_loss: 8.3707\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 8.2874 - val_loss: 8.3283\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2442 - val_loss: 8.2866\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.1984 - val_loss: 8.2342\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.1498 - val_loss: 8.1848\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0983 - val_loss: 8.1252\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.0439 - val_loss: 8.0688\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.9868 - val_loss: 8.0038\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.9269 - val_loss: 7.9419\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.8646 - val_loss: 7.8725\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.8001 - val_loss: 7.8056\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.7334 - val_loss: 7.7325\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.6647 - val_loss: 7.6619\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.5941 - val_loss: 7.5854\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 7.5220 - val_loss: 7.5122\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.4483 - val_loss: 7.4322\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.3732 - val_loss: 7.3588\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.2970 - val_loss: 7.2738\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.2197 - val_loss: 7.2027\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 7.1416 - val_loss: 7.1089\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.0628 - val_loss: 7.0464\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.9839 - val_loss: 6.9381\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 6.9053 - val_loss: 6.9002\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.8306 - val_loss: 6.7672\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.7576 - val_loss: 6.7806\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.6993 - val_loss: 6.6198\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.6473 - val_loss: 6.7376\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.6369 - val_loss: 6.5422\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.6200 - val_loss: 6.8058\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.6777 - val_loss: 6.5367\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.6540 - val_loss: 6.8983\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.7451 - val_loss: 6.4908\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.6155 - val_loss: 6.7739\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.6147 - val_loss: 6.2896\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.3985 - val_loss: 6.5008\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.3554 - val_loss: 6.0820\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.1830 - val_loss: 6.2636\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.1314 - val_loss: 5.8897\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.9788 - val_loss: 6.0532\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.9315 - val_loss: 5.7131\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 5.7940 - val_loss: 5.8496\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 5.7379 - val_loss: 5.5537\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.6338 - val_loss: 5.7042\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.5974 - val_loss: 5.4266\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.5075 - val_loss: 5.5798\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.4748 - val_loss: 5.3051\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.3870 - val_loss: 5.4651\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.3595 - val_loss: 5.1904\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.2737 - val_loss: 5.3530\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.2466 - val_loss: 5.0797\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 5.1646 - val_loss: 5.2451\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.1372 - val_loss: 4.9744\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.0611 - val_loss: 5.1378\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 5.0291 - val_loss: 4.8750\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.9630 - val_loss: 5.0425\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.9330 - val_loss: 4.7869\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.8776 - val_loss: 4.9597\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.8491 - val_loss: 4.7054\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.7991 - val_loss: 4.8760\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.7666 - val_loss: 4.6184\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.7140 - val_loss: 4.7981\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.6915 - val_loss: 4.5400\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.6372 - val_loss: 4.7244\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.6216 - val_loss: 4.4685\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.5666 - val_loss: 4.6532\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.5552 - val_loss: 4.4083\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.5064 - val_loss: 4.5905\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.4969 - val_loss: 4.3556\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.4534 - val_loss: 4.5334\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.4440 - val_loss: 4.3087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.4054 - val_loss: 4.4788\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.3937 - val_loss: 4.2658\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.3606 - val_loss: 4.4292\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.3480 - val_loss: 4.2270\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3186 - val_loss: 4.3855\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.3079 - val_loss: 4.1920\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.2797 - val_loss: 4.3460\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2717 - val_loss: 4.1552\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2389 - val_loss: 4.3063\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2357 - val_loss: 4.1256\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2050 - val_loss: 4.2714\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2041 - val_loss: 4.1003\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.1752 - val_loss: 4.2397\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.1750 - val_loss: 4.0779\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.1487 - val_loss: 4.2175\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.1537 - val_loss: 4.0597\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.1272 - val_loss: 4.1956\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.1330 - val_loss: 4.0392\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.1036 - val_loss: 4.1736\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.1108 - val_loss: 4.0192\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.0801 - val_loss: 4.1506\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.0878 - val_loss: 4.0006\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.0579 - val_loss: 4.1290\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.0661 - val_loss: 3.9833\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.0371 - val_loss: 4.1091\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0458 - val_loss: 3.9661\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0164 - val_loss: 4.0891\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0255 - val_loss: 3.9487\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.9960 - val_loss: 4.0695\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.0054 - val_loss: 3.9320\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.9768 - val_loss: 4.0517\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.9871 - val_loss: 3.9168\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9594 - val_loss: 4.0358\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.9705 - val_loss: 3.9026\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9430 - val_loss: 4.0204\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.9542 - val_loss: 3.8883\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9268 - val_loss: 4.0046\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9377 - val_loss: 3.8739\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9107 - val_loss: 3.9875\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9204 - val_loss: 3.8590\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8942 - val_loss: 3.9703\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.9035 - val_loss: 3.8447\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8784 - val_loss: 3.9595\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8920 - val_loss: 3.8329\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8652 - val_loss: 3.9481\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8808 - val_loss: 3.8223\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.8534 - val_loss: 3.9397\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8719 - val_loss: 3.8110\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8413 - val_loss: 3.9300\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8615 - val_loss: 3.7980\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.8274 - val_loss: 3.9182\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.8491 - val_loss: 3.7826\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8112 - val_loss: 3.9045\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.8350 - val_loss: 3.7683\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.7974 - val_loss: 3.8950\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.8242 - val_loss: 3.7548\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.7845 - val_loss: 3.8849\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8132 - val_loss: 3.7406\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.7701 - val_loss: 3.8726\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.8003 - val_loss: 3.7258\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7551 - val_loss: 3.8598\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7870 - val_loss: 3.7111\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7404 - val_loss: 3.8464\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7732 - val_loss: 3.6970\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.7262 - val_loss: 3.8331\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7590 - val_loss: 3.6826\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7120 - val_loss: 3.8225\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.7479 - val_loss: 3.6703\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6998 - val_loss: 3.8126\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.7376 - val_loss: 3.6585\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.6879 - val_loss: 3.8023\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7271 - val_loss: 3.6470\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.6764 - val_loss: 3.7923\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7168 - val_loss: 3.6360\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6653 - val_loss: 3.7826\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7067 - val_loss: 3.6253\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6546 - val_loss: 3.7729\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6967 - val_loss: 3.6143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6434 - val_loss: 3.7625\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.6862 - val_loss: 3.6025\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6310 - val_loss: 3.7511\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6747 - val_loss: 3.5900\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.6175 - val_loss: 3.7388\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6626 - val_loss: 3.5773\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6039 - val_loss: 3.7268\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6506 - val_loss: 3.5650\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5909 - val_loss: 3.7153\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.6389 - val_loss: 3.5530\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5782 - val_loss: 3.7036\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6269 - val_loss: 3.5407\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5653 - val_loss: 3.6914\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6144 - val_loss: 3.5284\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5523 - val_loss: 3.6796\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6022 - val_loss: 3.5167\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5397 - val_loss: 3.6686\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5911 - val_loss: 3.5060\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5281 - val_loss: 3.6586\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5812 - val_loss: 3.4963\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5176 - val_loss: 3.6497\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5723 - val_loss: 3.4878\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5084 - val_loss: 3.6415\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5643 - val_loss: 3.4803\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5001 - val_loss: 3.6308\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5543 - val_loss: 3.4728\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4918 - val_loss: 3.6208\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5446 - val_loss: 3.4654\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4837 - val_loss: 3.6208\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5442 - val_loss: 3.4613\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4792 - val_loss: 3.6085\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5329 - val_loss: 3.4550\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4718 - val_loss: 3.6023\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5272 - val_loss: 3.4506\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4667 - val_loss: 3.5967\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5224 - val_loss: 3.4467\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4621 - val_loss: 3.5904\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5168 - val_loss: 3.4425\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4572 - val_loss: 3.5835\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5106 - val_loss: 3.4379\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4520 - val_loss: 3.5768\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5046 - val_loss: 3.4332\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4467 - val_loss: 3.5703\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4987 - val_loss: 3.4286\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4414 - val_loss: 3.5639\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4929 - val_loss: 3.4240\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.4362 - val_loss: 3.5577\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4872 - val_loss: 3.4195\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4311 - val_loss: 3.5516\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4816 - val_loss: 3.4151\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4260 - val_loss: 3.5456\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4761 - val_loss: 3.4108\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4211 - val_loss: 3.5397\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.4707 - val_loss: 3.4066\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4161 - val_loss: 3.5339\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4654 - val_loss: 3.4024\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4113 - val_loss: 3.5283\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4603 - val_loss: 3.3983\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4065 - val_loss: 3.5228\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.4552 - val_loss: 3.3943\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4018 - val_loss: 3.5174\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4503 - val_loss: 3.3904\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3972 - val_loss: 3.5122\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4456 - val_loss: 3.3865\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3928 - val_loss: 3.5071\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4410 - val_loss: 3.3829\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3885 - val_loss: 3.5023\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4366 - val_loss: 3.3794\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3844 - val_loss: 3.4976\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4324 - val_loss: 3.3761\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3805 - val_loss: 3.4932\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4284 - val_loss: 3.3729\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.3768 - val_loss: 3.4888\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4245 - val_loss: 3.3699\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3733 - val_loss: 3.4846\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4207 - val_loss: 3.3670\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3698 - val_loss: 3.4802\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4169 - val_loss: 3.3641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.3663 - val_loss: 3.4759\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4130 - val_loss: 3.3612\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.3627 - val_loss: 3.4714\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4091 - val_loss: 3.3583\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3592 - val_loss: 3.4670\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.4052 - val_loss: 3.3554\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3556 - val_loss: 3.4626\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4013 - val_loss: 3.3525\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3521 - val_loss: 3.4583\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.3976 - val_loss: 3.3496\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3486 - val_loss: 3.4541\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3939 - val_loss: 3.3468\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3452 - val_loss: 3.4500\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3903 - val_loss: 3.3440\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.3419 - val_loss: 3.4460\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3868 - val_loss: 3.3413\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3386 - val_loss: 3.4420\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.3835 - val_loss: 3.3387\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3355 - val_loss: 3.4382\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3801 - val_loss: 3.3361\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3324 - val_loss: 3.4344\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.3769 - val_loss: 3.3336\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3294 - val_loss: 3.4306\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.3737 - val_loss: 3.3312\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3264 - val_loss: 3.4269\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.3706 - val_loss: 3.3289\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.3236 - val_loss: 3.4233\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3675 - val_loss: 3.3267\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3208 - val_loss: 3.4196\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.3644 - val_loss: 3.3246\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3180 - val_loss: 3.4160\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3613 - val_loss: 3.3225\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3154 - val_loss: 3.4125\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.3583 - val_loss: 3.3205\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3128 - val_loss: 3.4089\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3553 - val_loss: 3.3185\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 3.3103 - val_loss: 3.4054\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.3523 - val_loss: 3.3167\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3080 - val_loss: 3.4019\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3494 - val_loss: 3.3150\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3058 - val_loss: 3.3985\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.3465 - val_loss: 3.3138\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3039 - val_loss: 3.3953\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.3438 - val_loss: 3.3133\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3028 - val_loss: 3.3922\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.3413 - val_loss: 3.3138\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.3023 - val_loss: 3.3894\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3395 - val_loss: 3.3108\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.2981 - val_loss: 3.3856\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3366 - val_loss: 3.3097\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2963 - val_loss: 3.3826\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3342 - val_loss: 3.3081\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2941 - val_loss: 3.3795\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3315 - val_loss: 3.3060\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.2916 - val_loss: 3.3762\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3286 - val_loss: 3.3037\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.2888 - val_loss: 3.3730\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3256 - val_loss: 3.3013\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2860 - val_loss: 3.3698\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3225 - val_loss: 3.2987\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2830 - val_loss: 3.3666\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3194 - val_loss: 3.2961\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2799 - val_loss: 3.3634\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3162 - val_loss: 3.2934\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2769 - val_loss: 3.3602\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3131 - val_loss: 3.2907\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2737 - val_loss: 3.3571\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3099 - val_loss: 3.2879\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2706 - val_loss: 3.3540\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3067 - val_loss: 3.2851\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.2674 - val_loss: 3.3508\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.3035 - val_loss: 3.2823\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2642 - val_loss: 3.3477\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3003 - val_loss: 3.2795\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2610 - val_loss: 3.3446\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2970 - val_loss: 3.2767\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2578 - val_loss: 3.3415\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2938 - val_loss: 3.2740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2546 - val_loss: 3.3383\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2905 - val_loss: 3.2712\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 3.2515 - val_loss: 3.3352\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 3.2872 - val_loss: 3.2685\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2483 - val_loss: 3.3319\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2839 - val_loss: 3.2657\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2451 - val_loss: 3.3287\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2805 - val_loss: 3.2630\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2420 - val_loss: 3.3253\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2771 - val_loss: 3.2603\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2389 - val_loss: 3.3220\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.2736 - val_loss: 3.2576\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2357 - val_loss: 3.3186\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.2701 - val_loss: 3.2549\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2326 - val_loss: 3.3153\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2665 - val_loss: 3.2523\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2295 - val_loss: 3.3119\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2630 - val_loss: 3.2496\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2264 - val_loss: 3.3086\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2594 - val_loss: 3.2469\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2233 - val_loss: 3.3055\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.2559 - val_loss: 3.2443\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2202 - val_loss: 3.3024\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2525 - val_loss: 3.2417\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2172 - val_loss: 3.2995\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2493 - val_loss: 3.2392\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2143 - val_loss: 3.2967\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2462 - val_loss: 3.2368\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2115 - val_loss: 3.2941\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2432 - val_loss: 3.2346\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2088 - val_loss: 3.2917\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2404 - val_loss: 3.2324\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2062 - val_loss: 3.2893\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.2377 - val_loss: 3.2303\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 3.2037 - val_loss: 3.2870\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2352 - val_loss: 3.2283\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.2013 - val_loss: 3.2848\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2327 - val_loss: 3.2264\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1989 - val_loss: 3.2827\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2303 - val_loss: 3.2246\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1966 - val_loss: 3.2806\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2279 - val_loss: 3.2229\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1943 - val_loss: 3.2785\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2256 - val_loss: 3.2212\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1921 - val_loss: 3.2765\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2233 - val_loss: 3.2195\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1900 - val_loss: 3.2745\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2211 - val_loss: 3.2179\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1879 - val_loss: 3.2725\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2188 - val_loss: 3.2164\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1859 - val_loss: 3.2706\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2167 - val_loss: 3.2149\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1839 - val_loss: 3.2687\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2146 - val_loss: 3.2135\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1819 - val_loss: 3.2668\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2125 - val_loss: 3.2121\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1800 - val_loss: 3.2650\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2104 - val_loss: 3.2108\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1782 - val_loss: 3.2632\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2084 - val_loss: 3.2096\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1763 - val_loss: 3.2614\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.2065 - val_loss: 3.2084\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1745 - val_loss: 3.2596\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2045 - val_loss: 3.2072\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1728 - val_loss: 3.2578\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2026 - val_loss: 3.2060\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1710 - val_loss: 3.2561\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2008 - val_loss: 3.2048\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1691 - val_loss: 3.2543\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1989 - val_loss: 3.2036\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1672 - val_loss: 3.2526\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1971 - val_loss: 3.2024\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1653 - val_loss: 3.2508\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1952 - val_loss: 3.2011\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1634 - val_loss: 3.2491\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1934 - val_loss: 3.1998\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1614 - val_loss: 3.2474\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1915 - val_loss: 3.1985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1594 - val_loss: 3.2457\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1896 - val_loss: 3.1971\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1574 - val_loss: 3.2419\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1861 - val_loss: 3.1952\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1549 - val_loss: 3.2451\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1891 - val_loss: 3.1962\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1550 - val_loss: 3.2424\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1868 - val_loss: 3.1950\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1529 - val_loss: 3.2406\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1850 - val_loss: 3.1939\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1511 - val_loss: 3.2391\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1835 - val_loss: 3.1927\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1493 - val_loss: 3.2377\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1817 - val_loss: 3.1914\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1474 - val_loss: 3.2363\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.1799 - val_loss: 3.1900\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1455 - val_loss: 3.2348\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1780 - val_loss: 3.1885\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1435 - val_loss: 3.2334\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1761 - val_loss: 3.1871\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1416 - val_loss: 3.2320\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1742 - val_loss: 3.1856\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1397 - val_loss: 3.2307\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1724 - val_loss: 3.1841\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1378 - val_loss: 3.2293\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1705 - val_loss: 3.1827\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1359 - val_loss: 3.2280\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1687 - val_loss: 3.1813\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1341 - val_loss: 3.2267\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1669 - val_loss: 3.1800\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1323 - val_loss: 3.2254\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1651 - val_loss: 3.1786\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1305 - val_loss: 3.2241\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1633 - val_loss: 3.1773\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1287 - val_loss: 3.2229\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1615 - val_loss: 3.1761\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1270 - val_loss: 3.2217\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1598 - val_loss: 3.1748\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1253 - val_loss: 3.2204\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1581 - val_loss: 3.1736\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1237 - val_loss: 3.2192\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1564 - val_loss: 3.1724\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1220 - val_loss: 3.2180\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1547 - val_loss: 3.1712\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1204 - val_loss: 3.2168\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1530 - val_loss: 3.1701\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1188 - val_loss: 3.2157\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1513 - val_loss: 3.1689\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1172 - val_loss: 3.2145\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1497 - val_loss: 3.1678\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1156 - val_loss: 3.2133\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1481 - val_loss: 3.1667\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1141 - val_loss: 3.2122\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1464 - val_loss: 3.1657\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1126 - val_loss: 3.2110\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1448 - val_loss: 3.1646\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1111 - val_loss: 3.2099\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1432 - val_loss: 3.1636\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1096 - val_loss: 3.2088\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1416 - val_loss: 3.1626\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1081 - val_loss: 3.2076\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1400 - val_loss: 3.1616\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1066 - val_loss: 3.2065\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1384 - val_loss: 3.1606\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1052 - val_loss: 3.2054\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1369 - val_loss: 3.1597\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1038 - val_loss: 3.2043\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1353 - val_loss: 3.1587\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1024 - val_loss: 3.2032\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1338 - val_loss: 3.1578\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1010 - val_loss: 3.2021\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1322 - val_loss: 3.1569\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0996 - val_loss: 3.2010\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1307 - val_loss: 3.1560\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0982 - val_loss: 3.2000\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1292 - val_loss: 3.1552\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0969 - val_loss: 3.1989\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.1277 - val_loss: 3.1544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0955 - val_loss: 3.1979\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1262 - val_loss: 3.1535\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0942 - val_loss: 3.1969\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1248 - val_loss: 3.1527\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0929 - val_loss: 3.1958\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1234 - val_loss: 3.1520\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0917 - val_loss: 3.1949\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1220 - val_loss: 3.1512\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0904 - val_loss: 3.1939\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1206 - val_loss: 3.1505\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0892 - val_loss: 3.1929\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1192 - val_loss: 3.1497\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0879 - val_loss: 3.1920\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1179 - val_loss: 3.1490\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0867 - val_loss: 3.1911\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1166 - val_loss: 3.1483\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0855 - val_loss: 3.1902\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1153 - val_loss: 3.1476\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0843 - val_loss: 3.1894\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1140 - val_loss: 3.1469\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0831 - val_loss: 3.1885\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1127 - val_loss: 3.1462\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0820 - val_loss: 3.1877\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1115 - val_loss: 3.1456\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0808 - val_loss: 3.1869\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1102 - val_loss: 3.1449\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0797 - val_loss: 3.1861\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1090 - val_loss: 3.1442\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0785 - val_loss: 3.1854\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1078 - val_loss: 3.1435\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0774 - val_loss: 3.1846\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1066 - val_loss: 3.1428\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0762 - val_loss: 3.1839\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1054 - val_loss: 3.1422\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 38us/step - loss: 3.0751 - val_loss: 3.1832\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1043 - val_loss: 3.1415\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0740 - val_loss: 3.1825\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1031 - val_loss: 3.1408\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0728 - val_loss: 3.1818\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1019 - val_loss: 3.1401\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0717 - val_loss: 3.1811\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1008 - val_loss: 3.1394\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0706 - val_loss: 3.1805\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0996 - val_loss: 3.1386\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0694 - val_loss: 3.1798\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0985 - val_loss: 3.1379\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0683 - val_loss: 3.1792\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0974 - val_loss: 3.1372\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0671 - val_loss: 3.1786\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0962 - val_loss: 3.1364\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0660 - val_loss: 3.1780\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0951 - val_loss: 3.1356\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0648 - val_loss: 3.1774\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0939 - val_loss: 3.1348\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0637 - val_loss: 3.1769\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0928 - val_loss: 3.1340\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0625 - val_loss: 3.1763\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0917 - val_loss: 3.1332\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0614 - val_loss: 3.1758\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0905 - val_loss: 3.1324\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0602 - val_loss: 3.1752\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0894 - val_loss: 3.1316\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.0590 - val_loss: 3.1747\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0882 - val_loss: 3.1307\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0579 - val_loss: 3.1742\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0871 - val_loss: 3.1299\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0567 - val_loss: 3.1737\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0859 - val_loss: 3.1290\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0555 - val_loss: 3.1732\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.0848 - val_loss: 3.1281\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0542 - val_loss: 3.1726\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0836 - val_loss: 3.1272\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0530 - val_loss: 3.1721\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0824 - val_loss: 3.1262\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0517 - val_loss: 3.1716\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0812 - val_loss: 3.1253\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0504 - val_loss: 3.1710\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0800 - val_loss: 3.1243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0491 - val_loss: 3.1704\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0787 - val_loss: 3.1233\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0478 - val_loss: 3.1698\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0775 - val_loss: 3.1222\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0465 - val_loss: 3.1693\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0762 - val_loss: 3.1212\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0452 - val_loss: 3.1687\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0750 - val_loss: 3.1201\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0439 - val_loss: 3.1681\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0737 - val_loss: 3.1190\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0426 - val_loss: 3.1675\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0724 - val_loss: 3.1180\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0414 - val_loss: 3.1670\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0711 - val_loss: 3.1170\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.0403 - val_loss: 3.1665\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0698 - val_loss: 3.1161\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0392 - val_loss: 3.1660\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.0686 - val_loss: 3.1152\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0381 - val_loss: 3.1654\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0674 - val_loss: 3.1143\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0370 - val_loss: 3.1648\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0662 - val_loss: 3.1135\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0359 - val_loss: 3.1643\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0650 - val_loss: 3.1128\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.0349 - val_loss: 3.1637\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0639 - val_loss: 3.1122\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0339 - val_loss: 3.1631\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0629 - val_loss: 3.1116\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0330 - val_loss: 3.1626\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0619 - val_loss: 3.1111\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 56.5269 - val_loss: 53.3347\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 53.7725 - val_loss: 50.3406\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 50.7770 - val_loss: 46.9117\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 47.3492 - val_loss: 43.0344\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 43.4780 - val_loss: 38.8205\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 39.2756 - val_loss: 34.3872\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 34.8640 - val_loss: 29.9085\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 30.4356 - val_loss: 25.7289\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 26.3658 - val_loss: 22.3288\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 23.1250 - val_loss: 20.2606\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 21.0279 - val_loss: 18.8208\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 19.5658 - val_loss: 17.6750\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 18.3791 - val_loss: 16.7120\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 17.3709 - val_loss: 15.8548\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.4736 - val_loss: 15.0304\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.6137 - val_loss: 14.2000\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 14.7515 - val_loss: 13.3927\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 13.9147 - val_loss: 12.5973\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.0922 - val_loss: 11.7754\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 12.2466 - val_loss: 10.9348\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.3869 - val_loss: 10.1272\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.5774 - val_loss: 9.4513\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.9207 - val_loss: 8.9553\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 9.4524 - val_loss: 8.6080\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.1371 - val_loss: 8.3768\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.9347 - val_loss: 8.2252\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.8080 - val_loss: 8.1326\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.7278 - val_loss: 8.0683\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 8.6697 - val_loss: 8.0162\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.6211 - val_loss: 7.9727\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5785 - val_loss: 7.9353\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.5388 - val_loss: 7.8975\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.5006 - val_loss: 7.8611\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4625 - val_loss: 7.8232\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.4233 - val_loss: 7.7849\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.3827 - val_loss: 7.7446\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3403 - val_loss: 7.7033\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.2960 - val_loss: 7.6603\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2500 - val_loss: 7.6161\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.2022 - val_loss: 7.5707\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1527 - val_loss: 7.5253\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1016 - val_loss: 7.4760\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.0488 - val_loss: 7.4273\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.9943 - val_loss: 7.3751\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 7.9380 - val_loss: 7.3229\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.8799 - val_loss: 7.2677\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8200 - val_loss: 7.2122\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.7584 - val_loss: 7.1539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.6951 - val_loss: 7.0955\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.6301 - val_loss: 7.0344\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.5635 - val_loss: 6.9737\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.4954 - val_loss: 6.9110\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.4258 - val_loss: 6.8490\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.3549 - val_loss: 6.7800\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.2824 - val_loss: 6.7185\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.2084 - val_loss: 6.6438\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.1327 - val_loss: 6.5819\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.0550 - val_loss: 6.5008\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.9753 - val_loss: 6.4392\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.8931 - val_loss: 6.3492\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 6.8084 - val_loss: 6.2908\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.7207 - val_loss: 6.1865\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.6304 - val_loss: 6.1404\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.5379 - val_loss: 6.0126\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.4456 - val_loss: 6.0150\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 6.3596 - val_loss: 5.8477\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.2906 - val_loss: 6.0104\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.2674 - val_loss: 5.8763\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 6.3618 - val_loss: 6.5662\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 6.6663 - val_loss: 6.4178\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.9409 - val_loss: 7.1338\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.1383 - val_loss: 6.1953\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.6637 - val_loss: 6.5662\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.5943 - val_loss: 5.8463\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.2831 - val_loss: 6.2445\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.2757 - val_loss: 5.6613\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.0780 - val_loss: 6.0666\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 6.0823 - val_loss: 5.4674\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 5.8541 - val_loss: 5.8386\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.8480 - val_loss: 5.2480\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.6012 - val_loss: 5.6101\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.6175 - val_loss: 5.0715\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.3873 - val_loss: 5.3995\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.4067 - val_loss: 4.9313\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 5.2091 - val_loss: 5.2351\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.2351 - val_loss: 4.8277\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.0695 - val_loss: 5.1280\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 5.1093 - val_loss: 4.7434\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.9525 - val_loss: 5.0220\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.9840 - val_loss: 4.6621\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.8390 - val_loss: 4.9202\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.8641 - val_loss: 4.5937\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.7412 - val_loss: 4.8361\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.7587 - val_loss: 4.5278\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.6471 - val_loss: 4.7578\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.6598 - val_loss: 4.4674\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.5609 - val_loss: 4.6746\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.5599 - val_loss: 4.4097\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.4796 - val_loss: 4.6167\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.4812 - val_loss: 4.3644\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.4151 - val_loss: 4.5806\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.4213 - val_loss: 4.3300\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3659 - val_loss: 4.5594\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.3749 - val_loss: 4.3041\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3297 - val_loss: 4.5476\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.3378 - val_loss: 4.2834\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3024 - val_loss: 4.5429\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.3085 - val_loss: 4.2651\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2805 - val_loss: 4.5486\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 4.2903 - val_loss: 4.2490\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2631 - val_loss: 4.5420\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2650 - val_loss: 4.2264\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.2383 - val_loss: 4.5248\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.2333 - val_loss: 4.2018\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.2117 - val_loss: 4.5043\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.2012 - val_loss: 4.1773\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.1857 - val_loss: 4.4807\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.1687 - val_loss: 4.1486\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.1550 - val_loss: 4.4522\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.1342 - val_loss: 4.1243\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.1298 - val_loss: 4.4275\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.1043 - val_loss: 4.1007\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.1052 - val_loss: 4.4035\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0763 - val_loss: 4.0770\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.0802 - val_loss: 4.3803\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.0498 - val_loss: 4.0544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 4.0561 - val_loss: 4.3577\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.0247 - val_loss: 4.0320\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.0319 - val_loss: 4.3327\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.9982 - val_loss: 4.0079\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.0054 - val_loss: 4.3051\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9701 - val_loss: 3.9832\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9777 - val_loss: 4.2786\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.9434 - val_loss: 3.9590\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.9502 - val_loss: 4.2548\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.9196 - val_loss: 3.9357\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9241 - val_loss: 4.2322\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.8970 - val_loss: 3.9134\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.8994 - val_loss: 4.2062\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.8715 - val_loss: 3.8901\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.8735 - val_loss: 4.1786\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.8447 - val_loss: 3.8666\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8474 - val_loss: 4.1542\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8211 - val_loss: 3.8443\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8229 - val_loss: 4.1322\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7994 - val_loss: 3.8220\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.7987 - val_loss: 4.1068\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.7747 - val_loss: 3.7984\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7733 - val_loss: 4.0810\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7496 - val_loss: 3.7757\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.7488 - val_loss: 4.0585\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.7274 - val_loss: 3.7546\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7266 - val_loss: 4.0396\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7085 - val_loss: 3.7349\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.7060 - val_loss: 4.0220\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6908 - val_loss: 3.7157\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.6863 - val_loss: 4.0042\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6729 - val_loss: 3.6970\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.6670 - val_loss: 3.9873\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6558 - val_loss: 3.6793\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6490 - val_loss: 3.9718\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.6400 - val_loss: 3.6628\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6323 - val_loss: 3.9574\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6253 - val_loss: 3.6472\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6165 - val_loss: 3.9436\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.6113 - val_loss: 3.6323\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6015 - val_loss: 3.9302\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.5978 - val_loss: 3.6181\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5872 - val_loss: 3.9172\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 3.5846 - val_loss: 3.6044\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.5735 - val_loss: 3.9046\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.5720 - val_loss: 3.5914\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5605 - val_loss: 3.8926\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.5600 - val_loss: 3.5791\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5481 - val_loss: 3.8812\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5485 - val_loss: 3.5675\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5364 - val_loss: 3.8705\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5379 - val_loss: 3.5566\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.5254 - val_loss: 3.8615\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5288 - val_loss: 3.5467\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5155 - val_loss: 3.8541\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.5211 - val_loss: 3.5386\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5074 - val_loss: 3.8451\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.5121 - val_loss: 3.5303\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4990 - val_loss: 3.8357\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.5029 - val_loss: 3.5221\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4906 - val_loss: 3.8266\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4938 - val_loss: 3.5144\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4825 - val_loss: 3.8191\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4865 - val_loss: 3.5074\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.4753 - val_loss: 3.8127\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4802 - val_loss: 3.5007\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4685 - val_loss: 3.8055\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4733 - val_loss: 3.4940\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4615 - val_loss: 3.7979\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4659 - val_loss: 3.4874\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.4545 - val_loss: 3.7902\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4585 - val_loss: 3.4809\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.4476 - val_loss: 3.7825\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4511 - val_loss: 3.4744\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.4404 - val_loss: 3.7747\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4437 - val_loss: 3.4674\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4323 - val_loss: 3.7666\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4361 - val_loss: 3.4611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4247 - val_loss: 3.7597\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.4298 - val_loss: 3.4570\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4200 - val_loss: 3.7554\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4258 - val_loss: 3.4538\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4164 - val_loss: 3.7511\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4218 - val_loss: 3.4505\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4127 - val_loss: 3.7456\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4170 - val_loss: 3.4470\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4088 - val_loss: 3.7352\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4083 - val_loss: 3.4420\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4032 - val_loss: 3.7280\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4018 - val_loss: 3.4376\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3979 - val_loss: 3.7217\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3960 - val_loss: 3.4333\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3927 - val_loss: 3.7157\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3904 - val_loss: 3.4291\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3877 - val_loss: 3.7099\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3850 - val_loss: 3.4250\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3828 - val_loss: 3.7043\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3798 - val_loss: 3.4211\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3780 - val_loss: 3.6990\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3749 - val_loss: 3.4173\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3734 - val_loss: 3.6939\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3701 - val_loss: 3.4137\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3689 - val_loss: 3.6889\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3655 - val_loss: 3.4101\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3645 - val_loss: 3.6841\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3610 - val_loss: 3.4065\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3600 - val_loss: 3.6793\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3566 - val_loss: 3.4029\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3555 - val_loss: 3.6745\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3522 - val_loss: 3.3993\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3510 - val_loss: 3.6698\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3478 - val_loss: 3.3956\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3465 - val_loss: 3.6651\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3435 - val_loss: 3.3920\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3419 - val_loss: 3.6604\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3391 - val_loss: 3.3884\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3374 - val_loss: 3.6557\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3348 - val_loss: 3.3848\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3329 - val_loss: 3.6511\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3305 - val_loss: 3.3813\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3284 - val_loss: 3.6465\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3262 - val_loss: 3.3779\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3241 - val_loss: 3.6421\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3220 - val_loss: 3.3745\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3198 - val_loss: 3.6376\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.3179 - val_loss: 3.3712\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.3155 - val_loss: 3.6333\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3138 - val_loss: 3.3680\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3113 - val_loss: 3.6290\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.3098 - val_loss: 3.3648\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3072 - val_loss: 3.6248\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3058 - val_loss: 3.3617\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3031 - val_loss: 3.6207\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3019 - val_loss: 3.3586\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2991 - val_loss: 3.6166\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2980 - val_loss: 3.3556\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2951 - val_loss: 3.6125\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2942 - val_loss: 3.3526\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2911 - val_loss: 3.6085\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2904 - val_loss: 3.3496\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2872 - val_loss: 3.6046\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2866 - val_loss: 3.3467\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2833 - val_loss: 3.6007\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2829 - val_loss: 3.3437\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.2794 - val_loss: 3.5968\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2792 - val_loss: 3.3409\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2755 - val_loss: 3.5930\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2755 - val_loss: 3.3380\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2716 - val_loss: 3.5892\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2718 - val_loss: 3.3351\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2678 - val_loss: 3.5855\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2682 - val_loss: 3.3323\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2640 - val_loss: 3.5819\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2647 - val_loss: 3.3295\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2602 - val_loss: 3.5782\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2611 - val_loss: 3.3267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2564 - val_loss: 3.5747\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2576 - val_loss: 3.3239\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2526 - val_loss: 3.5711\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2541 - val_loss: 3.3212\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2489 - val_loss: 3.5677\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2507 - val_loss: 3.3160\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2414 - val_loss: 3.5615\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2451 - val_loss: 3.3128\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2370 - val_loss: 3.5577\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2415 - val_loss: 3.3104\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2336 - val_loss: 3.5547\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2385 - val_loss: 3.3081\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2303 - val_loss: 3.5517\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2354 - val_loss: 3.3056\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2269 - val_loss: 3.5486\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2323 - val_loss: 3.3031\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2233 - val_loss: 3.5455\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2290 - val_loss: 3.3004\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2197 - val_loss: 3.5423\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2257 - val_loss: 3.2977\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2161 - val_loss: 3.5391\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2224 - val_loss: 3.2950\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.2124 - val_loss: 3.5359\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2190 - val_loss: 3.2923\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2088 - val_loss: 3.5327\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2157 - val_loss: 3.2897\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2052 - val_loss: 3.5296\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2124 - val_loss: 3.2871\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2017 - val_loss: 3.5265\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2091 - val_loss: 3.2845\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1982 - val_loss: 3.5234\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2059 - val_loss: 3.2820\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1948 - val_loss: 3.5204\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2027 - val_loss: 3.2795\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1914 - val_loss: 3.5174\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1995 - val_loss: 3.2770\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1880 - val_loss: 3.5145\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1963 - val_loss: 3.2746\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1847 - val_loss: 3.5115\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1932 - val_loss: 3.2723\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1815 - val_loss: 3.5087\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1900 - val_loss: 3.2700\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1783 - val_loss: 3.5058\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1869 - val_loss: 3.2677\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1751 - val_loss: 3.5030\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1838 - val_loss: 3.2655\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1720 - val_loss: 3.5002\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1808 - val_loss: 3.2634\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1690 - val_loss: 3.4975\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1778 - val_loss: 3.2613\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1660 - val_loss: 3.4948\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1748 - val_loss: 3.2593\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1631 - val_loss: 3.4921\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1718 - val_loss: 3.2573\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1602 - val_loss: 3.4895\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1689 - val_loss: 3.2553\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1574 - val_loss: 3.4869\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1660 - val_loss: 3.2535\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1547 - val_loss: 3.4844\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1632 - val_loss: 3.2517\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1520 - val_loss: 3.4819\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1604 - val_loss: 3.2500\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1494 - val_loss: 3.4795\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1576 - val_loss: 3.2484\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1469 - val_loss: 3.4771\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1549 - val_loss: 3.2469\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1446 - val_loss: 3.4749\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1523 - val_loss: 3.2456\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1425 - val_loss: 3.4728\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1498 - val_loss: 3.2450\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1410 - val_loss: 3.4711\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1476 - val_loss: 3.2420\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1371 - val_loss: 3.4674\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1436 - val_loss: 3.2379\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1319 - val_loss: 3.4631\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1392 - val_loss: 3.2349\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1279 - val_loss: 3.4596\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1354 - val_loss: 3.2326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1248 - val_loss: 3.4568\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1323 - val_loss: 3.2308\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1221 - val_loss: 3.4545\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1296 - val_loss: 3.2294\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1198 - val_loss: 3.4524\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1272 - val_loss: 3.2282\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1177 - val_loss: 3.4505\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1249 - val_loss: 3.2271\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1158 - val_loss: 3.4487\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1228 - val_loss: 3.2261\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1139 - val_loss: 3.4470\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1208 - val_loss: 3.2252\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1122 - val_loss: 3.4454\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1188 - val_loss: 3.2242\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1104 - val_loss: 3.4437\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1168 - val_loss: 3.2233\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1087 - val_loss: 3.4421\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.1148 - val_loss: 3.2225\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1069 - val_loss: 3.4404\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1128 - val_loss: 3.2216\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1052 - val_loss: 3.4388\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1108 - val_loss: 3.2207\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1034 - val_loss: 3.4372\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1088 - val_loss: 3.2198\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1017 - val_loss: 3.4355\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1068 - val_loss: 3.2189\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1000 - val_loss: 3.4339\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1048 - val_loss: 3.2181\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0982 - val_loss: 3.4323\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1028 - val_loss: 3.2172\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0966 - val_loss: 3.4307\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.1008 - val_loss: 3.2164\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0949 - val_loss: 3.4291\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0988 - val_loss: 3.2156\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0932 - val_loss: 3.4275\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0969 - val_loss: 3.2148\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0916 - val_loss: 3.4260\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0949 - val_loss: 3.2141\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0900 - val_loss: 3.4245\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0930 - val_loss: 3.2133\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0884 - val_loss: 3.4229\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0911 - val_loss: 3.2126\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0868 - val_loss: 3.4214\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0891 - val_loss: 3.2118\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0852 - val_loss: 3.4199\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0872 - val_loss: 3.2111\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0837 - val_loss: 3.4184\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0853 - val_loss: 3.2104\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0821 - val_loss: 3.4169\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0834 - val_loss: 3.2096\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0805 - val_loss: 3.4154\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0815 - val_loss: 3.2089\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0790 - val_loss: 3.4139\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0796 - val_loss: 3.2081\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0774 - val_loss: 3.4124\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0776 - val_loss: 3.2074\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0758 - val_loss: 3.4109\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0757 - val_loss: 3.2066\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0742 - val_loss: 3.4095\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0738 - val_loss: 3.2058\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0726 - val_loss: 3.4081\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0719 - val_loss: 3.2051\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0710 - val_loss: 3.4067\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0699 - val_loss: 3.2043\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0694 - val_loss: 3.4053\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0680 - val_loss: 3.2035\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0678 - val_loss: 3.4040\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0661 - val_loss: 3.2027\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0662 - val_loss: 3.4026\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0642 - val_loss: 3.2019\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0646 - val_loss: 3.4012\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0623 - val_loss: 3.2012\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0630 - val_loss: 3.3999\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0604 - val_loss: 3.2004\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0614 - val_loss: 3.3985\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0585 - val_loss: 3.1996\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0597 - val_loss: 3.3972\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0566 - val_loss: 3.1989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0582 - val_loss: 3.3958\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0547 - val_loss: 3.1981\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0566 - val_loss: 3.3945\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0529 - val_loss: 3.1974\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0550 - val_loss: 3.3932\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.0510 - val_loss: 3.1966\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0534 - val_loss: 3.3919\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0492 - val_loss: 3.1959\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0518 - val_loss: 3.3906\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0474 - val_loss: 3.1951\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0502 - val_loss: 3.3893\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0455 - val_loss: 3.1944\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0486 - val_loss: 3.3880\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0437 - val_loss: 3.1937\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0470 - val_loss: 3.3868\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0419 - val_loss: 3.1929\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0455 - val_loss: 3.3855\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0401 - val_loss: 3.1922\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0439 - val_loss: 3.3843\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0384 - val_loss: 3.1915\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0423 - val_loss: 3.3830\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0366 - val_loss: 3.1908\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0408 - val_loss: 3.3818\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0348 - val_loss: 3.1901\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0392 - val_loss: 3.3805\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0331 - val_loss: 3.1894\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0377 - val_loss: 3.3793\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0313 - val_loss: 3.1887\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0361 - val_loss: 3.3781\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0296 - val_loss: 3.1880\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0346 - val_loss: 3.3769\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0279 - val_loss: 3.1873\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0331 - val_loss: 3.3757\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0262 - val_loss: 3.1866\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0316 - val_loss: 3.3745\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0245 - val_loss: 3.1859\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0301 - val_loss: 3.3734\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0228 - val_loss: 3.1852\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0286 - val_loss: 3.3722\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0212 - val_loss: 3.1846\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0271 - val_loss: 3.3711\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0195 - val_loss: 3.1839\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0256 - val_loss: 3.3700\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0179 - val_loss: 3.1833\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0242 - val_loss: 3.3689\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0163 - val_loss: 3.1826\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0227 - val_loss: 3.3678\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0147 - val_loss: 3.1820\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0213 - val_loss: 3.3667\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0131 - val_loss: 3.1813\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0199 - val_loss: 3.3656\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0116 - val_loss: 3.1807\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0184 - val_loss: 3.3645\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0100 - val_loss: 3.1801\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0170 - val_loss: 3.3635\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0085 - val_loss: 3.1794\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0156 - val_loss: 3.3625\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0069 - val_loss: 3.1788\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0142 - val_loss: 3.3614\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0054 - val_loss: 3.1782\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0128 - val_loss: 3.3604\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0039 - val_loss: 3.1775\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0114 - val_loss: 3.3594\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0024 - val_loss: 3.1769\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0099 - val_loss: 3.3584\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0009 - val_loss: 3.1763\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0085 - val_loss: 3.3574\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9994 - val_loss: 3.1756\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0071 - val_loss: 3.3564\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9979 - val_loss: 3.1750\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0057 - val_loss: 3.3554\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9964 - val_loss: 3.1743\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0043 - val_loss: 3.3544\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9949 - val_loss: 3.1737\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0028 - val_loss: 3.3534\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9934 - val_loss: 3.1731\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0014 - val_loss: 3.3524\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9920 - val_loss: 3.1724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0000 - val_loss: 3.3515\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9905 - val_loss: 3.1718\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9986 - val_loss: 3.3505\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9890 - val_loss: 3.1711\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9971 - val_loss: 3.3496\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9875 - val_loss: 3.1705\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9957 - val_loss: 3.3486\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9861 - val_loss: 3.1698\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9943 - val_loss: 3.3476\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.9846 - val_loss: 3.1692\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9929 - val_loss: 3.3467\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 2.9831 - val_loss: 3.1686\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9914 - val_loss: 3.3458\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9817 - val_loss: 3.1679\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9900 - val_loss: 3.3448\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9803 - val_loss: 3.1673\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9886 - val_loss: 3.3439\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9788 - val_loss: 3.1667\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9872 - val_loss: 3.3430\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9774 - val_loss: 3.1661\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9857 - val_loss: 3.3421\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9759 - val_loss: 3.1654\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9843 - val_loss: 3.3412\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9745 - val_loss: 3.1648\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.9829 - val_loss: 3.3403\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9731 - val_loss: 3.1642\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9815 - val_loss: 3.3394\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.9716 - val_loss: 3.1636\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9801 - val_loss: 3.3385\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9702 - val_loss: 3.1630\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9787 - val_loss: 3.3376\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9688 - val_loss: 3.1624\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9773 - val_loss: 3.3367\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 2.9674 - val_loss: 3.1618\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9759 - val_loss: 3.3358\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.9659 - val_loss: 3.1613\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9745 - val_loss: 3.3349\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 2.9645 - val_loss: 3.1607\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9731 - val_loss: 3.3341\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9631 - val_loss: 3.1601\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9717 - val_loss: 3.3332\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9617 - val_loss: 3.1595\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9702 - val_loss: 3.3323\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9603 - val_loss: 3.1590\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9688 - val_loss: 3.3314\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9589 - val_loss: 3.1584\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9674 - val_loss: 3.3305\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9574 - val_loss: 3.1578\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9660 - val_loss: 3.3296\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9560 - val_loss: 3.1573\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9646 - val_loss: 3.3287\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9546 - val_loss: 3.1567\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9632 - val_loss: 3.3278\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9531 - val_loss: 3.1561\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9618 - val_loss: 3.3269\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 2.9517 - val_loss: 3.1556\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9604 - val_loss: 3.3260\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9502 - val_loss: 3.1550\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9589 - val_loss: 3.3250\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9488 - val_loss: 3.1544\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9575 - val_loss: 3.3241\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9473 - val_loss: 3.1539\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9561 - val_loss: 3.3232\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9459 - val_loss: 3.1533\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9546 - val_loss: 3.3223\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9444 - val_loss: 3.1527\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9532 - val_loss: 3.3213\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9429 - val_loss: 3.1522\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9517 - val_loss: 3.3204\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9415 - val_loss: 3.1516\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9503 - val_loss: 3.3194\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9400 - val_loss: 3.1510\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9488 - val_loss: 3.3185\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9385 - val_loss: 3.1504\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9473 - val_loss: 3.3175\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9370 - val_loss: 3.1499\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9459 - val_loss: 3.3166\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.9355 - val_loss: 3.1493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9444 - val_loss: 3.3156\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9340 - val_loss: 3.1487\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9429 - val_loss: 3.3147\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9325 - val_loss: 3.1481\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9414 - val_loss: 3.3137\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9310 - val_loss: 3.1476\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 58.2616 - val_loss: 55.8689\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 55.6307 - val_loss: 53.3547\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 53.1153 - val_loss: 50.6455\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 50.4041 - val_loss: 47.4191\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 47.1756 - val_loss: 43.4488\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 43.2039 - val_loss: 38.7592\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 38.5094 - val_loss: 33.6374\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 33.3703 - val_loss: 28.5986\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 28.2866 - val_loss: 24.3674\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 23.9644 - val_loss: 21.5267\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 21.0589 - val_loss: 19.8924\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 19.4424 - val_loss: 18.7006\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 18.2779 - val_loss: 17.7633\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 17.3493 - val_loss: 16.9477\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 16.5439 - val_loss: 16.1522\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 15.7655 - val_loss: 15.3176\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 14.9526 - val_loss: 14.4304\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 14.0925 - val_loss: 13.4937\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 13.1923 - val_loss: 12.5544\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 12.3019 - val_loss: 11.6541\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 11.4486 - val_loss: 10.8100\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 10.6566 - val_loss: 10.0848\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.9811 - val_loss: 9.5160\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 9.4532 - val_loss: 9.1089\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.0761 - val_loss: 8.8357\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.8249 - val_loss: 8.6539\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.6619 - val_loss: 8.5286\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.5541 - val_loss: 8.4342\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4750 - val_loss: 8.3573\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.4076 - val_loss: 8.2888\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3452 - val_loss: 8.2247\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.2844 - val_loss: 8.1619\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.2224 - val_loss: 8.0979\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1585 - val_loss: 8.0331\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.0928 - val_loss: 7.9671\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.0250 - val_loss: 7.8998\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.9555 - val_loss: 7.8314\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 7.8846 - val_loss: 7.7623\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.8125 - val_loss: 7.6922\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.7394 - val_loss: 7.6213\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.6653 - val_loss: 7.5495\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.5903 - val_loss: 7.4771\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.5144 - val_loss: 7.4038\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.4377 - val_loss: 7.3298\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.3601 - val_loss: 7.2549\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.2817 - val_loss: 7.1796\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.2025 - val_loss: 7.1033\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.1224 - val_loss: 7.0266\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.0412 - val_loss: 6.9483\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.9591 - val_loss: 6.8701\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 6.8759 - val_loss: 6.7899\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.7915 - val_loss: 6.7094\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.7057 - val_loss: 6.6263\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 6.6185 - val_loss: 6.5437\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 6.5299 - val_loss: 6.4575\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.4398 - val_loss: 6.3735\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.3482 - val_loss: 6.2836\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.2554 - val_loss: 6.2002\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.1616 - val_loss: 6.1047\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 6.0672 - val_loss: 6.0284\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.9730 - val_loss: 5.9200\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.8803 - val_loss: 5.8735\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.7923 - val_loss: 5.7364\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 5.7175 - val_loss: 5.7922\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.6742 - val_loss: 5.6364\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 5.7014 - val_loss: 5.8980\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.7396 - val_loss: 5.7743\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.9478 - val_loss: 6.1329\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.9525 - val_loss: 5.9423\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.1774 - val_loss: 5.9753\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 5.7753 - val_loss: 5.6470\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.8485 - val_loss: 5.7767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 5.5702 - val_loss: 5.4479\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.6192 - val_loss: 5.6301\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.4176 - val_loss: 5.2987\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.4518 - val_loss: 5.4894\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.2706 - val_loss: 5.1565\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.2900 - val_loss: 5.3753\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.1500 - val_loss: 5.0406\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.1604 - val_loss: 5.2694\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.0385 - val_loss: 4.9346\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.0416 - val_loss: 5.1740\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.9376 - val_loss: 4.8351\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.9286 - val_loss: 5.0762\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.8357 - val_loss: 4.7414\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.8205 - val_loss: 4.9795\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.7363 - val_loss: 4.6568\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.7215 - val_loss: 4.8954\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.6490 - val_loss: 4.5843\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.6360 - val_loss: 4.8196\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.5699 - val_loss: 4.5169\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.5555 - val_loss: 4.7506\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.4984 - val_loss: 4.4560\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.4835 - val_loss: 4.6921\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.4378 - val_loss: 4.3982\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.4163 - val_loss: 4.6416\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.3861 - val_loss: 4.3492\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.3625 - val_loss: 4.5909\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.3363 - val_loss: 4.3045\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.3148 - val_loss: 4.5498\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.2955 - val_loss: 4.2649\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.2743 - val_loss: 4.5139\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.2600 - val_loss: 4.2292\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2388 - val_loss: 4.4787\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.2264 - val_loss: 4.1973\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2075 - val_loss: 4.4446\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.1944 - val_loss: 4.1641\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.1747 - val_loss: 4.4114\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.1631 - val_loss: 4.1317\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.1424 - val_loss: 4.3806\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.1338 - val_loss: 4.1015\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.1118 - val_loss: 4.3503\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.1046 - val_loss: 4.0755\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0854 - val_loss: 4.3261\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.0805 - val_loss: 4.0487\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0579 - val_loss: 4.3013\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.0554 - val_loss: 4.0232\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.0313 - val_loss: 4.2721\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0265 - val_loss: 3.9976\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0032 - val_loss: 4.2453\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9994 - val_loss: 3.9736\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9760 - val_loss: 4.2221\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9753 - val_loss: 3.9515\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9508 - val_loss: 4.2021\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9535 - val_loss: 3.9320\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 31us/step - loss: 3.9279 - val_loss: 4.1845\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.9333 - val_loss: 3.9145\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.9071 - val_loss: 4.1686\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.9142 - val_loss: 3.8986\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8874 - val_loss: 4.1538\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.8954 - val_loss: 3.8840\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8684 - val_loss: 4.1395\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8769 - val_loss: 3.8708\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8501 - val_loss: 4.1260\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8591 - val_loss: 3.8620\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8351 - val_loss: 4.1174\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.8453 - val_loss: 3.8517\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8185 - val_loss: 4.1065\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8297 - val_loss: 3.8405\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.8010 - val_loss: 4.0941\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8128 - val_loss: 3.8278\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7817 - val_loss: 4.0797\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7945 - val_loss: 3.8144\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7614 - val_loss: 4.0638\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7749 - val_loss: 3.8010\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.7411 - val_loss: 4.0466\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.7544 - val_loss: 3.7878\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7209 - val_loss: 4.0288\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.7334 - val_loss: 3.7754\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7015 - val_loss: 4.0135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7147 - val_loss: 3.7645\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6843 - val_loss: 3.9994\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.6970 - val_loss: 3.7545\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6682 - val_loss: 3.9853\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.6793 - val_loss: 3.7450\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6526 - val_loss: 3.9713\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.6618 - val_loss: 3.7360\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6377 - val_loss: 3.9576\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.6444 - val_loss: 3.7275\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6233 - val_loss: 3.9443\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6276 - val_loss: 3.7194\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6094 - val_loss: 3.9315\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.6112 - val_loss: 3.7115\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5959 - val_loss: 3.9190\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.5952 - val_loss: 3.7038\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5825 - val_loss: 3.9065\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.5792 - val_loss: 3.6959\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5691 - val_loss: 3.8938\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5632 - val_loss: 3.6879\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5556 - val_loss: 3.8812\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.5472 - val_loss: 3.6797\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5421 - val_loss: 3.8689\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5315 - val_loss: 3.6715\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.5287 - val_loss: 3.8570\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5163 - val_loss: 3.6633\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5155 - val_loss: 3.8453\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5013 - val_loss: 3.6549\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.5022 - val_loss: 3.8336\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4862 - val_loss: 3.6458\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.4883 - val_loss: 3.8204\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4696 - val_loss: 3.6351\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4726 - val_loss: 3.8047\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4504 - val_loss: 3.6225\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4547 - val_loss: 3.7889\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4312 - val_loss: 3.6103\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4377 - val_loss: 3.7745\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4136 - val_loss: 3.5998\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.4229 - val_loss: 3.7622\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3981 - val_loss: 3.5909\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4104 - val_loss: 3.7519\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3845 - val_loss: 3.5833\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3994 - val_loss: 3.7431\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3723 - val_loss: 3.5766\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3897 - val_loss: 3.7361\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3620 - val_loss: 3.5712\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3814 - val_loss: 3.7312\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3538 - val_loss: 3.5669\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3746 - val_loss: 3.7277\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3472 - val_loss: 3.5635\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3687 - val_loss: 3.7248\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3413 - val_loss: 3.5604\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3635 - val_loss: 3.7221\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3357 - val_loss: 3.5575\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3585 - val_loss: 3.7194\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.3301 - val_loss: 3.5546\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3535 - val_loss: 3.7164\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3244 - val_loss: 3.5516\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.3485 - val_loss: 3.7135\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3190 - val_loss: 3.5489\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3438 - val_loss: 3.7111\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3141 - val_loss: 3.5463\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3394 - val_loss: 3.7091\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3097 - val_loss: 3.5439\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.3352 - val_loss: 3.7070\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3052 - val_loss: 3.5413\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.3309 - val_loss: 3.7048\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3006 - val_loss: 3.5387\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3266 - val_loss: 3.7023\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2958 - val_loss: 3.5358\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3220 - val_loss: 3.6995\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.2907 - val_loss: 3.5328\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3173 - val_loss: 3.6965\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.2855 - val_loss: 3.5297\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3126 - val_loss: 3.6933\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2802 - val_loss: 3.5265\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.3077 - val_loss: 3.6901\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2749 - val_loss: 3.5234\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3029 - val_loss: 3.6869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2696 - val_loss: 3.5202\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2982 - val_loss: 3.6837\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2643 - val_loss: 3.5171\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2935 - val_loss: 3.6807\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.2591 - val_loss: 3.5141\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2890 - val_loss: 3.6777\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2540 - val_loss: 3.5112\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2845 - val_loss: 3.6748\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2491 - val_loss: 3.5084\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2802 - val_loss: 3.6720\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2442 - val_loss: 3.5057\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2760 - val_loss: 3.6694\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2395 - val_loss: 3.5031\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.2720 - val_loss: 3.6668\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2350 - val_loss: 3.5007\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.2681 - val_loss: 3.6644\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2306 - val_loss: 3.4984\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 36us/step - loss: 3.2642 - val_loss: 3.6623\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.2265 - val_loss: 3.4954\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.2592 - val_loss: 3.6600\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2221 - val_loss: 3.4932\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2555 - val_loss: 3.6578\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.2181 - val_loss: 3.4911\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2521 - val_loss: 3.6556\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2139 - val_loss: 3.4888\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2484 - val_loss: 3.6531\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.2097 - val_loss: 3.4863\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2447 - val_loss: 3.6506\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.2055 - val_loss: 3.4838\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2410 - val_loss: 3.6481\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.2012 - val_loss: 3.4812\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2372 - val_loss: 3.6455\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1970 - val_loss: 3.4787\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2335 - val_loss: 3.6430\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1929 - val_loss: 3.4762\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2298 - val_loss: 3.6404\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1888 - val_loss: 3.4737\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2262 - val_loss: 3.6379\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1848 - val_loss: 3.4713\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2227 - val_loss: 3.6355\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1808 - val_loss: 3.4689\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2192 - val_loss: 3.6330\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1769 - val_loss: 3.4665\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.2158 - val_loss: 3.6306\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1731 - val_loss: 3.4642\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2124 - val_loss: 3.6282\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1693 - val_loss: 3.4619\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2090 - val_loss: 3.6258\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1655 - val_loss: 3.4596\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.2057 - val_loss: 3.6234\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1617 - val_loss: 3.4573\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.2023 - val_loss: 3.6210\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1580 - val_loss: 3.4549\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1989 - val_loss: 3.6186\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1543 - val_loss: 3.4526\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1955 - val_loss: 3.6162\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1506 - val_loss: 3.4502\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1921 - val_loss: 3.6138\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1469 - val_loss: 3.4477\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1886 - val_loss: 3.6113\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1432 - val_loss: 3.4452\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1851 - val_loss: 3.6089\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1396 - val_loss: 3.4427\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1816 - val_loss: 3.6065\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1360 - val_loss: 3.4402\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1781 - val_loss: 3.6041\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1324 - val_loss: 3.4377\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1747 - val_loss: 3.6017\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1289 - val_loss: 3.4353\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1713 - val_loss: 3.5995\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1256 - val_loss: 3.4330\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1681 - val_loss: 3.5973\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1223 - val_loss: 3.4308\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1649 - val_loss: 3.5953\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1192 - val_loss: 3.4287\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1619 - val_loss: 3.5934\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1162 - val_loss: 3.4268\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1591 - val_loss: 3.5916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1133 - val_loss: 3.4250\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.1564 - val_loss: 3.5899\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1106 - val_loss: 3.4233\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1539 - val_loss: 3.5883\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1080 - val_loss: 3.4218\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1515 - val_loss: 3.5868\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1055 - val_loss: 3.4205\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1492 - val_loss: 3.5853\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1030 - val_loss: 3.4192\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1471 - val_loss: 3.5840\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1007 - val_loss: 3.4181\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1451 - val_loss: 3.5827\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0984 - val_loss: 3.4170\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1432 - val_loss: 3.5814\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0962 - val_loss: 3.4161\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1414 - val_loss: 3.5802\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0940 - val_loss: 3.4152\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1398 - val_loss: 3.5790\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.0919 - val_loss: 3.4143\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1381 - val_loss: 3.5778\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0898 - val_loss: 3.4133\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1364 - val_loss: 3.5766\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0876 - val_loss: 3.4123\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1347 - val_loss: 3.5754\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0855 - val_loss: 3.4112\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1328 - val_loss: 3.5740\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0833 - val_loss: 3.4099\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1309 - val_loss: 3.5727\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0810 - val_loss: 3.4086\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1289 - val_loss: 3.5712\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0788 - val_loss: 3.4073\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1268 - val_loss: 3.5698\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0765 - val_loss: 3.4059\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1246 - val_loss: 3.5683\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0742 - val_loss: 3.4045\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1225 - val_loss: 3.5669\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0720 - val_loss: 3.4030\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1203 - val_loss: 3.5655\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0698 - val_loss: 3.4017\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1182 - val_loss: 3.5641\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0676 - val_loss: 3.4003\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1161 - val_loss: 3.5627\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0655 - val_loss: 3.3990\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1140 - val_loss: 3.5614\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0634 - val_loss: 3.3977\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1120 - val_loss: 3.5601\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0614 - val_loss: 3.3965\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1100 - val_loss: 3.5589\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0595 - val_loss: 3.3953\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1081 - val_loss: 3.5577\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0576 - val_loss: 3.3941\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1062 - val_loss: 3.5566\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0557 - val_loss: 3.3930\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1043 - val_loss: 3.5554\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0539 - val_loss: 3.3919\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1024 - val_loss: 3.5543\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0521 - val_loss: 3.3908\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1005 - val_loss: 3.5532\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0503 - val_loss: 3.3897\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0987 - val_loss: 3.5521\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0486 - val_loss: 3.3886\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0969 - val_loss: 3.5510\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0469 - val_loss: 3.3876\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0951 - val_loss: 3.5499\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0452 - val_loss: 3.3866\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0933 - val_loss: 3.5489\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0435 - val_loss: 3.3856\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0915 - val_loss: 3.5478\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0419 - val_loss: 3.3846\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0898 - val_loss: 3.5468\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0403 - val_loss: 3.3836\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0882 - val_loss: 3.5458\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0387 - val_loss: 3.3827\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0865 - val_loss: 3.5448\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0372 - val_loss: 3.3818\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0849 - val_loss: 3.5439\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0358 - val_loss: 3.3810\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0834 - val_loss: 3.5431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0345 - val_loss: 3.3802\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0820 - val_loss: 3.5423\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0332 - val_loss: 3.3796\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0807 - val_loss: 3.5417\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0321 - val_loss: 3.3790\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0794 - val_loss: 3.5411\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0312 - val_loss: 3.3787\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.0784 - val_loss: 3.5405\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0302 - val_loss: 3.3784\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0774 - val_loss: 3.5397\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0290 - val_loss: 3.3781\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0764 - val_loss: 3.5389\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0278 - val_loss: 3.3777\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0753 - val_loss: 3.5380\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0267 - val_loss: 3.3772\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0741 - val_loss: 3.5372\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0255 - val_loss: 3.3766\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0728 - val_loss: 3.5362\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0241 - val_loss: 3.3757\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0713 - val_loss: 3.5350\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0226 - val_loss: 3.3747\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0696 - val_loss: 3.5337\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0209 - val_loss: 3.3735\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0678 - val_loss: 3.5322\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0191 - val_loss: 3.3718\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0653 - val_loss: 3.5303\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0169 - val_loss: 3.3689\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0597 - val_loss: 3.5271\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0133 - val_loss: 3.3688\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0587 - val_loss: 3.5267\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0124 - val_loss: 3.3688\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0581 - val_loss: 3.5262\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0116 - val_loss: 3.3684\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0571 - val_loss: 3.5254\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0104 - val_loss: 3.3675\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0556 - val_loss: 3.5241\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0088 - val_loss: 3.3663\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0538 - val_loss: 3.5226\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.0070 - val_loss: 3.3649\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0517 - val_loss: 3.5209\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0050 - val_loss: 3.3632\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0494 - val_loss: 3.5191\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0029 - val_loss: 3.3615\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0471 - val_loss: 3.5172\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0007 - val_loss: 3.3597\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0447 - val_loss: 3.5153\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9986 - val_loss: 3.3579\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0424 - val_loss: 3.5134\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9964 - val_loss: 3.3561\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0400 - val_loss: 3.5116\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9943 - val_loss: 3.3543\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0377 - val_loss: 3.5097\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9922 - val_loss: 3.3526\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.0355 - val_loss: 3.5079\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9902 - val_loss: 3.3510\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0333 - val_loss: 3.5062\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9883 - val_loss: 3.3494\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0312 - val_loss: 3.5045\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9864 - val_loss: 3.3478\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0292 - val_loss: 3.5029\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9846 - val_loss: 3.3463\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0272 - val_loss: 3.5013\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9828 - val_loss: 3.3449\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0253 - val_loss: 3.4997\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9811 - val_loss: 3.3435\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0234 - val_loss: 3.4982\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9794 - val_loss: 3.3422\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0216 - val_loss: 3.4968\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9778 - val_loss: 3.3409\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0198 - val_loss: 3.4953\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9762 - val_loss: 3.3396\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.0181 - val_loss: 3.4939\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9747 - val_loss: 3.3384\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0164 - val_loss: 3.4925\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.9732 - val_loss: 3.3371\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0147 - val_loss: 3.4912\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9717 - val_loss: 3.3359\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0130 - val_loss: 3.4898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9702 - val_loss: 3.3347\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0114 - val_loss: 3.4885\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9688 - val_loss: 3.3335\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0098 - val_loss: 3.4872\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9673 - val_loss: 3.3323\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0082 - val_loss: 3.4859\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9659 - val_loss: 3.3312\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0066 - val_loss: 3.4846\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9645 - val_loss: 3.3300\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0050 - val_loss: 3.4833\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9632 - val_loss: 3.3289\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0035 - val_loss: 3.4820\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 2.9618 - val_loss: 3.3278\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0020 - val_loss: 3.4808\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 2.9605 - val_loss: 3.3267\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0005 - val_loss: 3.4796\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.9592 - val_loss: 3.3256\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9990 - val_loss: 3.4784\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 2.9579 - val_loss: 3.3245\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9976 - val_loss: 3.4772\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9566 - val_loss: 3.3235\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9961 - val_loss: 3.4760\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9554 - val_loss: 3.3225\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9947 - val_loss: 3.4749\n",
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9542 - val_loss: 3.3215\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9934 - val_loss: 3.4738\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 2.9530 - val_loss: 3.3205\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.9920 - val_loss: 3.4727\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9519 - val_loss: 3.3196\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.9907 - val_loss: 3.4716\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9507 - val_loss: 3.3186\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9893 - val_loss: 3.4705\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9496 - val_loss: 3.3177\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.9880 - val_loss: 3.4694\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9485 - val_loss: 3.3166\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9866 - val_loss: 3.4684\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9474 - val_loss: 3.3156\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9851 - val_loss: 3.4673\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9463 - val_loss: 3.3144\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9834 - val_loss: 3.4661\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9451 - val_loss: 3.3130\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9816 - val_loss: 3.4649\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9438 - val_loss: 3.3112\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9794 - val_loss: 3.4634\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 2.9424 - val_loss: 3.3093\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9771 - val_loss: 3.4618\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9408 - val_loss: 3.3078\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9756 - val_loss: 3.4603\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9394 - val_loss: 3.3067\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9744 - val_loss: 3.4590\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9381 - val_loss: 3.3056\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9731 - val_loss: 3.4577\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9369 - val_loss: 3.3045\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9718 - val_loss: 3.4565\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9357 - val_loss: 3.3035\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9705 - val_loss: 3.4555\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9347 - val_loss: 3.3026\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.9693 - val_loss: 3.4545\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9338 - val_loss: 3.3016\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9679 - val_loss: 3.4535\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9330 - val_loss: 3.2974\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9628 - val_loss: 3.4508\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9302 - val_loss: 3.2944\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9593 - val_loss: 3.4483\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9275 - val_loss: 3.2927\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9574 - val_loss: 3.4465\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9257 - val_loss: 3.2912\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9557 - val_loss: 3.4450\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9243 - val_loss: 3.2898\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9542 - val_loss: 3.4438\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9231 - val_loss: 3.2888\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9529 - val_loss: 3.4428\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9222 - val_loss: 3.2880\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9519 - val_loss: 3.4420\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9215 - val_loss: 3.2873\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9509 - val_loss: 3.4413\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9209 - val_loss: 3.2868\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9502 - val_loss: 3.4407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9204 - val_loss: 3.2865\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9495 - val_loss: 3.4402\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9200 - val_loss: 3.2861\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9489 - val_loss: 3.4398\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9197 - val_loss: 3.2858\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9483 - val_loss: 3.4394\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9193 - val_loss: 3.2856\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9477 - val_loss: 3.4390\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9190 - val_loss: 3.2853\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9472 - val_loss: 3.4386\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9187 - val_loss: 3.2851\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9466 - val_loss: 3.4381\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9184 - val_loss: 3.2848\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9460 - val_loss: 3.4377\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9181 - val_loss: 3.2846\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9454 - val_loss: 3.4372\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 2.9177 - val_loss: 3.2843\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9448 - val_loss: 3.4368\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9174 - val_loss: 3.2840\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9442 - val_loss: 3.4363\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.9170 - val_loss: 3.2837\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9436 - val_loss: 3.4358\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9166 - val_loss: 3.2834\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9430 - val_loss: 3.4353\n",
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9163 - val_loss: 3.2831\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9423 - val_loss: 3.4347\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9159 - val_loss: 3.2828\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9417 - val_loss: 3.4342\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9155 - val_loss: 3.2825\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9411 - val_loss: 3.4337\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9151 - val_loss: 3.2822\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9404 - val_loss: 3.4331\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9147 - val_loss: 3.2819\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9398 - val_loss: 3.4326\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9142 - val_loss: 3.2816\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9392 - val_loss: 3.4320\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9138 - val_loss: 3.2813\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9386 - val_loss: 3.4314\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9134 - val_loss: 3.2810\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9379 - val_loss: 3.4309\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9130 - val_loss: 3.2806\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9373 - val_loss: 3.4303\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9125 - val_loss: 3.2803\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.9367 - val_loss: 3.4297\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9121 - val_loss: 3.2799\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9361 - val_loss: 3.4290\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9116 - val_loss: 3.2796\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 28us/step - loss: 2.9354 - val_loss: 3.4284\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9112 - val_loss: 3.2792\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9348 - val_loss: 3.4278\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9107 - val_loss: 3.2788\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9341 - val_loss: 3.4271\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9102 - val_loss: 3.2783\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9334 - val_loss: 3.4264\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.9097 - val_loss: 3.2779\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9327 - val_loss: 3.4257\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.9091 - val_loss: 3.2774\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 2.9320 - val_loss: 3.4250\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9086 - val_loss: 3.2770\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9313 - val_loss: 3.4243\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 56.4637 - val_loss: 52.6627\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 53.6884 - val_loss: 49.6520\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 50.6695 - val_loss: 46.1450\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 47.1529 - val_loss: 42.0146\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 43.0099 - val_loss: 37.3641\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 38.3388 - val_loss: 32.4912\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 33.4218 - val_loss: 27.8662\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 28.6890 - val_loss: 24.1047\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 24.6806 - val_loss: 21.6326\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 21.9027 - val_loss: 20.1666\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 20.2458 - val_loss: 19.0679\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 19.0039 - val_loss: 18.2020\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 18.0305 - val_loss: 17.5120\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.2564 - val_loss: 16.9148\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 16.5954 - val_loss: 16.3107\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.9485 - val_loss: 15.6455\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.2575 - val_loss: 14.9743\n",
      "Epoch 18/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 11us/step - loss: 14.5674 - val_loss: 14.3448\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 13.9169 - val_loss: 13.7065\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 13.2594 - val_loss: 12.9766\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 12.5261 - val_loss: 12.0993\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.6846 - val_loss: 11.1855\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.8218 - val_loss: 10.4242\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 10.0798 - val_loss: 9.8798\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 9.5220 - val_loss: 9.4964\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 9.1319 - val_loss: 9.2410\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.8749 - val_loss: 9.0725\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.7110 - val_loss: 8.9603\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6068 - val_loss: 8.8825\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.5374 - val_loss: 8.8250\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.4856 - val_loss: 8.7750\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.4407 - val_loss: 8.7328\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 8.3999 - val_loss: 8.6925\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3602 - val_loss: 8.6538\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3203 - val_loss: 8.6136\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.2791 - val_loss: 8.5720\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 8.2361 - val_loss: 8.5291\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.1910 - val_loss: 8.4838\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.1438 - val_loss: 8.4372\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0944 - val_loss: 8.3881\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.0430 - val_loss: 8.3382\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.9896 - val_loss: 8.2847\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.9342 - val_loss: 8.2313\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.8771 - val_loss: 8.1739\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.8184 - val_loss: 8.1177\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 7.7584 - val_loss: 8.0568\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.6973 - val_loss: 7.9989\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.6353 - val_loss: 7.9344\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 7.5725 - val_loss: 7.8774\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.5093 - val_loss: 7.8036\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 7.4462 - val_loss: 7.7639\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 7.3841 - val_loss: 7.6607\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 43us/step - loss: 7.3255 - val_loss: 7.6792\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.2728 - val_loss: 7.5202\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.2378 - val_loss: 7.6866\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.2302 - val_loss: 7.4805\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.3062 - val_loss: 7.9890\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.4612 - val_loss: 7.8247\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.8106 - val_loss: 8.4181\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8493 - val_loss: 7.8581\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8935 - val_loss: 8.0698\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.5153 - val_loss: 7.5110\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.4921 - val_loss: 7.7950\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.2515 - val_loss: 7.2918\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 7.2251 - val_loss: 7.5995\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.0632 - val_loss: 7.1050\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.0110 - val_loss: 7.3738\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 6.8509 - val_loss: 6.8881\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.7574 - val_loss: 7.1377\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 6.6309 - val_loss: 6.6916\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.5293 - val_loss: 6.9222\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 6.4291 - val_loss: 6.5008\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 6.3120 - val_loss: 6.6988\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 6.2212 - val_loss: 6.3168\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.1090 - val_loss: 6.4849\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.0191 - val_loss: 6.1308\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.9122 - val_loss: 6.2767\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.8204 - val_loss: 5.9503\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.7262 - val_loss: 6.0789\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.6314 - val_loss: 5.7748\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.5500 - val_loss: 5.9095\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.4652 - val_loss: 5.6113\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.3936 - val_loss: 5.7625\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 5.3209 - val_loss: 5.4719\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.2669 - val_loss: 5.6563\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.2164 - val_loss: 5.3606\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.1785 - val_loss: 5.5622\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.1312 - val_loss: 5.2583\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.0979 - val_loss: 5.4635\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.0472 - val_loss: 5.1540\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.0135 - val_loss: 5.3488\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.9517 - val_loss: 5.0365\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.9108 - val_loss: 5.2132\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 4.8387 - val_loss: 4.9209\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.8058 - val_loss: 5.0861\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.7343 - val_loss: 4.8116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.7068 - val_loss: 4.9736\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.6428 - val_loss: 4.7135\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.6175 - val_loss: 4.8651\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 4.5526 - val_loss: 4.6276\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 4.5395 - val_loss: 4.7821\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 4.4863 - val_loss: 4.5595\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 4.4798 - val_loss: 4.7135\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.4341 - val_loss: 4.4958\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 4.4242 - val_loss: 4.6472\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.3840 - val_loss: 4.4354\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 4.3700 - val_loss: 4.5796\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.3317 - val_loss: 4.3744\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 4.3134 - val_loss: 4.5107\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 4.2777 - val_loss: 4.3180\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 4.2608 - val_loss: 4.4481\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.2290 - val_loss: 4.2647\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2116 - val_loss: 4.3871\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 4.1802 - val_loss: 4.2128\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.1641 - val_loss: 4.3302\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 4.1344 - val_loss: 4.1617\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.1169 - val_loss: 4.2783\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.0929 - val_loss: 4.1150\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.0738 - val_loss: 4.2298\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.0545 - val_loss: 4.0729\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 26us/step - loss: 4.0363 - val_loss: 4.1854\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 4.0197 - val_loss: 4.0346\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 4.0034 - val_loss: 4.1451\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9884 - val_loss: 4.0001\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.9752 - val_loss: 4.1068\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9585 - val_loss: 3.9659\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9478 - val_loss: 4.0685\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.9280 - val_loss: 3.9293\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.9179 - val_loss: 4.0294\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8964 - val_loss: 3.8954\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.8895 - val_loss: 3.9932\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.8671 - val_loss: 3.8603\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.8603 - val_loss: 3.9577\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.8378 - val_loss: 3.8308\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.8353 - val_loss: 3.9273\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.8132 - val_loss: 3.8024\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.8119 - val_loss: 3.8981\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.7896 - val_loss: 3.7753\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7899 - val_loss: 3.8703\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7670 - val_loss: 3.7495\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.7691 - val_loss: 3.8436\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.7454 - val_loss: 3.7246\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7491 - val_loss: 3.8181\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.7247 - val_loss: 3.7005\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.7299 - val_loss: 3.7937\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7046 - val_loss: 3.6778\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.7121 - val_loss: 3.7694\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6842 - val_loss: 3.6560\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.6950 - val_loss: 3.7463\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.6647 - val_loss: 3.6349\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6783 - val_loss: 3.7246\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6469 - val_loss: 3.6062\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6552 - val_loss: 3.6988\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6253 - val_loss: 3.5829\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6354 - val_loss: 3.6771\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6075 - val_loss: 3.5633\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.6190 - val_loss: 3.6582\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5923 - val_loss: 3.5462\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6047 - val_loss: 3.6412\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5787 - val_loss: 3.5306\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.5918 - val_loss: 3.6255\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.5662 - val_loss: 3.5156\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.5793 - val_loss: 3.6105\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.5544 - val_loss: 3.5010\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5669 - val_loss: 3.5966\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5431 - val_loss: 3.4864\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.5544 - val_loss: 3.5792\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.5282 - val_loss: 3.4699\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.5389 - val_loss: 3.5626\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.5147 - val_loss: 3.4520\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.5198 - val_loss: 3.5413\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 29us/step - loss: 3.4980 - val_loss: 3.4367\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.5033 - val_loss: 3.5263\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.4862 - val_loss: 3.4269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4936 - val_loss: 3.5173\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4792 - val_loss: 3.4212\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4885 - val_loss: 3.5124\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.4757 - val_loss: 3.4179\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4857 - val_loss: 3.5094\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.4739 - val_loss: 3.4151\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.4832 - val_loss: 3.5068\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4726 - val_loss: 3.4126\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.4809 - val_loss: 3.5043\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4714 - val_loss: 3.4101\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.4787 - val_loss: 3.5021\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4704 - val_loss: 3.4077\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4768 - val_loss: 3.5033\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4726 - val_loss: 3.4086\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4784 - val_loss: 3.5044\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4749 - val_loss: 3.4077\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4782 - val_loss: 3.5029\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4751 - val_loss: 3.4047\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.4756 - val_loss: 3.4978\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.4722 - val_loss: 3.3998\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4705 - val_loss: 3.4905\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.4671 - val_loss: 3.3924\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4625 - val_loss: 3.4817\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.4608 - val_loss: 3.3835\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4532 - val_loss: 3.4728\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.4539 - val_loss: 3.3750\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4446 - val_loss: 3.4648\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4473 - val_loss: 3.3667\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4367 - val_loss: 3.4574\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.4409 - val_loss: 3.3587\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4292 - val_loss: 3.4505\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.4346 - val_loss: 3.3507\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4219 - val_loss: 3.4438\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4283 - val_loss: 3.3430\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.4148 - val_loss: 3.4373\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.4220 - val_loss: 3.3353\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4079 - val_loss: 3.4309\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4158 - val_loss: 3.3277\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.4010 - val_loss: 3.4246\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4096 - val_loss: 3.3202\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.3940 - val_loss: 3.4184\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4034 - val_loss: 3.3127\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3870 - val_loss: 3.4123\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3972 - val_loss: 3.3051\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.3798 - val_loss: 3.4061\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3910 - val_loss: 3.2973\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3724 - val_loss: 3.3998\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.3846 - val_loss: 3.2892\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.3646 - val_loss: 3.3932\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3778 - val_loss: 3.2807\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3565 - val_loss: 3.3865\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3708 - val_loss: 3.2727\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.3489 - val_loss: 3.3799\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3636 - val_loss: 3.2654\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3419 - val_loss: 3.3738\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3569 - val_loss: 3.2590\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3356 - val_loss: 3.3682\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3508 - val_loss: 3.2533\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.3299 - val_loss: 3.3631\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3452 - val_loss: 3.2483\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.3247 - val_loss: 3.3583\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.3400 - val_loss: 3.2438\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3200 - val_loss: 3.3538\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3353 - val_loss: 3.2396\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3157 - val_loss: 3.3495\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.3308 - val_loss: 3.2358\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3116 - val_loss: 3.3453\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3264 - val_loss: 3.2320\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3076 - val_loss: 3.3412\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3221 - val_loss: 3.2284\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3036 - val_loss: 3.3370\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.3179 - val_loss: 3.2247\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2997 - val_loss: 3.3327\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3135 - val_loss: 3.2210\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2956 - val_loss: 3.3283\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3090 - val_loss: 3.2172\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2915 - val_loss: 3.3237\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3044 - val_loss: 3.2132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2872 - val_loss: 3.3190\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2997 - val_loss: 3.2091\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2828 - val_loss: 3.3142\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2947 - val_loss: 3.2049\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2782 - val_loss: 3.3092\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2896 - val_loss: 3.2005\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2734 - val_loss: 3.3041\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2844 - val_loss: 3.1960\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2686 - val_loss: 3.2990\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2792 - val_loss: 3.1915\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2637 - val_loss: 3.2938\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2739 - val_loss: 3.1869\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2588 - val_loss: 3.2887\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2686 - val_loss: 3.1824\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2539 - val_loss: 3.2835\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2633 - val_loss: 3.1778\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2490 - val_loss: 3.2784\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2581 - val_loss: 3.1733\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2441 - val_loss: 3.2732\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2528 - val_loss: 3.1689\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2392 - val_loss: 3.2681\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.2476 - val_loss: 3.1645\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2343 - val_loss: 3.2629\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 30us/step - loss: 3.2423 - val_loss: 3.1600\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2293 - val_loss: 3.2574\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2368 - val_loss: 3.1552\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 3.2239 - val_loss: 3.2471\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2278 - val_loss: 3.1472\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.2148 - val_loss: 3.2334\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2165 - val_loss: 3.1380\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.2041 - val_loss: 3.2233\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2073 - val_loss: 3.1312\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1967 - val_loss: 3.2168\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.2010 - val_loss: 3.1261\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1910 - val_loss: 3.2118\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1960 - val_loss: 3.1218\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1860 - val_loss: 3.2076\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1916 - val_loss: 3.1180\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1819 - val_loss: 3.2039\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1879 - val_loss: 3.1149\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1784 - val_loss: 3.2008\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1846 - val_loss: 3.1122\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1753 - val_loss: 3.1980\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.1817 - val_loss: 3.1098\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1726 - val_loss: 3.1954\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1791 - val_loss: 3.1077\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1702 - val_loss: 3.1930\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 22us/step - loss: 3.1767 - val_loss: 3.1059\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1679 - val_loss: 3.1907\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.1745 - val_loss: 3.1044\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1659 - val_loss: 3.1886\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1725 - val_loss: 3.1032\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1642 - val_loss: 3.1867\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1708 - val_loss: 3.1023\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1627 - val_loss: 3.1851\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.1694 - val_loss: 3.1019\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1616 - val_loss: 3.1838\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1684 - val_loss: 3.1019\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 32us/step - loss: 3.1609 - val_loss: 3.1828\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1678 - val_loss: 3.1023\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1606 - val_loss: 3.1823\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1676 - val_loss: 3.1033\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1607 - val_loss: 3.1821\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1678 - val_loss: 3.1048\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1613 - val_loss: 3.1823\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1685 - val_loss: 3.1068\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1622 - val_loss: 3.1827\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.1694 - val_loss: 3.1092\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1635 - val_loss: 3.1833\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1707 - val_loss: 3.1121\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1650 - val_loss: 3.1840\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1722 - val_loss: 3.1152\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1667 - val_loss: 3.1846\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1737 - val_loss: 3.1186\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1685 - val_loss: 3.1847\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1751 - val_loss: 3.1222\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.1701 - val_loss: 3.1843\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1762 - val_loss: 3.1256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.1716 - val_loss: 3.1832\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1769 - val_loss: 3.1288\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1729 - val_loss: 3.1819\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1774 - val_loss: 3.1321\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1743 - val_loss: 3.1807\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1782 - val_loss: 3.1360\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1762 - val_loss: 3.1801\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1797 - val_loss: 3.1411\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1794 - val_loss: 3.1807\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1825 - val_loss: 3.1486\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.1847 - val_loss: 3.1840\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.1887 - val_loss: 3.1598\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1930 - val_loss: 3.1947\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2040 - val_loss: 3.1789\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2076 - val_loss: 3.2073\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2227 - val_loss: 3.2011\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2245 - val_loss: 3.2185\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.2422 - val_loss: 3.2241\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2417 - val_loss: 3.2306\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2637 - val_loss: 3.2493\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2615 - val_loss: 3.2398\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.2871 - val_loss: 3.2671\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.2738 - val_loss: 3.2321\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2950 - val_loss: 3.2869\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2879 - val_loss: 3.2240\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3035 - val_loss: 3.2932\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2883 - val_loss: 3.1919\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2852 - val_loss: 3.2860\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2737 - val_loss: 3.1575\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2618 - val_loss: 3.2663\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2459 - val_loss: 3.1134\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2243 - val_loss: 3.2342\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2076 - val_loss: 3.0594\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1743 - val_loss: 3.1899\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1608 - val_loss: 3.0107\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1280 - val_loss: 3.1607\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1295 - val_loss: 2.9770\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0966 - val_loss: 3.1293\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 3.0976 - val_loss: 2.9471\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0658 - val_loss: 3.0984\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0666 - val_loss: 2.9190\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0355 - val_loss: 3.0759\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0434 - val_loss: 2.9000\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0153 - val_loss: 3.0592\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0262 - val_loss: 2.8880\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0045 - val_loss: 3.0494\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0151 - val_loss: 2.8826\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9971 - val_loss: 3.0454\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0092 - val_loss: 2.8803\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9917 - val_loss: 3.0434\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 21us/step - loss: 3.0059 - val_loss: 2.8843\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9932 - val_loss: 3.0502\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.0102 - val_loss: 2.8888\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9942 - val_loss: 3.0511\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 27us/step - loss: 3.0104 - val_loss: 2.8916\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.9926 - val_loss: 3.0506\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0099 - val_loss: 2.8972\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9942 - val_loss: 3.0539\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0133 - val_loss: 2.9063\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9991 - val_loss: 3.0586\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0184 - val_loss: 2.9153\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0043 - val_loss: 3.0630\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0233 - val_loss: 2.9262\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 3.0114 - val_loss: 3.0685\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0297 - val_loss: 2.9385\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0206 - val_loss: 3.0742\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 3.0372 - val_loss: 2.9484\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 3.0273 - val_loss: 3.0774\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0434 - val_loss: 2.9582\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0335 - val_loss: 3.0799\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0486 - val_loss: 2.9659\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0365 - val_loss: 3.0804\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.0519 - val_loss: 2.9745\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0410 - val_loss: 3.0816\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0558 - val_loss: 2.9838\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0471 - val_loss: 3.0836\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0603 - val_loss: 2.9934\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0541 - val_loss: 3.0899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0682 - val_loss: 3.0043\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0624 - val_loss: 3.0966\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0760 - val_loss: 3.0141\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0694 - val_loss: 3.1015\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0821 - val_loss: 3.0218\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0745 - val_loss: 3.1047\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0864 - val_loss: 3.0277\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0779 - val_loss: 3.1067\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0895 - val_loss: 3.0322\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0801 - val_loss: 3.1080\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0915 - val_loss: 3.0355\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0813 - val_loss: 3.1085\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0927 - val_loss: 3.0378\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0817 - val_loss: 3.1084\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0932 - val_loss: 3.0393\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0814 - val_loss: 3.1079\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0931 - val_loss: 3.0402\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0807 - val_loss: 3.1071\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0926 - val_loss: 3.0405\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0797 - val_loss: 3.1060\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0918 - val_loss: 3.0405\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0784 - val_loss: 3.1048\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0908 - val_loss: 3.0403\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0770 - val_loss: 3.1035\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0896 - val_loss: 3.0399\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0754 - val_loss: 3.1022\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0885 - val_loss: 3.0394\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0738 - val_loss: 3.1008\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0873 - val_loss: 3.0388\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0723 - val_loss: 3.0995\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0861 - val_loss: 3.0383\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0707 - val_loss: 3.0982\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0850 - val_loss: 3.0379\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0692 - val_loss: 3.0967\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0838 - val_loss: 3.0376\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0677 - val_loss: 3.0945\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0820 - val_loss: 3.0372\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 25us/step - loss: 3.0661 - val_loss: 3.0920\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0801 - val_loss: 3.0369\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0646 - val_loss: 3.0900\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0788 - val_loss: 3.0367\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0633 - val_loss: 3.0884\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0778 - val_loss: 3.0366\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0622 - val_loss: 3.0870\n",
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0769 - val_loss: 3.0367\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0612 - val_loss: 3.0857\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0762 - val_loss: 3.0369\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0604 - val_loss: 3.0845\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0755 - val_loss: 3.0374\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.0599 - val_loss: 3.0836\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0752 - val_loss: 3.0383\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0598 - val_loss: 3.0829\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0751 - val_loss: 3.0395\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0600 - val_loss: 3.0824\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0752 - val_loss: 3.0408\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0602 - val_loss: 3.0815\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0749 - val_loss: 3.0420\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0602 - val_loss: 3.0831\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0774 - val_loss: 3.0458\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0625 - val_loss: 3.0861\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0814 - val_loss: 3.0503\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0655 - val_loss: 3.0891\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0858 - val_loss: 3.0554\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0690 - val_loss: 3.0911\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0902 - val_loss: 3.0614\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0731 - val_loss: 3.0932\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0952 - val_loss: 3.0683\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 3.0770 - val_loss: 3.0972\n",
      "Epoch 477/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1024 - val_loss: 3.0763\n",
      "Epoch 478/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0816 - val_loss: 3.0970\n",
      "Epoch 479/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1071 - val_loss: 3.0892\n",
      "Epoch 480/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0910 - val_loss: 3.0964\n",
      "Epoch 481/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1133 - val_loss: 3.0986\n",
      "Epoch 482/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0961 - val_loss: 3.0877\n",
      "Epoch 483/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1123 - val_loss: 3.1021\n",
      "Epoch 484/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0957 - val_loss: 3.0728\n",
      "Epoch 485/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1059 - val_loss: 3.1037\n",
      "Epoch 486/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0936 - val_loss: 3.0596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1026 - val_loss: 3.1077\n",
      "Epoch 488/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0933 - val_loss: 3.0528\n",
      "Epoch 489/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1091 - val_loss: 3.1324\n",
      "Epoch 490/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1123 - val_loss: 3.0577\n",
      "Epoch 491/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1327 - val_loss: 3.1708\n",
      "Epoch 492/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1413 - val_loss: 3.0504\n",
      "Epoch 493/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1446 - val_loss: 3.1759\n",
      "Epoch 494/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1365 - val_loss: 3.0153\n",
      "Epoch 495/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1190 - val_loss: 3.1503\n",
      "Epoch 496/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1060 - val_loss: 2.9812\n",
      "Epoch 497/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0880 - val_loss: 3.1207\n",
      "Epoch 498/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0750 - val_loss: 2.9543\n",
      "Epoch 499/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0615 - val_loss: 3.0962\n",
      "Epoch 500/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0504 - val_loss: 2.9320\n",
      "Epoch 501/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0388 - val_loss: 3.0776\n",
      "Epoch 502/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0320 - val_loss: 2.9129\n",
      "Epoch 503/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0194 - val_loss: 3.0589\n",
      "Epoch 504/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0142 - val_loss: 2.8966\n",
      "Epoch 505/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0030 - val_loss: 3.0398\n",
      "Epoch 506/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9959 - val_loss: 2.8816\n",
      "Epoch 507/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9879 - val_loss: 3.0262\n",
      "Epoch 508/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9822 - val_loss: 2.8705\n",
      "Epoch 509/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9767 - val_loss: 3.0185\n",
      "Epoch 510/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.9733 - val_loss: 2.8622\n",
      "Epoch 511/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9683 - val_loss: 3.0126\n",
      "Epoch 512/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9659 - val_loss: 2.8555\n",
      "Epoch 513/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9620 - val_loss: 3.0093\n",
      "Epoch 514/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9610 - val_loss: 2.8513\n",
      "Epoch 515/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9583 - val_loss: 3.0103\n",
      "Epoch 516/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9595 - val_loss: 2.8502\n",
      "Epoch 517/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9578 - val_loss: 3.0118\n",
      "Epoch 518/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9586 - val_loss: 2.8477\n",
      "Epoch 519/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9543 - val_loss: 3.0114\n",
      "Epoch 520/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9561 - val_loss: 2.8456\n",
      "Epoch 521/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9507 - val_loss: 3.0108\n",
      "Epoch 522/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9537 - val_loss: 2.8442\n",
      "Epoch 523/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9478 - val_loss: 3.0103\n",
      "Epoch 524/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9516 - val_loss: 2.8432\n",
      "Epoch 525/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9452 - val_loss: 3.0078\n",
      "Epoch 526/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9484 - val_loss: 2.8416\n",
      "Epoch 527/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9421 - val_loss: 3.0056\n",
      "Epoch 528/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9457 - val_loss: 2.8403\n",
      "Epoch 529/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9395 - val_loss: 3.0042\n",
      "Epoch 530/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9436 - val_loss: 2.8393\n",
      "Epoch 531/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9373 - val_loss: 3.0031\n",
      "Epoch 532/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9419 - val_loss: 2.8385\n",
      "Epoch 533/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9354 - val_loss: 3.0022\n",
      "Epoch 534/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9403 - val_loss: 2.8380\n",
      "Epoch 535/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9340 - val_loss: 3.0016\n",
      "Epoch 536/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9390 - val_loss: 2.8379\n",
      "Epoch 537/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9330 - val_loss: 3.0012\n",
      "Epoch 538/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9379 - val_loss: 2.8380\n",
      "Epoch 539/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9321 - val_loss: 3.0009\n",
      "Epoch 540/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9368 - val_loss: 2.8379\n",
      "Epoch 541/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9313 - val_loss: 3.0004\n",
      "Epoch 542/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9356 - val_loss: 2.8377\n",
      "Epoch 543/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9303 - val_loss: 2.9999\n",
      "Epoch 544/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9344 - val_loss: 2.8373\n",
      "Epoch 545/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9292 - val_loss: 2.9992\n",
      "Epoch 546/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9331 - val_loss: 2.8369\n",
      "Epoch 547/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9281 - val_loss: 2.9984\n",
      "Epoch 548/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9317 - val_loss: 2.8364\n",
      "Epoch 549/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9269 - val_loss: 2.9976\n",
      "Epoch 550/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9303 - val_loss: 2.8359\n",
      "Epoch 551/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9257 - val_loss: 2.9967\n",
      "Epoch 552/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9289 - val_loss: 2.8354\n",
      "Epoch 553/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9246 - val_loss: 2.9959\n",
      "Epoch 554/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9275 - val_loss: 2.8348\n",
      "Epoch 555/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9235 - val_loss: 2.9950\n",
      "Epoch 556/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9261 - val_loss: 2.8343\n",
      "Epoch 557/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9223 - val_loss: 2.9941\n",
      "Epoch 558/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9247 - val_loss: 2.8337\n",
      "Epoch 559/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9212 - val_loss: 2.9932\n",
      "Epoch 560/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9232 - val_loss: 2.8332\n",
      "Epoch 561/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9202 - val_loss: 2.9922\n",
      "Epoch 562/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9218 - val_loss: 2.8326\n",
      "Epoch 563/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9191 - val_loss: 2.9913\n",
      "Epoch 564/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9204 - val_loss: 2.8321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 565/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9180 - val_loss: 2.9903\n",
      "Epoch 566/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9190 - val_loss: 2.8315\n",
      "Epoch 567/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9170 - val_loss: 2.9893\n",
      "Epoch 568/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9176 - val_loss: 2.8310\n",
      "Epoch 569/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9159 - val_loss: 2.9883\n",
      "Epoch 570/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9161 - val_loss: 2.8306\n",
      "Epoch 571/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9149 - val_loss: 2.9873\n",
      "Epoch 572/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9147 - val_loss: 2.8301\n",
      "Epoch 573/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9138 - val_loss: 2.9862\n",
      "Epoch 574/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9132 - val_loss: 2.8297\n",
      "Epoch 575/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9127 - val_loss: 2.9851\n",
      "Epoch 576/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9117 - val_loss: 2.8292\n",
      "Epoch 577/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9116 - val_loss: 2.9839\n",
      "Epoch 578/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9102 - val_loss: 2.8289\n",
      "Epoch 579/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9105 - val_loss: 2.9827\n",
      "Epoch 580/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9086 - val_loss: 2.8286\n",
      "Epoch 581/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9094 - val_loss: 2.9815\n",
      "Epoch 582/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9071 - val_loss: 2.8284\n",
      "Epoch 583/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9083 - val_loss: 2.9804\n",
      "Epoch 584/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9058 - val_loss: 2.8286\n",
      "Epoch 585/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9073 - val_loss: 2.9795\n",
      "Epoch 586/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9048 - val_loss: 2.8291\n",
      "Epoch 587/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9066 - val_loss: 2.9790\n",
      "Epoch 588/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9042 - val_loss: 2.8302\n",
      "Epoch 589/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 2.9062 - val_loss: 2.9785\n",
      "Epoch 590/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9038 - val_loss: 2.8318\n",
      "Epoch 591/600\n",
      "1036/1036 [==============================] - 0s 23us/step - loss: 2.9058 - val_loss: 2.9775\n",
      "Epoch 592/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9033 - val_loss: 2.8346\n",
      "Epoch 593/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9059 - val_loss: 2.9757\n",
      "Epoch 594/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.9029 - val_loss: 2.8358\n",
      "Epoch 595/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9024 - val_loss: 2.9736\n",
      "Epoch 596/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9032 - val_loss: 2.8430\n",
      "Epoch 597/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9036 - val_loss: 2.9775\n",
      "Epoch 598/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9101 - val_loss: 2.8580\n",
      "Epoch 599/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9112 - val_loss: 2.9866\n",
      "Epoch 600/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9240 - val_loss: 2.8809\n"
     ]
    }
   ],
   "source": [
    "historyVal_LM = []\n",
    "historyTr_LM = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_LM()\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, batch_size=1036).history\n",
    "    historyVal_LM.append(history['val_loss'])\n",
    "    historyTr_LM.append(history['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_LM_mean=np.mean(historyVal_LM, axis=0)\n",
    "historyTr_LM_mean=np.mean(historyTr_LM, axis=0)\n",
    "\n",
    "historyVal_LM_sd=np.std(historyVal_LM, axis=0)\n",
    "historyTr_LM_sd=np.std(historyTr_LM, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_LM_mean)),historyTr_LM_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_LM_mean)), historyTr_LM_mean - historyTr_LM_sd, \n",
    "                 historyTr_LM_mean + historyTr_LM_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_LM_mean)), historyVal_LM_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_LM_mean)), historyVal_LM_mean - historyVal_LM_sd, \n",
    "                 historyVal_LM_mean + historyVal_LM_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.title('Learning curves low momentum', fontsize = 18)\n",
    "plt.legend()\n",
    "plt.ylim(5,14)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 14, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low momentum (mom=0.01) result:\n",
      "MEE on the validation 3.1332415103912354 with standard deviation 0.17323165611497843\n",
      "MEE on the training 2.978635835647583 with standard deviation 0.06134002958185802\n"
     ]
    }
   ],
   "source": [
    "print(\"Low momentum (mom=0.01) result:\")\n",
    "print(\"MEE on the validation\",historyVal_LM_mean[-1],\"with standard deviation\",historyVal_LM_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_LM_mean[-1],\"with standard deviation\",historyTr_LM_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ES():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=mlpr.best_params_['lr'], \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 1ms/step - loss: 58.1761 - val_loss: 53.9714\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 55.4619 - val_loss: 49.8553\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 51.3310 - val_loss: 44.3965\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 45.8511 - val_loss: 37.3088\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 38.7407 - val_loss: 29.1154\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 30.5020 - val_loss: 21.7680\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 23.0403 - val_loss: 18.1008\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 18.9437 - val_loss: 16.4724\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 16.9596 - val_loss: 15.8213\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 16.0474 - val_loss: 15.5057\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.5921 - val_loss: 15.0967\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 15.1510 - val_loss: 14.3812\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.4772 - val_loss: 13.2872\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 13.4650 - val_loss: 12.0675\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 12.3007 - val_loss: 10.9192\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 11.1261 - val_loss: 10.0492\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.1970 - val_loss: 9.3314\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.4399 - val_loss: 8.9257\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.0230 - val_loss: 8.6809\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.7999 - val_loss: 8.5341\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.6885 - val_loss: 8.4286\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.6066 - val_loss: 8.3402\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.5328 - val_loss: 8.2367\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.4430 - val_loss: 8.1263\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3413 - val_loss: 8.0143\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2323 - val_loss: 7.9006\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.1175 - val_loss: 7.7825\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.9962 - val_loss: 7.6553\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.8649 - val_loss: 7.5195\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.7239 - val_loss: 7.3808\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.5786 - val_loss: 7.2445\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.4350 - val_loss: 7.1107\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.2938 - val_loss: 6.9777\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.1530 - val_loss: 6.8457\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.0118 - val_loss: 6.7145\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.8704 - val_loss: 6.5830\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.7284 - val_loss: 6.4510\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.5857 - val_loss: 6.3187\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.4419 - val_loss: 6.1849\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.2970 - val_loss: 6.0504\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.1515 - val_loss: 5.9165\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.0056 - val_loss: 5.7836\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.8600 - val_loss: 5.6523\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.7152 - val_loss: 5.5241\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.5719 - val_loss: 5.3996\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.4307 - val_loss: 5.2788\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.2924 - val_loss: 5.1616\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 5.1572 - val_loss: 5.0480\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.0256 - val_loss: 4.9382\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 4.8979 - val_loss: 4.8323\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.7744 - val_loss: 4.7304\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.6558 - val_loss: 4.6322\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.5429 - val_loss: 4.5390\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.4360 - val_loss: 4.4508\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.3359 - val_loss: 4.3670\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.2431 - val_loss: 4.2878\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.1574 - val_loss: 4.2138\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.0785 - val_loss: 4.1490\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.0108 - val_loss: 4.1041\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9605 - val_loss: 4.0794\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.9431 - val_loss: 4.1334\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9895 - val_loss: 4.1722\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0537 - val_loss: 4.2116\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.0697 - val_loss: 4.1548\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.0296 - val_loss: 4.0645\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.9167 - val_loss: 3.9632\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8223 - val_loss: 3.9058\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 3.7450 - val_loss: 3.8544\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6926 - val_loss: 3.8458\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.6669 - val_loss: 3.8175\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6453 - val_loss: 3.8242\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6279 - val_loss: 3.7819\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.5967 - val_loss: 3.7826\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.5694 - val_loss: 3.7315\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5278 - val_loss: 3.7336\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5049 - val_loss: 3.6923\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4720 - val_loss: 3.7051\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4609 - val_loss: 3.6666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4317 - val_loss: 3.6765\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4184 - val_loss: 3.6343\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3831 - val_loss: 3.6437\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3716 - val_loss: 3.6076\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3418 - val_loss: 3.6198\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3326 - val_loss: 3.5844\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3034 - val_loss: 3.5941\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2921 - val_loss: 3.5605\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2627 - val_loss: 3.5659\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2490 - val_loss: 3.5320\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2179 - val_loss: 3.5369\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2063 - val_loss: 3.5061\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1772 - val_loss: 3.5193\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1771 - val_loss: 3.4939\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1532 - val_loss: 3.5104\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1591 - val_loss: 3.4857\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1351 - val_loss: 3.4957\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1347 - val_loss: 3.4693\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1069 - val_loss: 3.4750\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1041 - val_loss: 3.4517\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0770 - val_loss: 3.4559\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0765 - val_loss: 3.4359\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0516 - val_loss: 3.4389\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0506 - val_loss: 3.4223\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0298 - val_loss: 3.4256\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0300 - val_loss: 3.4102\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0113 - val_loss: 3.4156\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0131 - val_loss: 3.4016\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9985 - val_loss: 3.4101\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0018 - val_loss: 3.3950\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9882 - val_loss: 3.4048\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9911 - val_loss: 3.3878\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9774 - val_loss: 3.3980\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9788 - val_loss: 3.3794\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9654 - val_loss: 3.3902\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9660 - val_loss: 3.3682\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9500 - val_loss: 3.3791\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9494 - val_loss: 3.3578\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9352 - val_loss: 3.3666\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9318 - val_loss: 3.3495\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9209 - val_loss: 3.3561\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9173 - val_loss: 3.3415\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9089 - val_loss: 3.3423\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8995 - val_loss: 3.3305\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8931 - val_loss: 3.3310\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8847 - val_loss: 3.3240\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8835 - val_loss: 3.3274\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8793 - val_loss: 3.3216\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8806 - val_loss: 3.3266\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8777 - val_loss: 3.3191\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8781 - val_loss: 3.3250\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8748 - val_loss: 3.3153\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8739 - val_loss: 3.3222\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8702 - val_loss: 3.3109\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8690 - val_loss: 3.3190\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8654 - val_loss: 3.3069\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8644 - val_loss: 3.3164\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8614 - val_loss: 3.3045\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8619 - val_loss: 3.3151\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8594 - val_loss: 3.3021\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8591 - val_loss: 3.3123\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8557 - val_loss: 3.2975\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8536 - val_loss: 3.3074\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8493 - val_loss: 3.2919\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8466 - val_loss: 3.3023\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8428 - val_loss: 3.2869\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8402 - val_loss: 3.2978\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8372 - val_loss: 3.2823\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8344 - val_loss: 3.2935\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.8318 - val_loss: 3.2776\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8286 - val_loss: 3.2892\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8265 - val_loss: 3.2732\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8229 - val_loss: 3.2850\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8212 - val_loss: 3.2689\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8173 - val_loss: 3.2809\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8162 - val_loss: 3.2648\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8121 - val_loss: 3.2770\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8114 - val_loss: 3.2607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.8070 - val_loss: 3.2731\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8066 - val_loss: 3.2568\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8020 - val_loss: 3.2691\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8019 - val_loss: 3.2528\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7971 - val_loss: 3.2649\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7970 - val_loss: 3.2489\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7923 - val_loss: 3.2606\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7920 - val_loss: 3.2449\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7874 - val_loss: 3.2559\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7867 - val_loss: 3.2403\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7818 - val_loss: 3.2501\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7800 - val_loss: 3.2327\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7732 - val_loss: 3.2415\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7693 - val_loss: 3.2247\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7639 - val_loss: 3.2348\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7616 - val_loss: 3.2206\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7595 - val_loss: 3.2326\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7594 - val_loss: 3.2192\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7582 - val_loss: 3.2311\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7584 - val_loss: 3.2175\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7566 - val_loss: 3.2288\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7561 - val_loss: 3.2145\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7533 - val_loss: 3.2250\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7517 - val_loss: 3.2100\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7484 - val_loss: 3.2202\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7462 - val_loss: 3.2054\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7434 - val_loss: 3.2166\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7421 - val_loss: 3.2010\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7387 - val_loss: 3.2128\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7376 - val_loss: 3.1966\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7339 - val_loss: 3.2086\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7330 - val_loss: 3.1933\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7300 - val_loss: 3.2052\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7293 - val_loss: 3.1904\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7264 - val_loss: 3.2014\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7255 - val_loss: 3.1873\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7226 - val_loss: 3.1974\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7213 - val_loss: 3.1840\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7187 - val_loss: 3.1936\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7174 - val_loss: 3.1808\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7149 - val_loss: 3.1901\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7136 - val_loss: 3.1776\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7112 - val_loss: 3.1868\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7100 - val_loss: 3.1745\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7075 - val_loss: 3.1836\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7066 - val_loss: 3.1714\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7039 - val_loss: 3.1806\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7032 - val_loss: 3.1684\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7004 - val_loss: 3.1775\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6996 - val_loss: 3.1655\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6968 - val_loss: 3.1741\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6958 - val_loss: 3.1626\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6930 - val_loss: 3.1704\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6917 - val_loss: 3.1598\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6891 - val_loss: 3.1665\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6874 - val_loss: 3.1572\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6851 - val_loss: 3.1622\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6827 - val_loss: 3.1544\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6807 - val_loss: 3.1563\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6756 - val_loss: 3.1491\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6732 - val_loss: 3.1483\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6654 - val_loss: 3.1414\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6627 - val_loss: 3.1403\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6556 - val_loss: 3.1349\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6533 - val_loss: 3.1335\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6476 - val_loss: 3.1308\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6460 - val_loss: 3.1271\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6407 - val_loss: 3.1272\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6381 - val_loss: 3.1185\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6320 - val_loss: 3.1246\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6293 - val_loss: 3.1106\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6259 - val_loss: 3.1274\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6271 - val_loss: 3.1037\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6241 - val_loss: 3.1302\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6255 - val_loss: 3.0998\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6211 - val_loss: 3.1303\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6225 - val_loss: 3.0954\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6178 - val_loss: 3.1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6194 - val_loss: 3.0914\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6146 - val_loss: 3.1296\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6164 - val_loss: 3.0883\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6113 - val_loss: 3.1277\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6132 - val_loss: 3.0848\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6073 - val_loss: 3.1233\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6078 - val_loss: 3.0806\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6023 - val_loss: 3.1205\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6038 - val_loss: 3.0776\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5995 - val_loss: 3.1197\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6018 - val_loss: 3.0755\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5978 - val_loss: 3.1194\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6004 - val_loss: 3.0738\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5966 - val_loss: 3.1194\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5995 - val_loss: 3.0726\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5958 - val_loss: 3.1196\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5990 - val_loss: 3.0719\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5951 - val_loss: 3.1193\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.5983 - val_loss: 3.0712\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5940 - val_loss: 3.1183\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5970 - val_loss: 3.0704\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5924 - val_loss: 3.1173\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5954 - val_loss: 3.0692\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5908 - val_loss: 3.1186\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5959 - val_loss: 3.0691\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5911 - val_loss: 3.1186\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5955 - val_loss: 3.0681\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5895 - val_loss: 3.1167\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5930 - val_loss: 3.0664\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5868 - val_loss: 3.1115\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5871 - val_loss: 3.0627\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5812 - val_loss: 3.1098\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5837 - val_loss: 3.0607\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5793 - val_loss: 3.1070\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5796 - val_loss: 3.0570\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5745 - val_loss: 3.1061\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5762 - val_loss: 3.0546\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5721 - val_loss: 3.1067\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5742 - val_loss: 3.0525\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5701 - val_loss: 3.1089\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5740 - val_loss: 3.0515\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5691 - val_loss: 3.1101\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5732 - val_loss: 3.0506\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5690 - val_loss: 3.1143\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5760 - val_loss: 3.0529\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5774 - val_loss: 3.1312\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5925 - val_loss: 3.0627\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5957 - val_loss: 3.1469\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6107 - val_loss: 3.0716\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6099 - val_loss: 3.1449\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6103 - val_loss: 3.0653\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5974 - val_loss: 3.1284\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5919 - val_loss: 3.0557\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5773 - val_loss: 3.1153\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5755 - val_loss: 3.0497\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5660 - val_loss: 3.1108\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5685 - val_loss: 3.0470\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5625 - val_loss: 3.1118\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5682 - val_loss: 3.0458\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5603 - val_loss: 3.1044\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5610 - val_loss: 3.0413\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5492 - val_loss: 3.0888\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5475 - val_loss: 3.0420\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5433 - val_loss: 3.0890\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5506 - val_loss: 3.0538\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5536 - val_loss: 3.0967\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5627 - val_loss: 3.0636\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5617 - val_loss: 3.0954\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5631 - val_loss: 3.0639\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5593 - val_loss: 3.0919\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5592 - val_loss: 3.0617\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5560 - val_loss: 3.0907\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5565 - val_loss: 3.0594\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5534 - val_loss: 3.0899\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5537 - val_loss: 3.0567\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5499 - val_loss: 3.0886\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5502 - val_loss: 3.0541\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.5462 - val_loss: 3.0879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5471 - val_loss: 3.0501\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5432 - val_loss: 3.0900\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5451 - val_loss: 3.0464\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5396 - val_loss: 3.0896\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5411 - val_loss: 3.0421\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5355 - val_loss: 3.0888\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5375 - val_loss: 3.0389\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.5329 - val_loss: 3.0880\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.5342 - val_loss: 3.0359\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5300 - val_loss: 3.0884\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5322 - val_loss: 3.0342\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5288 - val_loss: 3.0897\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5318 - val_loss: 3.0332\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5279 - val_loss: 3.0902\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5310 - val_loss: 3.0322\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5266 - val_loss: 3.0900\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5297 - val_loss: 3.0314\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5258 - val_loss: 3.0910\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5298 - val_loss: 3.0321\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5286 - val_loss: 3.0977\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5361 - val_loss: 3.0356\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5373 - val_loss: 3.1102\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5512 - val_loss: 3.0429\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5478 - val_loss: 3.1101\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5531 - val_loss: 3.0416\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5443 - val_loss: 3.1008\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5435 - val_loss: 3.0343\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5302 - val_loss: 3.0877\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5279 - val_loss: 3.0282\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5192 - val_loss: 3.0825\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5200 - val_loss: 3.0269\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5164 - val_loss: 3.0845\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5206 - val_loss: 3.0283\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5184 - val_loss: 3.0885\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5238 - val_loss: 3.0293\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5201 - val_loss: 3.0882\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5222 - val_loss: 3.0280\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5174 - val_loss: 3.0841\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5163 - val_loss: 3.0255\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5101 - val_loss: 3.0744\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5067 - val_loss: 3.0245\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5007 - val_loss: 3.0680\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5036 - val_loss: 3.0342\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5056 - val_loss: 3.0765\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5181 - val_loss: 3.0527\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5249 - val_loss: 3.0856\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5341 - val_loss: 3.0650\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5342 - val_loss: 3.0822\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5367 - val_loss: 3.0739\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5385 - val_loss: 3.0851\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5475 - val_loss: 3.0927\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5555 - val_loss: 3.0979\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5724 - val_loss: 3.1174\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5797 - val_loss: 3.1114\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5945 - val_loss: 3.1292\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5906 - val_loss: 3.1140\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6013 - val_loss: 3.1279\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5879 - val_loss: 3.1054\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5871 - val_loss: 3.1135\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5708 - val_loss: 3.0907\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5625 - val_loss: 3.0887\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5451 - val_loss: 3.0780\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5404 - val_loss: 3.0757\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5326 - val_loss: 3.0736\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5316 - val_loss: 3.0719\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5277 - val_loss: 3.0709\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5267 - val_loss: 3.0684\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5234 - val_loss: 3.0678\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5220 - val_loss: 3.0653\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5195 - val_loss: 3.0650\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5177 - val_loss: 3.0626\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5158 - val_loss: 3.0629\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5139 - val_loss: 3.0603\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5127 - val_loss: 3.0617\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5108 - val_loss: 3.0583\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5098 - val_loss: 3.0608\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5081 - val_loss: 3.0568\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5073 - val_loss: 3.0601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5058 - val_loss: 3.0559\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5052 - val_loss: 3.0594\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5040 - val_loss: 3.0556\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5036 - val_loss: 3.0591\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5026 - val_loss: 3.0559\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5026 - val_loss: 3.0590\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5019 - val_loss: 3.0567\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5022 - val_loss: 3.0593\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5017 - val_loss: 3.0579\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5021 - val_loss: 3.0599\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5020 - val_loss: 3.0592\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5022 - val_loss: 3.0602\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5022 - val_loss: 3.0605\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 56.0894 - val_loss: 53.0666\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 53.4047 - val_loss: 48.8370\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 49.1700 - val_loss: 43.1580\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 43.4824 - val_loss: 35.9574\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 36.2685 - val_loss: 27.8816\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 28.1347 - val_loss: 21.3909\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 21.4769 - val_loss: 18.2821\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 18.1373 - val_loss: 16.7551\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 16.4902 - val_loss: 16.0761\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.7697 - val_loss: 15.5667\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.2504 - val_loss: 14.8187\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 14.4995 - val_loss: 13.7209\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 13.4218 - val_loss: 12.3603\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 12.1512 - val_loss: 11.0348\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 10.9835 - val_loss: 9.7666\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.8464 - val_loss: 8.9418\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.0906 - val_loss: 8.6754\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.8392 - val_loss: 8.5615\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.7099 - val_loss: 8.4711\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6059 - val_loss: 8.4148\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.5300 - val_loss: 8.3484\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.4431 - val_loss: 8.2662\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 8.3482 - val_loss: 8.1788\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.2562 - val_loss: 8.0938\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 8.1634 - val_loss: 8.0027\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.0620 - val_loss: 7.8939\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.9452 - val_loss: 7.7648\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 7.8105 - val_loss: 7.6222\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.6587 - val_loss: 7.4711\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 7.4924 - val_loss: 7.3187\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.3207 - val_loss: 7.1745\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.1534 - val_loss: 7.0349\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.9906 - val_loss: 6.8913\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.8279 - val_loss: 6.7431\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.6626 - val_loss: 6.5878\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.4945 - val_loss: 6.4282\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.3245 - val_loss: 6.2686\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.1530 - val_loss: 6.1077\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.9804 - val_loss: 5.9457\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.8073 - val_loss: 5.7847\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.6350 - val_loss: 5.6259\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 5.4654 - val_loss: 5.4750\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 5.3014 - val_loss: 5.3307\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.1448 - val_loss: 5.1959\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.9965 - val_loss: 5.0668\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.8578 - val_loss: 4.9506\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.7303 - val_loss: 4.8422\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.6147 - val_loss: 4.7497\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.5105 - val_loss: 4.6592\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.4182 - val_loss: 4.5818\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.3357 - val_loss: 4.5007\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2607 - val_loss: 4.4336\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.1914 - val_loss: 4.3602\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.1265 - val_loss: 4.3016\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.0650 - val_loss: 4.2340\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0070 - val_loss: 4.1901\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9539 - val_loss: 4.1224\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.9063 - val_loss: 4.1124\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8692 - val_loss: 4.0511\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8603 - val_loss: 4.1791\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.9067 - val_loss: 4.2314\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.0744 - val_loss: 4.6928\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3770 - val_loss: 4.6231\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.5288 - val_loss: 4.6498\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.3443 - val_loss: 4.2254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 4.0906 - val_loss: 4.1726\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9073 - val_loss: 3.9789\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.8212 - val_loss: 4.0483\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7924 - val_loss: 3.9410\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.7804 - val_loss: 4.0548\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.7995 - val_loss: 3.9362\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7887 - val_loss: 4.0411\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7915 - val_loss: 3.8947\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7549 - val_loss: 3.9638\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7212 - val_loss: 3.8016\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6595 - val_loss: 3.8558\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.6187 - val_loss: 3.7014\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5518 - val_loss: 3.7609\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5285 - val_loss: 3.6347\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4812 - val_loss: 3.7155\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4857 - val_loss: 3.6075\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4596 - val_loss: 3.7123\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4866 - val_loss: 3.6056\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4689 - val_loss: 3.7126\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4939 - val_loss: 3.5932\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4647 - val_loss: 3.6808\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4665 - val_loss: 3.5441\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4152 - val_loss: 3.6143\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.4039 - val_loss: 3.4788\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3432 - val_loss: 3.5450\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3397 - val_loss: 3.4328\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2943 - val_loss: 3.5029\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3031 - val_loss: 3.4063\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2695 - val_loss: 3.4804\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2857 - val_loss: 3.3889\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2558 - val_loss: 3.4629\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2721 - val_loss: 3.3713\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2408 - val_loss: 3.4411\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2541 - val_loss: 3.3511\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2218 - val_loss: 3.4165\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2338 - val_loss: 3.3316\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2030 - val_loss: 3.3936\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2142 - val_loss: 3.3124\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1851 - val_loss: 3.3713\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1950 - val_loss: 3.2951\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1685 - val_loss: 3.3504\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1755 - val_loss: 3.2781\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1515 - val_loss: 3.3332\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1599 - val_loss: 3.2669\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1409 - val_loss: 3.3208\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1493 - val_loss: 3.2612\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1362 - val_loss: 3.3126\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1422 - val_loss: 3.2528\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1276 - val_loss: 3.2984\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1296 - val_loss: 3.2394\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1129 - val_loss: 3.2816\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1139 - val_loss: 3.2261\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0982 - val_loss: 3.2670\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1001 - val_loss: 3.2156\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0866 - val_loss: 3.2559\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0897 - val_loss: 3.2052\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0740 - val_loss: 3.2434\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0775 - val_loss: 3.1951\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0625 - val_loss: 3.2319\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0663 - val_loss: 3.1845\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0513 - val_loss: 3.2193\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0547 - val_loss: 3.1740\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0401 - val_loss: 3.2059\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0425 - val_loss: 3.1637\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0289 - val_loss: 3.1893\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0266 - val_loss: 3.1498\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0133 - val_loss: 3.1748\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0120 - val_loss: 3.1386\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0019 - val_loss: 3.1651\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0029 - val_loss: 3.1311\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9941 - val_loss: 3.1586\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9980 - val_loss: 3.1272\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9895 - val_loss: 3.1547\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9956 - val_loss: 3.1237\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9855 - val_loss: 3.1486\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9909 - val_loss: 3.1174\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9784 - val_loss: 3.1394\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9829 - val_loss: 3.1094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9700 - val_loss: 3.1296\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9738 - val_loss: 3.1007\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9609 - val_loss: 3.1179\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9629 - val_loss: 3.0907\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9494 - val_loss: 3.1052\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9504 - val_loss: 3.0757\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9327 - val_loss: 3.0887\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9344 - val_loss: 3.0648\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9204 - val_loss: 3.0764\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9222 - val_loss: 3.0571\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9108 - val_loss: 3.0697\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9153 - val_loss: 3.0551\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9082 - val_loss: 3.0682\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9134 - val_loss: 3.0550\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.9077 - val_loss: 3.0661\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9112 - val_loss: 3.0535\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9054 - val_loss: 3.0622\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.9069 - val_loss: 3.0503\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9011 - val_loss: 3.0569\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9009 - val_loss: 3.0461\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8958 - val_loss: 3.0513\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8945 - val_loss: 3.0418\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8904 - val_loss: 3.0463\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8886 - val_loss: 3.0383\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8857 - val_loss: 3.0424\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8838 - val_loss: 3.0358\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8820 - val_loss: 3.0394\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8799 - val_loss: 3.0339\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8787 - val_loss: 3.0365\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8763 - val_loss: 3.0322\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8753 - val_loss: 3.0334\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8725 - val_loss: 3.0335\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8756 - val_loss: 3.0340\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8730 - val_loss: 3.0334\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8741 - val_loss: 3.0313\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8702 - val_loss: 3.0311\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8694 - val_loss: 3.0280\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8663 - val_loss: 3.0298\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8661 - val_loss: 3.0275\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8650 - val_loss: 3.0301\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8649 - val_loss: 3.0268\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8639 - val_loss: 3.0297\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8627 - val_loss: 3.0243\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8612 - val_loss: 3.0280\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8589 - val_loss: 3.0206\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8571 - val_loss: 3.0250\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8542 - val_loss: 3.0158\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8516 - val_loss: 3.0206\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8484 - val_loss: 3.0096\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8446 - val_loss: 3.0147\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8415 - val_loss: 3.0018\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8359 - val_loss: 3.0083\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8338 - val_loss: 2.9952\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8284 - val_loss: 3.0045\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8283 - val_loss: 2.9874\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8199 - val_loss: 2.9984\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8214 - val_loss: 2.9841\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.8154 - val_loss: 2.9972\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.8192 - val_loss: 2.9861\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8166 - val_loss: 3.0003\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8221 - val_loss: 2.9912\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8218 - val_loss: 3.0061\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8272 - val_loss: 2.9956\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8264 - val_loss: 3.0080\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8272 - val_loss: 2.9931\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8241 - val_loss: 3.0050\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8219 - val_loss: 2.9829\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8136 - val_loss: 2.9949\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8107 - val_loss: 2.9744\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8034 - val_loss: 2.9870\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8027 - val_loss: 2.9687\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7963 - val_loss: 2.9821\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7977 - val_loss: 2.9665\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7934 - val_loss: 2.9813\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7959 - val_loss: 2.9664\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7929 - val_loss: 2.9820\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7956 - val_loss: 2.9668\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7931 - val_loss: 2.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7948 - val_loss: 2.9661\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7921 - val_loss: 2.9810\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7926 - val_loss: 2.9640\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7895 - val_loss: 2.9786\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7892 - val_loss: 2.9612\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7861 - val_loss: 2.9757\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7855 - val_loss: 2.9585\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7826 - val_loss: 2.9728\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7819 - val_loss: 2.9561\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7794 - val_loss: 2.9701\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7785 - val_loss: 2.9540\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7764 - val_loss: 2.9678\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7753 - val_loss: 2.9520\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7736 - val_loss: 2.9655\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.7723 - val_loss: 2.9502\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7710 - val_loss: 2.9632\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7693 - val_loss: 2.9483\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7682 - val_loss: 2.9608\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7664 - val_loss: 2.9466\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7657 - val_loss: 2.9589\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7646 - val_loss: 2.9455\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7641 - val_loss: 2.9571\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7624 - val_loss: 2.9436\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7616 - val_loss: 2.9541\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7569 - val_loss: 2.9396\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7564 - val_loss: 2.9545\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7574 - val_loss: 2.9428\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7590 - val_loss: 2.9547\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7581 - val_loss: 2.9415\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7574 - val_loss: 2.9520\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7539 - val_loss: 2.9377\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7530 - val_loss: 2.9488\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7486 - val_loss: 2.9351\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7487 - val_loss: 2.9470\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7467 - val_loss: 2.9357\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7478 - val_loss: 2.9451\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7450 - val_loss: 2.9341\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7453 - val_loss: 2.9426\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7414 - val_loss: 2.9316\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7419 - val_loss: 2.9406\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7381 - val_loss: 2.9302\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7394 - val_loss: 2.9394\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7360 - val_loss: 2.9298\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7377 - val_loss: 2.9379\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7339 - val_loss: 2.9288\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7355 - val_loss: 2.9361\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7313 - val_loss: 2.9273\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7330 - val_loss: 2.9344\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7286 - val_loss: 2.9262\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7307 - val_loss: 2.9330\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7263 - val_loss: 2.9253\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7287 - val_loss: 2.9316\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7240 - val_loss: 2.9244\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7266 - val_loss: 2.9302\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7217 - val_loss: 2.9234\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7244 - val_loss: 2.9288\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.7194 - val_loss: 2.9224\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7223 - val_loss: 2.9274\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7171 - val_loss: 2.9216\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7202 - val_loss: 2.9261\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7148 - val_loss: 2.9207\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7181 - val_loss: 2.9248\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7126 - val_loss: 2.9198\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7160 - val_loss: 2.9235\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7103 - val_loss: 2.9190\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7140 - val_loss: 2.9223\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7081 - val_loss: 2.9181\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7119 - val_loss: 2.9210\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7059 - val_loss: 2.9173\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7099 - val_loss: 2.9198\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7036 - val_loss: 2.9165\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7078 - val_loss: 2.9186\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7014 - val_loss: 2.9157\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7058 - val_loss: 2.9175\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6992 - val_loss: 2.9150\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7037 - val_loss: 2.9163\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6970 - val_loss: 2.9142\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7017 - val_loss: 2.9152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6949 - val_loss: 2.9135\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6996 - val_loss: 2.9141\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6927 - val_loss: 2.9127\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6976 - val_loss: 2.9130\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6905 - val_loss: 2.9120\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6955 - val_loss: 2.9119\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6883 - val_loss: 2.9113\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6935 - val_loss: 2.9109\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6861 - val_loss: 2.9106\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6914 - val_loss: 2.9098\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6840 - val_loss: 2.9098\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6893 - val_loss: 2.9088\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6818 - val_loss: 2.9091\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6871 - val_loss: 2.9078\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6796 - val_loss: 2.9084\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6850 - val_loss: 2.9068\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6775 - val_loss: 2.9077\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6828 - val_loss: 2.9059\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6753 - val_loss: 2.9069\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6807 - val_loss: 2.9049\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6731 - val_loss: 2.9062\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6785 - val_loss: 2.9040\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6710 - val_loss: 2.9055\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6762 - val_loss: 2.9031\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6688 - val_loss: 2.9048\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6740 - val_loss: 2.9022\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6667 - val_loss: 2.9040\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6717 - val_loss: 2.9012\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6645 - val_loss: 2.9033\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6693 - val_loss: 2.9002\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6623 - val_loss: 2.9027\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6670 - val_loss: 2.8986\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6599 - val_loss: 2.9024\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6648 - val_loss: 2.8961\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6571 - val_loss: 2.9017\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6621 - val_loss: 2.8930\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6536 - val_loss: 2.8997\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6581 - val_loss: 2.8902\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6497 - val_loss: 2.8974\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6539 - val_loss: 2.8884\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6464 - val_loss: 2.8960\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6510 - val_loss: 2.8881\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6446 - val_loss: 2.8960\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6498 - val_loss: 2.8889\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6440 - val_loss: 2.8961\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6493 - val_loss: 2.8900\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6432 - val_loss: 2.8949\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6477 - val_loss: 2.8892\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6393 - val_loss: 2.8914\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6434 - val_loss: 2.8895\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6368 - val_loss: 2.8926\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6432 - val_loss: 2.8919\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6386 - val_loss: 2.8958\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6451 - val_loss: 2.8938\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6405 - val_loss: 2.9022\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6505 - val_loss: 2.8960\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6443 - val_loss: 2.9028\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6499 - val_loss: 2.8928\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6400 - val_loss: 2.8954\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6412 - val_loss: 2.8876\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6314 - val_loss: 2.8878\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6317 - val_loss: 2.8834\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6235 - val_loss: 2.8854\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6260 - val_loss: 2.8826\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6216 - val_loss: 2.8872\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6248 - val_loss: 2.8818\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6212 - val_loss: 2.8888\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6238 - val_loss: 2.8796\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6196 - val_loss: 2.8888\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6210 - val_loss: 2.8756\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6148 - val_loss: 2.8847\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6137 - val_loss: 2.8697\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6067 - val_loss: 2.8785\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6039 - val_loss: 2.8641\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5989 - val_loss: 2.8726\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5935 - val_loss: 2.8561\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5897 - val_loss: 2.8699\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5842 - val_loss: 2.8480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5821 - val_loss: 2.8681\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5753 - val_loss: 2.8418\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5757 - val_loss: 2.8663\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5685 - val_loss: 2.8369\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5711 - val_loss: 2.8660\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5637 - val_loss: 2.8343\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5687 - val_loss: 2.8670\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5618 - val_loss: 2.8349\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5681 - val_loss: 2.8681\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5618 - val_loss: 2.8377\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5687 - val_loss: 2.8693\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5629 - val_loss: 2.8419\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5703 - val_loss: 2.8717\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5658 - val_loss: 2.8483\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5742 - val_loss: 2.8761\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5728 - val_loss: 2.8624\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5852 - val_loss: 2.8894\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5957 - val_loss: 2.8957\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6162 - val_loss: 2.9222\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6494 - val_loss: 2.9446\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6632 - val_loss: 2.9479\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6824 - val_loss: 2.9523\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6659 - val_loss: 2.9287\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6603 - val_loss: 2.9192\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6316 - val_loss: 2.9020\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6257 - val_loss: 2.9000\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6146 - val_loss: 2.8978\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6172 - val_loss: 2.9017\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6165 - val_loss: 2.9031\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6221 - val_loss: 2.9054\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6186 - val_loss: 2.9017\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6197 - val_loss: 2.9003\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6124 - val_loss: 2.8951\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6102 - val_loss: 2.8941\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6053 - val_loss: 2.8917\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6039 - val_loss: 2.8921\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6024 - val_loss: 2.8913\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6013 - val_loss: 2.8912\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6006 - val_loss: 2.8903\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.5984 - val_loss: 2.8891\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.5976 - val_loss: 2.8883\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5943 - val_loss: 2.8867\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5942 - val_loss: 2.8868\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5905 - val_loss: 2.8848\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5913 - val_loss: 2.8861\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5874 - val_loss: 2.8830\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5888 - val_loss: 2.8855\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5845 - val_loss: 2.8813\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5862 - val_loss: 2.8847\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5817 - val_loss: 2.8796\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5836 - val_loss: 2.8838\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5790 - val_loss: 2.8786\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5814 - val_loss: 2.8833\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5770 - val_loss: 2.8785\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5801 - val_loss: 2.8834\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5761 - val_loss: 2.8795\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 58.0615 - val_loss: 56.4634\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 55.2563 - val_loss: 52.4555\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 51.2484 - val_loss: 47.4700\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 46.2529 - val_loss: 41.2120\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 39.9658 - val_loss: 33.9022\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 32.5987 - val_loss: 26.7237\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 25.3359 - val_loss: 21.5219\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 20.3071 - val_loss: 18.6783\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 17.8393 - val_loss: 17.1590\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 16.5717 - val_loss: 16.3834\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.9685 - val_loss: 15.9283\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 15.6145 - val_loss: 15.5162\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 15.2415 - val_loss: 15.0314\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 14.7535 - val_loss: 14.4019\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 14.0958 - val_loss: 13.4251\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 13.0615 - val_loss: 12.1240\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 11.6777 - val_loss: 11.1247\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 10.6370 - val_loss: 10.2034\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 9.7711 - val_loss: 9.5119\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 9.1571 - val_loss: 9.2209\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.8656 - val_loss: 9.0255\n",
      "Epoch 22/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.6420 - val_loss: 8.8762\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 8.4779 - val_loss: 8.7098\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.3349 - val_loss: 8.5214\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 8.1805 - val_loss: 8.3514\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0317 - val_loss: 8.1979\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.8842 - val_loss: 8.0482\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.7424 - val_loss: 7.8893\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.5993 - val_loss: 7.7271\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.4543 - val_loss: 7.5648\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 7.3067 - val_loss: 7.4010\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 7.1582 - val_loss: 7.2362\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.0116 - val_loss: 7.0759\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 6.8702 - val_loss: 6.9223\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.7349 - val_loss: 6.7713\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.6049 - val_loss: 6.6256\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.4792 - val_loss: 6.4869\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.3563 - val_loss: 6.3480\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.2352 - val_loss: 6.2106\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.1151 - val_loss: 6.0757\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.9952 - val_loss: 5.9409\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.8752 - val_loss: 5.8062\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.7547 - val_loss: 5.6715\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.6336 - val_loss: 5.5357\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.5118 - val_loss: 5.3993\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.3892 - val_loss: 5.2629\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.2661 - val_loss: 5.1261\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.1426 - val_loss: 4.9909\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.0187 - val_loss: 4.8557\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.8947 - val_loss: 4.7202\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.7715 - val_loss: 4.5849\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.6516 - val_loss: 4.4572\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.5363 - val_loss: 4.3340\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 4.4280 - val_loss: 4.2286\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.3269 - val_loss: 4.1244\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.2336 - val_loss: 4.0320\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 4.1484 - val_loss: 3.9457\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0712 - val_loss: 3.8747\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 4.0020 - val_loss: 3.7931\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9409 - val_loss: 3.7565\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8855 - val_loss: 3.6723\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.8380 - val_loss: 3.6684\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7974 - val_loss: 3.5757\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7667 - val_loss: 3.6198\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7461 - val_loss: 3.5155\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7507 - val_loss: 3.6318\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7520 - val_loss: 3.4758\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.7359 - val_loss: 3.5600\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.6915 - val_loss: 3.3925\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6473 - val_loss: 3.4477\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5964 - val_loss: 3.3197\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.5645 - val_loss: 3.3689\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.5273 - val_loss: 3.2623\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5019 - val_loss: 3.3098\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.4726 - val_loss: 3.2187\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.4531 - val_loss: 3.2654\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.4302 - val_loss: 3.1835\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.4119 - val_loss: 3.2287\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.3927 - val_loss: 3.1535\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3743 - val_loss: 3.1988\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3599 - val_loss: 3.1279\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3416 - val_loss: 3.1720\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.3293 - val_loss: 3.1047\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3101 - val_loss: 3.1482\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.2991 - val_loss: 3.0838\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2804 - val_loss: 3.1290\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2736 - val_loss: 3.0681\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2571 - val_loss: 3.1183\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2564 - val_loss: 3.0572\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2421 - val_loss: 3.1109\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2417 - val_loss: 3.0443\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2234 - val_loss: 3.0934\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2185 - val_loss: 3.0257\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1954 - val_loss: 3.0713\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1915 - val_loss: 3.0096\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1695 - val_loss: 3.0528\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1685 - val_loss: 2.9988\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1510 - val_loss: 3.0449\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1538 - val_loss: 2.9922\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1387 - val_loss: 3.0387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 3.1400 - val_loss: 2.9847\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1251 - val_loss: 3.0290\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1231 - val_loss: 2.9762\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.1096 - val_loss: 3.0217\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.1083 - val_loss: 2.9699\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0976 - val_loss: 3.0161\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 3.0958 - val_loss: 2.9649\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 3.0868 - val_loss: 3.0118\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0842 - val_loss: 2.9599\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 3.0757 - val_loss: 3.0073\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0726 - val_loss: 2.9550\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0646 - val_loss: 3.0041\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0624 - val_loss: 2.9516\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.0556 - val_loss: 3.0012\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0524 - val_loss: 2.9480\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0462 - val_loss: 2.9987\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0427 - val_loss: 2.9449\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0371 - val_loss: 2.9958\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0329 - val_loss: 2.9411\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0270 - val_loss: 2.9918\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0224 - val_loss: 2.9363\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0159 - val_loss: 2.9857\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0113 - val_loss: 2.9328\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0065 - val_loss: 2.9834\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0029 - val_loss: 2.9289\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9966 - val_loss: 2.9754\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9910 - val_loss: 2.9199\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9804 - val_loss: 2.9617\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9739 - val_loss: 2.9129\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9665 - val_loss: 2.9593\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9653 - val_loss: 2.9137\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.9616 - val_loss: 2.9614\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9612 - val_loss: 2.9146\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9570 - val_loss: 2.9607\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9547 - val_loss: 2.9131\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9494 - val_loss: 2.9584\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9469 - val_loss: 2.9116\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9419 - val_loss: 2.9568\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9399 - val_loss: 2.9110\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9354 - val_loss: 2.9561\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9336 - val_loss: 2.9106\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9293 - val_loss: 2.9554\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9276 - val_loss: 2.9101\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9233 - val_loss: 2.9546\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9216 - val_loss: 2.9096\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9175 - val_loss: 2.9537\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9157 - val_loss: 2.9091\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9120 - val_loss: 2.9530\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9101 - val_loss: 2.9086\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9066 - val_loss: 2.9524\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9049 - val_loss: 2.9082\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9016 - val_loss: 2.9521\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9000 - val_loss: 2.9078\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8966 - val_loss: 2.9518\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8954 - val_loss: 2.9071\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8917 - val_loss: 2.9513\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8907 - val_loss: 2.9062\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8865 - val_loss: 2.9506\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8860 - val_loss: 2.9051\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8814 - val_loss: 2.9498\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8813 - val_loss: 2.9041\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8764 - val_loss: 2.9490\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8768 - val_loss: 2.9031\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8717 - val_loss: 2.9481\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8725 - val_loss: 2.9021\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8671 - val_loss: 2.9473\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8683 - val_loss: 2.9011\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8627 - val_loss: 2.9465\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8642 - val_loss: 2.9002\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8584 - val_loss: 2.9456\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8603 - val_loss: 2.8992\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8542 - val_loss: 2.9440\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8560 - val_loss: 2.8977\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8495 - val_loss: 2.9395\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8493 - val_loss: 2.8946\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8421 - val_loss: 2.9373\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8445 - val_loss: 2.8961\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8409 - val_loss: 2.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8447 - val_loss: 2.9012\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8445 - val_loss: 2.9471\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8502 - val_loss: 2.9084\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8512 - val_loss: 2.9522\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8542 - val_loss: 2.9072\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8477 - val_loss: 2.9452\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8454 - val_loss: 2.9011\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.8385 - val_loss: 2.9404\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8390 - val_loss: 2.8988\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8340 - val_loss: 2.9387\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8359 - val_loss: 2.8973\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8307 - val_loss: 2.9370\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8325 - val_loss: 2.8955\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8270 - val_loss: 2.9350\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8289 - val_loss: 2.8936\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8232 - val_loss: 2.9331\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8254 - val_loss: 2.8918\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8196 - val_loss: 2.9310\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8220 - val_loss: 2.8899\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8159 - val_loss: 2.9288\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8185 - val_loss: 2.8880\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8121 - val_loss: 2.9266\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8150 - val_loss: 2.8861\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.8085 - val_loss: 2.9245\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8116 - val_loss: 2.8843\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8050 - val_loss: 2.9225\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8083 - val_loss: 2.8825\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8016 - val_loss: 2.9206\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8051 - val_loss: 2.8808\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7983 - val_loss: 2.9189\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.8020 - val_loss: 2.8793\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7952 - val_loss: 2.9171\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7989 - val_loss: 2.8777\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7922 - val_loss: 2.9150\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7955 - val_loss: 2.8760\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7890 - val_loss: 2.9095\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7893 - val_loss: 2.8714\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7823 - val_loss: 2.9021\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7815 - val_loss: 2.8644\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7709 - val_loss: 2.8895\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7695 - val_loss: 2.8551\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7574 - val_loss: 2.8786\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7580 - val_loss: 2.8471\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7480 - val_loss: 2.8754\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7542 - val_loss: 2.8472\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7479 - val_loss: 2.8784\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7561 - val_loss: 2.8507\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7512 - val_loss: 2.8831\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7591 - val_loss: 2.8565\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7573 - val_loss: 2.8899\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7647 - val_loss: 2.8619\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7624 - val_loss: 2.8918\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7646 - val_loss: 2.8592\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7583 - val_loss: 2.8878\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7589 - val_loss: 2.8548\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7523 - val_loss: 2.8846\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7547 - val_loss: 2.8528\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7493 - val_loss: 2.8835\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7528 - val_loss: 2.8522\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7480 - val_loss: 2.8832\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7513 - val_loss: 2.8515\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7463 - val_loss: 2.8826\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7491 - val_loss: 2.8503\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7442 - val_loss: 2.8816\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7468 - val_loss: 2.8491\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7421 - val_loss: 2.8808\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7447 - val_loss: 2.8482\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7404 - val_loss: 2.8802\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7427 - val_loss: 2.8474\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7388 - val_loss: 2.8796\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.7408 - val_loss: 2.8468\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7375 - val_loss: 2.8792\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7391 - val_loss: 2.8463\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7366 - val_loss: 2.8790\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7375 - val_loss: 2.8454\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7354 - val_loss: 2.8786\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7355 - val_loss: 2.8466\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7369 - val_loss: 2.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7382 - val_loss: 2.8518\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7438 - val_loss: 2.8872\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7396 - val_loss: 2.8535\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7451 - val_loss: 2.8876\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7375 - val_loss: 2.8481\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7382 - val_loss: 2.8800\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7285 - val_loss: 2.8372\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7246 - val_loss: 2.8689\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7165 - val_loss: 2.8258\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7097 - val_loss: 2.8582\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7044 - val_loss: 2.8177\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6977 - val_loss: 2.8516\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6970 - val_loss: 2.8108\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6876 - val_loss: 2.8444\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6888 - val_loss: 2.8078\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6823 - val_loss: 2.8441\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6882 - val_loss: 2.8125\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.6867 - val_loss: 2.8507\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.6949 - val_loss: 2.8226\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6980 - val_loss: 2.8650\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 18us/step - loss: 2.7116 - val_loss: 2.8401\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.7188 - val_loss: 2.8732\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.7185 - val_loss: 2.8367\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7149 - val_loss: 2.8666\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.7098 - val_loss: 2.8311\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7082 - val_loss: 2.8658\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7071 - val_loss: 2.8316\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7086 - val_loss: 2.8674\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7073 - val_loss: 2.8333\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.7105 - val_loss: 2.8678\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7057 - val_loss: 2.8318\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7087 - val_loss: 2.8665\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7024 - val_loss: 2.8291\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7054 - val_loss: 2.8640\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6990 - val_loss: 2.8262\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7014 - val_loss: 2.8611\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6955 - val_loss: 2.8233\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6970 - val_loss: 2.8581\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6925 - val_loss: 2.8193\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6909 - val_loss: 2.8544\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.6885 - val_loss: 2.8165\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6866 - val_loss: 2.8489\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6825 - val_loss: 2.8116\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6795 - val_loss: 2.8463\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6783 - val_loss: 2.8093\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6767 - val_loss: 2.8468\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6766 - val_loss: 2.8083\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.6753 - val_loss: 2.8466\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 24us/step - loss: 2.6745 - val_loss: 2.8070\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 20us/step - loss: 2.6730 - val_loss: 2.8449\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.6715 - val_loss: 2.8053\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.6699 - val_loss: 2.8430\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6688 - val_loss: 2.8045\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6682 - val_loss: 2.8427\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6681 - val_loss: 2.8065\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6702 - val_loss: 2.8454\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6709 - val_loss: 2.8112\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6757 - val_loss: 2.8487\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6740 - val_loss: 2.8128\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6773 - val_loss: 2.8475\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6720 - val_loss: 2.8098\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6731 - val_loss: 2.8444\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6677 - val_loss: 2.8064\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6684 - val_loss: 2.8416\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6639 - val_loss: 2.8033\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6640 - val_loss: 2.8389\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6599 - val_loss: 2.7997\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6590 - val_loss: 2.8355\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6550 - val_loss: 2.7946\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6523 - val_loss: 2.8305\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6479 - val_loss: 2.7855\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6409 - val_loss: 2.8203\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6351 - val_loss: 2.7740\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6267 - val_loss: 2.8105\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6232 - val_loss: 2.7681\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6190 - val_loss: 2.8074\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6179 - val_loss: 2.7666\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6163 - val_loss: 2.8075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6168 - val_loss: 2.7686\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6179 - val_loss: 2.8124\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6226 - val_loss: 2.7788\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6300 - val_loss: 2.8280\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6401 - val_loss: 2.8055\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6626 - val_loss: 2.8520\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.6698 - val_loss: 2.8161\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.6739 - val_loss: 2.8461\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6622 - val_loss: 2.8052\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.6589 - val_loss: 2.8359\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.6494 - val_loss: 2.7995\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6508 - val_loss: 2.8333\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6472 - val_loss: 2.7995\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6499 - val_loss: 2.8331\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6475 - val_loss: 2.7987\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6476 - val_loss: 2.8308\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6449 - val_loss: 2.7962\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6432 - val_loss: 2.8276\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6408 - val_loss: 2.7935\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6389 - val_loss: 2.8242\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6365 - val_loss: 2.7907\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6344 - val_loss: 2.8228\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 17us/step - loss: 2.6348 - val_loss: 2.7904\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6327 - val_loss: 2.8219\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6336 - val_loss: 2.7897\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6310 - val_loss: 2.8210\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6319 - val_loss: 2.7887\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6293 - val_loss: 2.8202\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 19us/step - loss: 2.6299 - val_loss: 2.7876\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6276 - val_loss: 2.8192\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6276 - val_loss: 2.7862\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6256 - val_loss: 2.8176\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6243 - val_loss: 2.7839\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6226 - val_loss: 2.8157\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6201 - val_loss: 2.7809\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6197 - val_loss: 2.8172\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6188 - val_loss: 2.7806\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6214 - val_loss: 2.8216\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6205 - val_loss: 2.7787\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6208 - val_loss: 2.8187\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6129 - val_loss: 2.7681\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6070 - val_loss: 2.8047\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5959 - val_loss: 2.7565\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5912 - val_loss: 2.7932\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.5837 - val_loss: 2.7513\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.5844 - val_loss: 2.7923\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5828 - val_loss: 2.7545\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5880 - val_loss: 2.7999\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.5903 - val_loss: 2.7643\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5993 - val_loss: 2.8121\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6044 - val_loss: 2.7811\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6193 - val_loss: 2.8262\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6258 - val_loss: 2.7976\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6393 - val_loss: 2.8343\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6399 - val_loss: 2.7976\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6368 - val_loss: 2.8229\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6283 - val_loss: 2.7859\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 14us/step - loss: 2.6185 - val_loss: 2.8086\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6129 - val_loss: 2.7771\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6049 - val_loss: 2.7965\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5989 - val_loss: 2.7664\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5901 - val_loss: 2.7852\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5872 - val_loss: 2.7591\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5797 - val_loss: 2.7784\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5793 - val_loss: 2.7556\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5739 - val_loss: 2.7754\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5750 - val_loss: 2.7536\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5701 - val_loss: 2.7736\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5714 - val_loss: 2.7521\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5673 - val_loss: 2.7729\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5689 - val_loss: 2.7519\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5662 - val_loss: 2.7748\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5691 - val_loss: 2.7544\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5690 - val_loss: 2.7821\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5758 - val_loss: 2.7618\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5784 - val_loss: 2.7934\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5857 - val_loss: 2.7701\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5895 - val_loss: 2.8063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5967 - val_loss: 2.7808\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6049 - val_loss: 2.8218\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6133 - val_loss: 2.7963\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6267 - val_loss: 2.8349\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6262 - val_loss: 2.7990\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6307 - val_loss: 2.8323\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6215 - val_loss: 2.7928\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6217 - val_loss: 2.8258\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6123 - val_loss: 2.7835\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6083 - val_loss: 2.8140\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5955 - val_loss: 2.7672\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5869 - val_loss: 2.7996\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5759 - val_loss: 2.7564\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5726 - val_loss: 2.7925\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5676 - val_loss: 2.7542\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5700 - val_loss: 2.7940\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5691 - val_loss: 2.7580\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 1ms/step - loss: 56.5649 - val_loss: 54.4222\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 53.8407 - val_loss: 50.2402\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.6551 - val_loss: 44.7269\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 44.1482 - val_loss: 37.7500\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 37.1982 - val_loss: 29.7099\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 29.2512 - val_loss: 22.3017\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 22.1998 - val_loss: 18.2739\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 18.6953 - val_loss: 16.2483\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 17.0214 - val_loss: 15.1732\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 16.1044 - val_loss: 14.3529\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 15.3269 - val_loss: 13.3224\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 14.2758 - val_loss: 12.1507\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 13.0546 - val_loss: 11.2459\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 12.0871 - val_loss: 10.2899\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 11.0292 - val_loss: 9.4599\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 10.0434 - val_loss: 8.9801\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 9.3694 - val_loss: 8.7761\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0093 - val_loss: 8.6975\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8022 - val_loss: 8.6407\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.6569 - val_loss: 8.5446\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 8.5237 - val_loss: 8.4092\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3883 - val_loss: 8.2469\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.2428 - val_loss: 8.0693\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 8.0939 - val_loss: 7.8840\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.9474 - val_loss: 7.7066\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.8047 - val_loss: 7.5438\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.6629 - val_loss: 7.3900\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.5190 - val_loss: 7.2386\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.3717 - val_loss: 7.0864\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.2207 - val_loss: 6.9280\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.0656 - val_loss: 6.7606\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.9064 - val_loss: 6.5846\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.7433 - val_loss: 6.4024\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.5769 - val_loss: 6.2171\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.4082 - val_loss: 6.0309\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 6.2377 - val_loss: 5.8456\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.0650 - val_loss: 5.6607\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.8890 - val_loss: 5.4738\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.7094 - val_loss: 5.2830\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.5266 - val_loss: 5.0915\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 5.3422 - val_loss: 4.9040\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.1596 - val_loss: 4.7216\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.9820 - val_loss: 4.5467\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.8119 - val_loss: 4.3881\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.6529 - val_loss: 4.2494\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.5095 - val_loss: 4.1287\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.3842 - val_loss: 4.0268\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.2761 - val_loss: 3.9423\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 4.1813 - val_loss: 3.8687\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.0961 - val_loss: 3.8042\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0191 - val_loss: 3.7462\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.9506 - val_loss: 3.6958\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8891 - val_loss: 3.6520\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.8332 - val_loss: 3.6117\n",
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7816 - val_loss: 3.5758\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.7342 - val_loss: 3.5418\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6913 - val_loss: 3.5129\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6516 - val_loss: 3.4858\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.6145 - val_loss: 3.4608\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5800 - val_loss: 3.4340\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5476 - val_loss: 3.4119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5171 - val_loss: 3.3875\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4881 - val_loss: 3.3696\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4605 - val_loss: 3.3450\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4342 - val_loss: 3.3328\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4106 - val_loss: 3.3094\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3930 - val_loss: 3.3354\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3943 - val_loss: 3.3739\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4365 - val_loss: 3.4734\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5201 - val_loss: 3.6054\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6403 - val_loss: 3.6616\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6986 - val_loss: 3.6570\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6827 - val_loss: 3.5774\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.6065 - val_loss: 3.4952\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.5254 - val_loss: 3.4371\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4682 - val_loss: 3.4183\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4431 - val_loss: 3.3880\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.4128 - val_loss: 3.3910\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4119 - val_loss: 3.3785\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3998 - val_loss: 3.3924\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.4056 - val_loss: 3.3677\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.3857 - val_loss: 3.3745\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3818 - val_loss: 3.3381\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.3524 - val_loss: 3.3474\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.3496 - val_loss: 3.3067\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3174 - val_loss: 3.3253\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3230 - val_loss: 3.2859\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2925 - val_loss: 3.3029\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2956 - val_loss: 3.2618\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2637 - val_loss: 3.2790\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2679 - val_loss: 3.2425\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.2399 - val_loss: 3.2570\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2421 - val_loss: 3.2259\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 3.2190 - val_loss: 3.2368\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2181 - val_loss: 3.2090\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1988 - val_loss: 3.2210\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1994 - val_loss: 3.1958\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1822 - val_loss: 3.2027\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1775 - val_loss: 3.1766\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1601 - val_loss: 3.1860\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1593 - val_loss: 3.1661\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1465 - val_loss: 3.1775\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1485 - val_loss: 3.1585\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1360 - val_loss: 3.1684\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1377 - val_loss: 3.1500\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1245 - val_loss: 3.1583\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1263 - val_loss: 3.1421\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.1135 - val_loss: 3.1490\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.1160 - val_loss: 3.1353\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1038 - val_loss: 3.1403\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 3.1067 - val_loss: 3.1295\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0949 - val_loss: 3.1315\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0977 - val_loss: 3.1235\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0859 - val_loss: 3.1218\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0883 - val_loss: 3.1158\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0748 - val_loss: 3.1109\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0778 - val_loss: 3.1072\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0624 - val_loss: 3.1001\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0674 - val_loss: 3.1014\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0543 - val_loss: 3.0941\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0617 - val_loss: 3.0995\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0507 - val_loss: 3.0897\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0582 - val_loss: 3.0983\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0478 - val_loss: 3.0846\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0540 - val_loss: 3.0927\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0398 - val_loss: 3.0733\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0434 - val_loss: 3.0808\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0250 - val_loss: 3.0562\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0268 - val_loss: 3.0609\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0012 - val_loss: 3.0268\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9950 - val_loss: 3.0318\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9711 - val_loss: 3.0019\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9667 - val_loss: 3.0153\n",
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9522 - val_loss: 2.9884\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9539 - val_loss: 3.0135\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9479 - val_loss: 2.9885\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9559 - val_loss: 3.0218\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9537 - val_loss: 2.9958\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9653 - val_loss: 3.0302\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9607 - val_loss: 3.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9729 - val_loss: 3.0348\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9642 - val_loss: 3.0001\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9732 - val_loss: 3.0309\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.9595 - val_loss: 2.9911\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9630 - val_loss: 3.0183\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9460 - val_loss: 2.9757\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9467 - val_loss: 3.0046\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9309 - val_loss: 2.9624\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9336 - val_loss: 2.9963\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9214 - val_loss: 2.9563\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9282 - val_loss: 2.9944\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9184 - val_loss: 2.9543\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9276 - val_loss: 2.9953\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9182 - val_loss: 2.9528\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9275 - val_loss: 2.9946\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9167 - val_loss: 2.9492\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9246 - val_loss: 2.9900\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9114 - val_loss: 2.9425\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9179 - val_loss: 2.9820\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9028 - val_loss: 2.9341\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9089 - val_loss: 2.9730\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8934 - val_loss: 2.9255\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8990 - val_loss: 2.9649\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8846 - val_loss: 2.9175\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8905 - val_loss: 2.9559\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8735 - val_loss: 2.9060\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8803 - val_loss: 2.9505\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8678 - val_loss: 2.9039\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8783 - val_loss: 2.9504\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8671 - val_loss: 2.9048\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8820 - val_loss: 2.9543\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8703 - val_loss: 2.9059\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8845 - val_loss: 2.9540\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8698 - val_loss: 2.9030\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8818 - val_loss: 2.9483\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8638 - val_loss: 2.8961\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8737 - val_loss: 2.9398\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8551 - val_loss: 2.8852\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8599 - val_loss: 2.9286\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8433 - val_loss: 2.8756\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8497 - val_loss: 2.9238\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8375 - val_loss: 2.8722\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8465 - val_loss: 2.9228\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8359 - val_loss: 2.8707\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8459 - val_loss: 2.9221\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8346 - val_loss: 2.8682\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8447 - val_loss: 2.9211\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8328 - val_loss: 2.8667\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8456 - val_loss: 2.9218\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8330 - val_loss: 2.8662\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8461 - val_loss: 2.9201\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8312 - val_loss: 2.8615\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8401 - val_loss: 2.9132\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8239 - val_loss: 2.8541\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8311 - val_loss: 2.9063\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8167 - val_loss: 2.8443\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8201 - val_loss: 2.8987\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8082 - val_loss: 2.8371\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8126 - val_loss: 2.8951\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8039 - val_loss: 2.8344\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8109 - val_loss: 2.8949\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8033 - val_loss: 2.8344\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8129 - val_loss: 2.8965\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8044 - val_loss: 2.8350\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8160 - val_loss: 2.8979\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8053 - val_loss: 2.8342\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 16us/step - loss: 2.8168 - val_loss: 2.8964\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8036 - val_loss: 2.8305\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8123 - val_loss: 2.8909\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7980 - val_loss: 2.8245\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8043 - val_loss: 2.8843\n",
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7913 - val_loss: 2.8185\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7965 - val_loss: 2.8790\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7855 - val_loss: 2.8138\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7912 - val_loss: 2.8758\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7819 - val_loss: 2.8107\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7882 - val_loss: 2.8741\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7798 - val_loss: 2.8088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7865 - val_loss: 2.8731\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7786 - val_loss: 2.8074\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7852 - val_loss: 2.8722\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7777 - val_loss: 2.8063\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7837 - val_loss: 2.8702\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7757 - val_loss: 2.8043\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7809 - val_loss: 2.8669\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7725 - val_loss: 2.8014\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7772 - val_loss: 2.8635\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7690 - val_loss: 2.7985\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7738 - val_loss: 2.8606\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7660 - val_loss: 2.7960\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7709 - val_loss: 2.8581\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7633 - val_loss: 2.7936\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7683 - val_loss: 2.8556\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7606 - val_loss: 2.7911\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7656 - val_loss: 2.8530\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7577 - val_loss: 2.7886\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7627 - val_loss: 2.8503\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7548 - val_loss: 2.7860\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7598 - val_loss: 2.8476\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7519 - val_loss: 2.7835\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7570 - val_loss: 2.8450\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7490 - val_loss: 2.7810\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7541 - val_loss: 2.8424\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7461 - val_loss: 2.7785\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7513 - val_loss: 2.8398\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7433 - val_loss: 2.7761\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7485 - val_loss: 2.8373\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7405 - val_loss: 2.7736\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7457 - val_loss: 2.8348\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7377 - val_loss: 2.7713\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7429 - val_loss: 2.8323\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7349 - val_loss: 2.7690\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7402 - val_loss: 2.8299\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7323 - val_loss: 2.7667\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7376 - val_loss: 2.8276\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7297 - val_loss: 2.7645\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7350 - val_loss: 2.8253\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7271 - val_loss: 2.7624\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7325 - val_loss: 2.8231\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7246 - val_loss: 2.7603\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7301 - val_loss: 2.8209\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7222 - val_loss: 2.7583\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7277 - val_loss: 2.8188\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7197 - val_loss: 2.7563\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7253 - val_loss: 2.8167\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7173 - val_loss: 2.7543\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7229 - val_loss: 2.8146\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7149 - val_loss: 2.7524\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7206 - val_loss: 2.8125\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7125 - val_loss: 2.7505\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7183 - val_loss: 2.8105\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7102 - val_loss: 2.7487\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7160 - val_loss: 2.8084\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.7078 - val_loss: 2.7469\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7137 - val_loss: 2.8064\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7055 - val_loss: 2.7451\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7114 - val_loss: 2.8044\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7031 - val_loss: 2.7433\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7092 - val_loss: 2.8024\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7008 - val_loss: 2.7415\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7070 - val_loss: 2.8005\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6985 - val_loss: 2.7397\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7047 - val_loss: 2.7986\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6963 - val_loss: 2.7379\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7025 - val_loss: 2.7967\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6940 - val_loss: 2.7361\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7002 - val_loss: 2.7949\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6917 - val_loss: 2.7343\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6979 - val_loss: 2.7930\n",
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6895 - val_loss: 2.7324\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6956 - val_loss: 2.7912\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6872 - val_loss: 2.7306\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6933 - val_loss: 2.7894\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6849 - val_loss: 2.7288\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6910 - val_loss: 2.7876\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6826 - val_loss: 2.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6887 - val_loss: 2.7859\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6805 - val_loss: 2.7255\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6865 - val_loss: 2.7843\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6784 - val_loss: 2.7241\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6846 - val_loss: 2.7829\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6765 - val_loss: 2.7231\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6830 - val_loss: 2.7817\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6748 - val_loss: 2.7224\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6816 - val_loss: 2.7806\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6733 - val_loss: 2.7215\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6801 - val_loss: 2.7794\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6715 - val_loss: 2.7201\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6781 - val_loss: 2.7777\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6694 - val_loss: 2.7181\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6756 - val_loss: 2.7758\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6669 - val_loss: 2.7155\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6726 - val_loss: 2.7736\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6639 - val_loss: 2.7123\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6689 - val_loss: 2.7708\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6603 - val_loss: 2.7080\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6641 - val_loss: 2.7671\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6556 - val_loss: 2.7040\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6591 - val_loss: 2.7641\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6522 - val_loss: 2.7030\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6571 - val_loss: 2.7639\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6517 - val_loss: 2.7054\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6590 - val_loss: 2.7674\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6553 - val_loss: 2.7130\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6666 - val_loss: 2.7727\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6609 - val_loss: 2.7183\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6714 - val_loss: 2.7726\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6603 - val_loss: 2.7147\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6668 - val_loss: 2.7677\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6548 - val_loss: 2.7094\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6604 - val_loss: 2.7639\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6504 - val_loss: 2.7054\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6553 - val_loss: 2.7606\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6467 - val_loss: 2.7029\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6517 - val_loss: 2.7582\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6440 - val_loss: 2.7016\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6493 - val_loss: 2.7568\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6423 - val_loss: 2.7019\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6487 - val_loss: 2.7568\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6423 - val_loss: 2.7045\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6506 - val_loss: 2.7579\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6435 - val_loss: 2.7066\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6520 - val_loss: 2.7578\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6428 - val_loss: 2.7051\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6499 - val_loss: 2.7555\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6399 - val_loss: 2.7019\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6458 - val_loss: 2.7527\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6363 - val_loss: 2.6988\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6417 - val_loss: 2.7500\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6329 - val_loss: 2.6965\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6385 - val_loss: 2.7481\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6304 - val_loss: 2.6955\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6366 - val_loss: 2.7473\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6294 - val_loss: 2.6964\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6367 - val_loss: 2.7480\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6300 - val_loss: 2.6987\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6383 - val_loss: 2.7489\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6305 - val_loss: 2.6992\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6381 - val_loss: 2.7480\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6288 - val_loss: 2.6969\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6350 - val_loss: 2.7454\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6252 - val_loss: 2.6933\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6305 - val_loss: 2.7421\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6209 - val_loss: 2.6895\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6256 - val_loss: 2.7375\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6148 - val_loss: 2.6799\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6151 - val_loss: 2.7301\n",
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6064 - val_loss: 2.6764\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6098 - val_loss: 2.7291\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6051 - val_loss: 2.6798\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6120 - val_loss: 2.7313\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6070 - val_loss: 2.6833\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6147 - val_loss: 2.7358\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6126 - val_loss: 2.6944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6255 - val_loss: 2.7436\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6211 - val_loss: 2.7024\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6320 - val_loss: 2.7431\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6192 - val_loss: 2.6937\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6224 - val_loss: 2.7371\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6124 - val_loss: 2.6903\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6183 - val_loss: 2.7357\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6087 - val_loss: 2.6859\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6136 - val_loss: 2.7320\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6044 - val_loss: 2.6838\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6104 - val_loss: 2.7305\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6026 - val_loss: 2.6842\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6090 - val_loss: 2.7298\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6021 - val_loss: 2.6860\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6091 - val_loss: 2.7322\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6062 - val_loss: 2.6950\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6159 - val_loss: 2.7333\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6066 - val_loss: 2.6939\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6119 - val_loss: 2.7273\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6010 - val_loss: 2.6866\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6022 - val_loss: 2.7154\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5865 - val_loss: 2.6731\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5860 - val_loss: 2.7075\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5781 - val_loss: 2.6707\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5830 - val_loss: 2.7099\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5795 - val_loss: 2.6767\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5886 - val_loss: 2.7158\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5858 - val_loss: 2.6856\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5966 - val_loss: 2.7230\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5948 - val_loss: 2.6936\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6038 - val_loss: 2.7258\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5952 - val_loss: 2.6890\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5990 - val_loss: 2.7218\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5892 - val_loss: 2.6830\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5926 - val_loss: 2.7161\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5805 - val_loss: 2.6739\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5831 - val_loss: 2.7109\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5742 - val_loss: 2.6712\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5798 - val_loss: 2.7113\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5742 - val_loss: 2.6755\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5829 - val_loss: 2.7177\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5830 - val_loss: 2.6898\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5959 - val_loss: 2.7263\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5917 - val_loss: 2.6933\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5985 - val_loss: 2.7248\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5885 - val_loss: 2.6867\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5912 - val_loss: 2.7196\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5817 - val_loss: 2.6811\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5848 - val_loss: 2.7123\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5719 - val_loss: 2.6708\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5738 - val_loss: 2.7056\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5633 - val_loss: 2.6647\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5667 - val_loss: 2.7040\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5610 - val_loss: 2.6679\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5683 - val_loss: 2.7104\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5703 - val_loss: 2.6862\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5850 - val_loss: 2.7225\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5845 - val_loss: 2.6971\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5946 - val_loss: 2.7228\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5835 - val_loss: 2.6893\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5862 - val_loss: 2.7179\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5765 - val_loss: 2.6818\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5782 - val_loss: 2.7108\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5672 - val_loss: 2.6742\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5694 - val_loss: 2.7056\n",
      "Epoch 440/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5611 - val_loss: 2.6719\n",
      "Epoch 441/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5661 - val_loss: 2.7055\n",
      "Epoch 442/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5609 - val_loss: 2.6750\n",
      "Epoch 443/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5679 - val_loss: 2.7087\n",
      "Epoch 444/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5648 - val_loss: 2.6803\n",
      "Epoch 445/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5720 - val_loss: 2.7126\n",
      "Epoch 446/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5692 - val_loss: 2.6850\n",
      "Epoch 447/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5753 - val_loss: 2.7139\n",
      "Epoch 448/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5697 - val_loss: 2.6840\n",
      "Epoch 449/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5731 - val_loss: 2.7098\n",
      "Epoch 450/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5637 - val_loss: 2.6769\n",
      "Epoch 451/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5651 - val_loss: 2.7039\n",
      "Epoch 452/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5556 - val_loss: 2.6710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5590 - val_loss: 2.7030\n",
      "Epoch 454/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5536 - val_loss: 2.6724\n",
      "Epoch 455/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5604 - val_loss: 2.7070\n",
      "Epoch 456/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5570 - val_loss: 2.6765\n",
      "Epoch 457/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5647 - val_loss: 2.7118\n",
      "Epoch 458/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5610 - val_loss: 2.6795\n",
      "Epoch 459/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5681 - val_loss: 2.7173\n",
      "Epoch 460/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5661 - val_loss: 2.6848\n",
      "Epoch 461/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5739 - val_loss: 2.7210\n",
      "Epoch 462/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5686 - val_loss: 2.6840\n",
      "Epoch 463/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5732 - val_loss: 2.7185\n",
      "Epoch 464/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5639 - val_loss: 2.6773\n",
      "Epoch 465/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5667 - val_loss: 2.7116\n",
      "Epoch 466/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5544 - val_loss: 2.6691\n",
      "Epoch 467/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5578 - val_loss: 2.7075\n",
      "Epoch 468/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5497 - val_loss: 2.6690\n",
      "Epoch 469/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5566 - val_loss: 2.7083\n",
      "Epoch 470/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5512 - val_loss: 2.6734\n",
      "Epoch 471/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5588 - val_loss: 2.7111\n",
      "Epoch 472/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5555 - val_loss: 2.6789\n",
      "Epoch 473/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5622 - val_loss: 2.7137\n",
      "Epoch 474/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5591 - val_loss: 2.6839\n",
      "Epoch 475/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5645 - val_loss: 2.7130\n",
      "Epoch 476/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5591 - val_loss: 2.6858\n",
      "Train on 1036 samples, validate on 259 samples\n",
      "Epoch 1/600\n",
      "1036/1036 [==============================] - 1s 1ms/step - loss: 56.3828 - val_loss: 53.6831\n",
      "Epoch 2/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 53.6461 - val_loss: 49.3823\n",
      "Epoch 3/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 49.3564 - val_loss: 43.7232\n",
      "Epoch 4/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 43.7099 - val_loss: 36.7502\n",
      "Epoch 5/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 36.7629 - val_loss: 28.9578\n",
      "Epoch 6/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 29.0533 - val_loss: 22.3041\n",
      "Epoch 7/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 22.3763 - val_loss: 19.0278\n",
      "Epoch 8/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 18.7770 - val_loss: 17.3009\n",
      "Epoch 9/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 16.8363 - val_loss: 16.3596\n",
      "Epoch 10/600\n",
      "1036/1036 [==============================] - 0s 5us/step - loss: 15.8020 - val_loss: 15.6630\n",
      "Epoch 11/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 15.0612 - val_loss: 14.8370\n",
      "Epoch 12/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 14.2207 - val_loss: 13.6864\n",
      "Epoch 13/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 13.0912 - val_loss: 12.0550\n",
      "Epoch 14/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 11.5458 - val_loss: 10.7310\n",
      "Epoch 15/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 10.3821 - val_loss: 9.7908\n",
      "Epoch 16/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.5260 - val_loss: 9.2387\n",
      "Epoch 17/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 9.0327 - val_loss: 8.9306\n",
      "Epoch 18/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.8042 - val_loss: 8.6759\n",
      "Epoch 19/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 8.6134 - val_loss: 8.5298\n",
      "Epoch 20/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.4806 - val_loss: 8.4155\n",
      "Epoch 21/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.3336 - val_loss: 8.3236\n",
      "Epoch 22/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.1924 - val_loss: 8.2159\n",
      "Epoch 23/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 8.0450 - val_loss: 8.1058\n",
      "Epoch 24/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 7.9035 - val_loss: 7.9941\n",
      "Epoch 25/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.7632 - val_loss: 7.8684\n",
      "Epoch 26/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.6199 - val_loss: 7.7247\n",
      "Epoch 27/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 7.4698 - val_loss: 7.5673\n",
      "Epoch 28/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 7.3125 - val_loss: 7.4007\n",
      "Epoch 29/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 7.1498 - val_loss: 7.2298\n",
      "Epoch 30/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.9836 - val_loss: 7.0571\n",
      "Epoch 31/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.8153 - val_loss: 6.8832\n",
      "Epoch 32/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.6461 - val_loss: 6.7092\n",
      "Epoch 33/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 6.4765 - val_loss: 6.5350\n",
      "Epoch 34/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 6.3059 - val_loss: 6.3597\n",
      "Epoch 35/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 6.1344 - val_loss: 6.1838\n",
      "Epoch 36/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.9623 - val_loss: 6.0084\n",
      "Epoch 37/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 5.7903 - val_loss: 5.8342\n",
      "Epoch 38/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 5.6195 - val_loss: 5.6626\n",
      "Epoch 39/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.4507 - val_loss: 5.4956\n",
      "Epoch 40/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 5.2844 - val_loss: 5.3319\n",
      "Epoch 41/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 5.1218 - val_loss: 5.1710\n",
      "Epoch 42/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 4.9647 - val_loss: 5.0183\n",
      "Epoch 43/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.8143 - val_loss: 4.8736\n",
      "Epoch 44/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.6723 - val_loss: 4.7419\n",
      "Epoch 45/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.5416 - val_loss: 4.6254\n",
      "Epoch 46/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.4238 - val_loss: 4.5226\n",
      "Epoch 47/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 4.3192 - val_loss: 4.4343\n",
      "Epoch 48/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.2276 - val_loss: 4.3537\n",
      "Epoch 49/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.1463 - val_loss: 4.2830\n",
      "Epoch 50/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 4.0726 - val_loss: 4.2160\n",
      "Epoch 51/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 4.0047 - val_loss: 4.1543\n",
      "Epoch 52/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.9419 - val_loss: 4.0971\n",
      "Epoch 53/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.8837 - val_loss: 4.0431\n",
      "Epoch 54/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.8299 - val_loss: 3.9920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.7795 - val_loss: 3.9427\n",
      "Epoch 56/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.7321 - val_loss: 3.8975\n",
      "Epoch 57/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.6874 - val_loss: 3.8558\n",
      "Epoch 58/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.6457 - val_loss: 3.8167\n",
      "Epoch 59/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.6066 - val_loss: 3.7789\n",
      "Epoch 60/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.5698 - val_loss: 3.7413\n",
      "Epoch 61/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5352 - val_loss: 3.7053\n",
      "Epoch 62/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.5027 - val_loss: 3.6710\n",
      "Epoch 63/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4722 - val_loss: 3.6379\n",
      "Epoch 64/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4435 - val_loss: 3.6056\n",
      "Epoch 65/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.4163 - val_loss: 3.5741\n",
      "Epoch 66/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.3903 - val_loss: 3.5442\n",
      "Epoch 67/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.3654 - val_loss: 3.5155\n",
      "Epoch 68/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3417 - val_loss: 3.4890\n",
      "Epoch 69/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.3193 - val_loss: 3.4633\n",
      "Epoch 70/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2980 - val_loss: 3.4378\n",
      "Epoch 71/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.2778 - val_loss: 3.4139\n",
      "Epoch 72/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2584 - val_loss: 3.3907\n",
      "Epoch 73/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.2400 - val_loss: 3.3685\n",
      "Epoch 74/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.2225 - val_loss: 3.3465\n",
      "Epoch 75/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.2060 - val_loss: 3.3260\n",
      "Epoch 76/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1904 - val_loss: 3.3059\n",
      "Epoch 77/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1755 - val_loss: 3.2869\n",
      "Epoch 78/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1612 - val_loss: 3.2687\n",
      "Epoch 79/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1475 - val_loss: 3.2514\n",
      "Epoch 80/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.1343 - val_loss: 3.2351\n",
      "Epoch 81/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.1218 - val_loss: 3.2195\n",
      "Epoch 82/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.1097 - val_loss: 3.2043\n",
      "Epoch 83/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0982 - val_loss: 3.1894\n",
      "Epoch 84/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0872 - val_loss: 3.1770\n",
      "Epoch 85/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0769 - val_loss: 3.1611\n",
      "Epoch 86/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0681 - val_loss: 3.1562\n",
      "Epoch 87/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0602 - val_loss: 3.1389\n",
      "Epoch 88/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0542 - val_loss: 3.1396\n",
      "Epoch 89/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0485 - val_loss: 3.1205\n",
      "Epoch 90/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0436 - val_loss: 3.1239\n",
      "Epoch 91/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 3.0371 - val_loss: 3.1026\n",
      "Epoch 92/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0338 - val_loss: 3.1084\n",
      "Epoch 93/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 3.0259 - val_loss: 3.0855\n",
      "Epoch 94/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0239 - val_loss: 3.0939\n",
      "Epoch 95/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 3.0143 - val_loss: 3.0689\n",
      "Epoch 96/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 3.0121 - val_loss: 3.0780\n",
      "Epoch 97/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 3.0015 - val_loss: 3.0516\n",
      "Epoch 98/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9990 - val_loss: 3.0621\n",
      "Epoch 99/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9884 - val_loss: 3.0356\n",
      "Epoch 100/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.9866 - val_loss: 3.0479\n",
      "Epoch 101/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9766 - val_loss: 3.0230\n",
      "Epoch 102/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.9769 - val_loss: 3.0371\n",
      "Epoch 103/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9677 - val_loss: 3.0110\n",
      "Epoch 104/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9679 - val_loss: 3.0262\n",
      "Epoch 105/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9585 - val_loss: 3.0003\n",
      "Epoch 106/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9593 - val_loss: 3.0139\n",
      "Epoch 107/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9482 - val_loss: 2.9858\n",
      "Epoch 108/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9476 - val_loss: 2.9989\n",
      "Epoch 109/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9350 - val_loss: 2.9678\n",
      "Epoch 110/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9330 - val_loss: 2.9846\n",
      "Epoch 111/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9221 - val_loss: 2.9544\n",
      "Epoch 112/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.9212 - val_loss: 2.9738\n",
      "Epoch 113/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9127 - val_loss: 2.9459\n",
      "Epoch 114/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9131 - val_loss: 2.9652\n",
      "Epoch 115/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9054 - val_loss: 2.9394\n",
      "Epoch 116/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9049 - val_loss: 2.9495\n",
      "Epoch 117/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8942 - val_loss: 2.9303\n",
      "Epoch 118/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8911 - val_loss: 2.9333\n",
      "Epoch 119/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8814 - val_loss: 2.9170\n",
      "Epoch 120/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8787 - val_loss: 2.9230\n",
      "Epoch 121/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8716 - val_loss: 2.9085\n",
      "Epoch 122/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8707 - val_loss: 2.9161\n",
      "Epoch 123/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8649 - val_loss: 2.9030\n",
      "Epoch 124/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8650 - val_loss: 2.9117\n",
      "Epoch 125/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8595 - val_loss: 2.8972\n",
      "Epoch 126/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8597 - val_loss: 2.9078\n",
      "Epoch 127/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8541 - val_loss: 2.8914\n",
      "Epoch 128/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8545 - val_loss: 2.9047\n",
      "Epoch 129/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8493 - val_loss: 2.8867\n",
      "Epoch 130/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8505 - val_loss: 2.9039\n",
      "Epoch 131/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8465 - val_loss: 2.8881\n",
      "Epoch 132/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8530 - val_loss: 2.9185\n",
      "Epoch 133/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8571 - val_loss: 2.9009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8696 - val_loss: 2.9513\n",
      "Epoch 135/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8826 - val_loss: 2.9339\n",
      "Epoch 136/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9083 - val_loss: 2.9965\n",
      "Epoch 137/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9202 - val_loss: 2.9533\n",
      "Epoch 138/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.9295 - val_loss: 2.9962\n",
      "Epoch 139/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.9183 - val_loss: 2.9351\n",
      "Epoch 140/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.9123 - val_loss: 2.9660\n",
      "Epoch 141/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8911 - val_loss: 2.9033\n",
      "Epoch 142/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8811 - val_loss: 2.9406\n",
      "Epoch 143/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8681 - val_loss: 2.8886\n",
      "Epoch 144/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8656 - val_loss: 2.9322\n",
      "Epoch 145/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8600 - val_loss: 2.8835\n",
      "Epoch 146/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8605 - val_loss: 2.9302\n",
      "Epoch 147/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8566 - val_loss: 2.8793\n",
      "Epoch 148/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8567 - val_loss: 2.9275\n",
      "Epoch 149/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.8524 - val_loss: 2.8751\n",
      "Epoch 150/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8522 - val_loss: 2.9231\n",
      "Epoch 151/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8475 - val_loss: 2.8699\n",
      "Epoch 152/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8456 - val_loss: 2.9161\n",
      "Epoch 153/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8407 - val_loss: 2.8639\n",
      "Epoch 154/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8378 - val_loss: 2.9093\n",
      "Epoch 155/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8336 - val_loss: 2.8590\n",
      "Epoch 156/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8319 - val_loss: 2.9053\n",
      "Epoch 157/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8287 - val_loss: 2.8557\n",
      "Epoch 158/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8279 - val_loss: 2.9028\n",
      "Epoch 159/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8251 - val_loss: 2.8528\n",
      "Epoch 160/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8244 - val_loss: 2.9002\n",
      "Epoch 161/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8215 - val_loss: 2.8498\n",
      "Epoch 162/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8206 - val_loss: 2.8973\n",
      "Epoch 163/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8177 - val_loss: 2.8469\n",
      "Epoch 164/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8169 - val_loss: 2.8948\n",
      "Epoch 165/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8144 - val_loss: 2.8443\n",
      "Epoch 166/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8134 - val_loss: 2.8927\n",
      "Epoch 167/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8114 - val_loss: 2.8419\n",
      "Epoch 168/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8102 - val_loss: 2.8906\n",
      "Epoch 169/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8084 - val_loss: 2.8396\n",
      "Epoch 170/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.8069 - val_loss: 2.8882\n",
      "Epoch 171/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.8052 - val_loss: 2.8372\n",
      "Epoch 172/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.8032 - val_loss: 2.8852\n",
      "Epoch 173/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.8017 - val_loss: 2.8344\n",
      "Epoch 174/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7988 - val_loss: 2.8812\n",
      "Epoch 175/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7973 - val_loss: 2.8304\n",
      "Epoch 176/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7930 - val_loss: 2.8754\n",
      "Epoch 177/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7915 - val_loss: 2.8260\n",
      "Epoch 178/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7868 - val_loss: 2.8695\n",
      "Epoch 179/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.7854 - val_loss: 2.8217\n",
      "Epoch 180/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7812 - val_loss: 2.8641\n",
      "Epoch 181/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7795 - val_loss: 2.8176\n",
      "Epoch 182/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7760 - val_loss: 2.8603\n",
      "Epoch 183/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7750 - val_loss: 2.8152\n",
      "Epoch 184/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7723 - val_loss: 2.8578\n",
      "Epoch 185/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7720 - val_loss: 2.8134\n",
      "Epoch 186/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7694 - val_loss: 2.8557\n",
      "Epoch 187/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.7693 - val_loss: 2.8115\n",
      "Epoch 188/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7664 - val_loss: 2.8534\n",
      "Epoch 189/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7663 - val_loss: 2.8093\n",
      "Epoch 190/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7631 - val_loss: 2.8508\n",
      "Epoch 191/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7630 - val_loss: 2.8070\n",
      "Epoch 192/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7597 - val_loss: 2.8478\n",
      "Epoch 193/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7595 - val_loss: 2.8045\n",
      "Epoch 194/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7561 - val_loss: 2.8444\n",
      "Epoch 195/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7557 - val_loss: 2.8020\n",
      "Epoch 196/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7521 - val_loss: 2.8376\n",
      "Epoch 197/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7499 - val_loss: 2.7978\n",
      "Epoch 198/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7453 - val_loss: 2.8277\n",
      "Epoch 199/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7416 - val_loss: 2.7922\n",
      "Epoch 200/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7364 - val_loss: 2.8162\n",
      "Epoch 201/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.7321 - val_loss: 2.7869\n",
      "Epoch 202/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7275 - val_loss: 2.8072\n",
      "Epoch 203/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7239 - val_loss: 2.7808\n",
      "Epoch 204/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7189 - val_loss: 2.7990\n",
      "Epoch 205/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7161 - val_loss: 2.7771\n",
      "Epoch 206/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7131 - val_loss: 2.7964\n",
      "Epoch 207/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7132 - val_loss: 2.7779\n",
      "Epoch 208/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7124 - val_loss: 2.7978\n",
      "Epoch 209/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7142 - val_loss: 2.7805\n",
      "Epoch 210/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7136 - val_loss: 2.7996\n",
      "Epoch 211/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7151 - val_loss: 2.7811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7130 - val_loss: 2.7980\n",
      "Epoch 213/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7130 - val_loss: 2.7790\n",
      "Epoch 214/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7086 - val_loss: 2.7907\n",
      "Epoch 215/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7070 - val_loss: 2.7782\n",
      "Epoch 216/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7039 - val_loss: 2.7841\n",
      "Epoch 217/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7028 - val_loss: 2.7776\n",
      "Epoch 218/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6990 - val_loss: 2.7757\n",
      "Epoch 219/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6976 - val_loss: 2.7775\n",
      "Epoch 220/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6949 - val_loss: 2.7697\n",
      "Epoch 221/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6948 - val_loss: 2.7794\n",
      "Epoch 222/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6922 - val_loss: 2.7658\n",
      "Epoch 223/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6929 - val_loss: 2.7808\n",
      "Epoch 224/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6904 - val_loss: 2.7646\n",
      "Epoch 225/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6913 - val_loss: 2.7795\n",
      "Epoch 226/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6880 - val_loss: 2.7643\n",
      "Epoch 227/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6888 - val_loss: 2.7772\n",
      "Epoch 228/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6851 - val_loss: 2.7638\n",
      "Epoch 229/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6861 - val_loss: 2.7743\n",
      "Epoch 230/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6819 - val_loss: 2.7631\n",
      "Epoch 231/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6834 - val_loss: 2.7716\n",
      "Epoch 232/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6793 - val_loss: 2.7639\n",
      "Epoch 233/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6819 - val_loss: 2.7708\n",
      "Epoch 234/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6789 - val_loss: 2.7670\n",
      "Epoch 235/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6816 - val_loss: 2.7707\n",
      "Epoch 236/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6794 - val_loss: 2.7709\n",
      "Epoch 237/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6813 - val_loss: 2.7690\n",
      "Epoch 238/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6793 - val_loss: 2.7748\n",
      "Epoch 239/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6808 - val_loss: 2.7657\n",
      "Epoch 240/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6810 - val_loss: 2.7822\n",
      "Epoch 241/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6835 - val_loss: 2.7653\n",
      "Epoch 242/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6852 - val_loss: 2.7910\n",
      "Epoch 243/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6881 - val_loss: 2.7679\n",
      "Epoch 244/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6922 - val_loss: 2.8009\n",
      "Epoch 245/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6936 - val_loss: 2.7698\n",
      "Epoch 246/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.7003 - val_loss: 2.8102\n",
      "Epoch 247/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6996 - val_loss: 2.7683\n",
      "Epoch 248/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7030 - val_loss: 2.8116\n",
      "Epoch 249/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6990 - val_loss: 2.7654\n",
      "Epoch 250/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7002 - val_loss: 2.8065\n",
      "Epoch 251/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6937 - val_loss: 2.7624\n",
      "Epoch 252/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6942 - val_loss: 2.7995\n",
      "Epoch 253/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6874 - val_loss: 2.7609\n",
      "Epoch 254/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6880 - val_loss: 2.7941\n",
      "Epoch 255/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6829 - val_loss: 2.7583\n",
      "Epoch 256/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6791 - val_loss: 2.7824\n",
      "Epoch 257/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6734 - val_loss: 2.7545\n",
      "Epoch 258/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6689 - val_loss: 2.7711\n",
      "Epoch 259/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6649 - val_loss: 2.7511\n",
      "Epoch 260/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6596 - val_loss: 2.7621\n",
      "Epoch 261/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6577 - val_loss: 2.7491\n",
      "Epoch 262/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6527 - val_loss: 2.7560\n",
      "Epoch 263/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6525 - val_loss: 2.7483\n",
      "Epoch 264/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6491 - val_loss: 2.7533\n",
      "Epoch 265/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6501 - val_loss: 2.7478\n",
      "Epoch 266/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6468 - val_loss: 2.7517\n",
      "Epoch 267/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6483 - val_loss: 2.7472\n",
      "Epoch 268/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6446 - val_loss: 2.7502\n",
      "Epoch 269/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6464 - val_loss: 2.7464\n",
      "Epoch 270/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6425 - val_loss: 2.7491\n",
      "Epoch 271/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6446 - val_loss: 2.7456\n",
      "Epoch 272/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6408 - val_loss: 2.7484\n",
      "Epoch 273/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6429 - val_loss: 2.7447\n",
      "Epoch 274/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6391 - val_loss: 2.7481\n",
      "Epoch 275/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6412 - val_loss: 2.7438\n",
      "Epoch 276/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6375 - val_loss: 2.7480\n",
      "Epoch 277/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6396 - val_loss: 2.7428\n",
      "Epoch 278/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6361 - val_loss: 2.7483\n",
      "Epoch 279/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6380 - val_loss: 2.7417\n",
      "Epoch 280/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6350 - val_loss: 2.7492\n",
      "Epoch 281/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6368 - val_loss: 2.7406\n",
      "Epoch 282/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6342 - val_loss: 2.7510\n",
      "Epoch 283/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6361 - val_loss: 2.7396\n",
      "Epoch 284/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6340 - val_loss: 2.7535\n",
      "Epoch 285/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6358 - val_loss: 2.7387\n",
      "Epoch 286/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6345 - val_loss: 2.7564\n",
      "Epoch 287/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6359 - val_loss: 2.7380\n",
      "Epoch 288/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6364 - val_loss: 2.7611\n",
      "Epoch 289/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6378 - val_loss: 2.7374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6393 - val_loss: 2.7669\n",
      "Epoch 291/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6405 - val_loss: 2.7385\n",
      "Epoch 292/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6436 - val_loss: 2.7738\n",
      "Epoch 293/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6442 - val_loss: 2.7407\n",
      "Epoch 294/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6489 - val_loss: 2.7815\n",
      "Epoch 295/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6489 - val_loss: 2.7424\n",
      "Epoch 296/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6541 - val_loss: 2.7876\n",
      "Epoch 297/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6522 - val_loss: 2.7441\n",
      "Epoch 298/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6602 - val_loss: 2.7950\n",
      "Epoch 299/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6567 - val_loss: 2.7465\n",
      "Epoch 300/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6660 - val_loss: 2.8027\n",
      "Epoch 301/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6621 - val_loss: 2.7493\n",
      "Epoch 302/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6706 - val_loss: 2.8046\n",
      "Epoch 303/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6628 - val_loss: 2.7467\n",
      "Epoch 304/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.6680 - val_loss: 2.7998\n",
      "Epoch 305/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6574 - val_loss: 2.7400\n",
      "Epoch 306/600\n",
      "1036/1036 [==============================] - 0s 13us/step - loss: 2.6587 - val_loss: 2.7884\n",
      "Epoch 307/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6460 - val_loss: 2.7347\n",
      "Epoch 308/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6486 - val_loss: 2.7781\n",
      "Epoch 309/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6360 - val_loss: 2.7316\n",
      "Epoch 310/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6374 - val_loss: 2.7692\n",
      "Epoch 311/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6281 - val_loss: 2.7274\n",
      "Epoch 312/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.6246 - val_loss: 2.7517\n",
      "Epoch 313/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6145 - val_loss: 2.7223\n",
      "Epoch 314/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6099 - val_loss: 2.7406\n",
      "Epoch 315/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6076 - val_loss: 2.7251\n",
      "Epoch 316/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6062 - val_loss: 2.7372\n",
      "Epoch 317/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6072 - val_loss: 2.7263\n",
      "Epoch 318/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6045 - val_loss: 2.7342\n",
      "Epoch 319/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6054 - val_loss: 2.7257\n",
      "Epoch 320/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6021 - val_loss: 2.7319\n",
      "Epoch 321/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6028 - val_loss: 2.7252\n",
      "Epoch 322/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6001 - val_loss: 2.7307\n",
      "Epoch 323/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6008 - val_loss: 2.7249\n",
      "Epoch 324/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5985 - val_loss: 2.7299\n",
      "Epoch 325/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5991 - val_loss: 2.7245\n",
      "Epoch 326/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5970 - val_loss: 2.7288\n",
      "Epoch 327/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5971 - val_loss: 2.7239\n",
      "Epoch 328/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5953 - val_loss: 2.7275\n",
      "Epoch 329/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5950 - val_loss: 2.7235\n",
      "Epoch 330/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5936 - val_loss: 2.7262\n",
      "Epoch 331/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5935 - val_loss: 2.7236\n",
      "Epoch 332/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5921 - val_loss: 2.7261\n",
      "Epoch 333/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5947 - val_loss: 2.7260\n",
      "Epoch 334/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5921 - val_loss: 2.7252\n",
      "Epoch 335/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5951 - val_loss: 2.7273\n",
      "Epoch 336/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5912 - val_loss: 2.7236\n",
      "Epoch 337/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5940 - val_loss: 2.7277\n",
      "Epoch 338/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5900 - val_loss: 2.7231\n",
      "Epoch 339/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5931 - val_loss: 2.7281\n",
      "Epoch 340/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5891 - val_loss: 2.7231\n",
      "Epoch 341/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5923 - val_loss: 2.7282\n",
      "Epoch 342/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5880 - val_loss: 2.7229\n",
      "Epoch 343/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5910 - val_loss: 2.7277\n",
      "Epoch 344/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5866 - val_loss: 2.7227\n",
      "Epoch 345/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5895 - val_loss: 2.7272\n",
      "Epoch 346/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5852 - val_loss: 2.7227\n",
      "Epoch 347/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5879 - val_loss: 2.7266\n",
      "Epoch 348/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5839 - val_loss: 2.7227\n",
      "Epoch 349/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.5864 - val_loss: 2.7260\n",
      "Epoch 350/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5825 - val_loss: 2.7228\n",
      "Epoch 351/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5848 - val_loss: 2.7253\n",
      "Epoch 352/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5811 - val_loss: 2.7229\n",
      "Epoch 353/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5833 - val_loss: 2.7247\n",
      "Epoch 354/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5798 - val_loss: 2.7231\n",
      "Epoch 355/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5819 - val_loss: 2.7240\n",
      "Epoch 356/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5786 - val_loss: 2.7234\n",
      "Epoch 357/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5805 - val_loss: 2.7234\n",
      "Epoch 358/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5774 - val_loss: 2.7242\n",
      "Epoch 359/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5793 - val_loss: 2.7226\n",
      "Epoch 360/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5763 - val_loss: 2.7250\n",
      "Epoch 361/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5769 - val_loss: 2.7204\n",
      "Epoch 362/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5752 - val_loss: 2.7284\n",
      "Epoch 363/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5762 - val_loss: 2.7190\n",
      "Epoch 364/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5766 - val_loss: 2.7372\n",
      "Epoch 365/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5790 - val_loss: 2.7200\n",
      "Epoch 366/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5825 - val_loss: 2.7488\n",
      "Epoch 367/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5837 - val_loss: 2.7210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5937 - val_loss: 2.7688\n",
      "Epoch 369/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5977 - val_loss: 2.7399\n",
      "Epoch 370/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6295 - val_loss: 2.8195\n",
      "Epoch 371/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6446 - val_loss: 2.7823\n",
      "Epoch 372/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6872 - val_loss: 2.8782\n",
      "Epoch 373/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6992 - val_loss: 2.8126\n",
      "Epoch 374/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.7307 - val_loss: 2.8962\n",
      "Epoch 375/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.7148 - val_loss: 2.7865\n",
      "Epoch 376/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.7063 - val_loss: 2.8549\n",
      "Epoch 377/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6753 - val_loss: 2.7529\n",
      "Epoch 378/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6675 - val_loss: 2.8127\n",
      "Epoch 379/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6332 - val_loss: 2.7149\n",
      "Epoch 380/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6200 - val_loss: 2.7630\n",
      "Epoch 381/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5830 - val_loss: 2.6831\n",
      "Epoch 382/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5732 - val_loss: 2.7269\n",
      "Epoch 383/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5463 - val_loss: 2.6678\n",
      "Epoch 384/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5481 - val_loss: 2.7127\n",
      "Epoch 385/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5338 - val_loss: 2.6605\n",
      "Epoch 386/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5322 - val_loss: 2.7064\n",
      "Epoch 387/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5258 - val_loss: 2.6585\n",
      "Epoch 388/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5265 - val_loss: 2.7042\n",
      "Epoch 389/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5226 - val_loss: 2.6585\n",
      "Epoch 390/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5241 - val_loss: 2.7087\n",
      "Epoch 391/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5242 - val_loss: 2.6659\n",
      "Epoch 392/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5350 - val_loss: 2.7260\n",
      "Epoch 393/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5401 - val_loss: 2.7026\n",
      "Epoch 394/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5723 - val_loss: 2.7762\n",
      "Epoch 395/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5903 - val_loss: 2.7484\n",
      "Epoch 396/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.6148 - val_loss: 2.7998\n",
      "Epoch 397/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6143 - val_loss: 2.7399\n",
      "Epoch 398/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.6019 - val_loss: 2.7684\n",
      "Epoch 399/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5834 - val_loss: 2.7214\n",
      "Epoch 400/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5782 - val_loss: 2.7539\n",
      "Epoch 401/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5700 - val_loss: 2.7182\n",
      "Epoch 402/600\n",
      "1036/1036 [==============================] - 0s 11us/step - loss: 2.5701 - val_loss: 2.7524\n",
      "Epoch 403/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5698 - val_loss: 2.7209\n",
      "Epoch 404/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5691 - val_loss: 2.7476\n",
      "Epoch 405/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5674 - val_loss: 2.7188\n",
      "Epoch 406/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5639 - val_loss: 2.7410\n",
      "Epoch 407/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5626 - val_loss: 2.7160\n",
      "Epoch 408/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5583 - val_loss: 2.7338\n",
      "Epoch 409/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5565 - val_loss: 2.7144\n",
      "Epoch 410/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5545 - val_loss: 2.7332\n",
      "Epoch 411/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5563 - val_loss: 2.7158\n",
      "Epoch 412/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.5545 - val_loss: 2.7333\n",
      "Epoch 413/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5562 - val_loss: 2.7158\n",
      "Epoch 414/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5532 - val_loss: 2.7322\n",
      "Epoch 415/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5544 - val_loss: 2.7149\n",
      "Epoch 416/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5513 - val_loss: 2.7311\n",
      "Epoch 417/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5524 - val_loss: 2.7142\n",
      "Epoch 418/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5499 - val_loss: 2.7313\n",
      "Epoch 419/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5512 - val_loss: 2.7138\n",
      "Epoch 420/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5491 - val_loss: 2.7320\n",
      "Epoch 421/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5502 - val_loss: 2.7133\n",
      "Epoch 422/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5486 - val_loss: 2.7330\n",
      "Epoch 423/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5493 - val_loss: 2.7124\n",
      "Epoch 424/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5480 - val_loss: 2.7342\n",
      "Epoch 425/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5482 - val_loss: 2.7110\n",
      "Epoch 426/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5474 - val_loss: 2.7358\n",
      "Epoch 427/600\n",
      "1036/1036 [==============================] - 0s 6us/step - loss: 2.5470 - val_loss: 2.7115\n",
      "Epoch 428/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5503 - val_loss: 2.7443\n",
      "Epoch 429/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5518 - val_loss: 2.7146\n",
      "Epoch 430/600\n",
      "1036/1036 [==============================] - 0s 8us/step - loss: 2.5586 - val_loss: 2.7547\n",
      "Epoch 431/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5576 - val_loss: 2.7181\n",
      "Epoch 432/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5694 - val_loss: 2.7655\n",
      "Epoch 433/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5650 - val_loss: 2.7260\n",
      "Epoch 434/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.5875 - val_loss: 2.7904\n",
      "Epoch 435/600\n",
      "1036/1036 [==============================] - 0s 9us/step - loss: 2.5873 - val_loss: 2.7442\n",
      "Epoch 436/600\n",
      "1036/1036 [==============================] - 0s 12us/step - loss: 2.6147 - val_loss: 2.8123\n",
      "Epoch 437/600\n",
      "1036/1036 [==============================] - 0s 15us/step - loss: 2.6071 - val_loss: 2.7540\n",
      "Epoch 438/600\n",
      "1036/1036 [==============================] - 0s 10us/step - loss: 2.6295 - val_loss: 2.8184\n",
      "Epoch 439/600\n",
      "1036/1036 [==============================] - 0s 7us/step - loss: 2.6117 - val_loss: 2.7535\n"
     ]
    }
   ],
   "source": [
    "historyVal_ES = []\n",
    "historyTr_ES = []\n",
    "\n",
    "el=EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_ES()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=1036, callbacks=[el]).history\n",
    "    historyVal_ES.append(history['val_loss'])\n",
    "    historyTr_ES.append(history['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_k=600\n",
    "for i in range(0,5):\n",
    "    if len(historyVal_ES[i])<min_k:\n",
    "        min_k=len(historyVal_ES[i])\n",
    "for i in range(0,5):\n",
    "    del historyVal_ES[i][min_k:]\n",
    "    del historyTr_ES[i][min_k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/403\n",
      "1295/1295 [==============================] - 1s 1ms/step - loss: 57.1528\n",
      "Epoch 2/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 49.9122\n",
      "Epoch 3/403\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 37.1860\n",
      "Epoch 4/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 22.5878\n",
      "Epoch 5/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 16.9672\n",
      "Epoch 6/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 15.3731\n",
      "Epoch 7/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 13.7093\n",
      "Epoch 8/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 11.5272\n",
      "Epoch 9/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 9.7506\n",
      "Epoch 10/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 9.0008\n",
      "Epoch 11/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 8.6299\n",
      "Epoch 12/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 8.4012\n",
      "Epoch 13/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 8.1333\n",
      "Epoch 14/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 7.8777\n",
      "Epoch 15/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 7.6168\n",
      "Epoch 16/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 7.3571\n",
      "Epoch 17/403\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 7.0778\n",
      "Epoch 18/403\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 6.7610\n",
      "Epoch 19/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 6.4519\n",
      "Epoch 20/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 6.2565\n",
      "Epoch 21/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 5.9026\n",
      "Epoch 22/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 5.5923\n",
      "Epoch 23/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 5.2962\n",
      "Epoch 24/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 5.0880\n",
      "Epoch 25/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 4.7652\n",
      "Epoch 26/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 4.4957\n",
      "Epoch 27/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 4.2762\n",
      "Epoch 28/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 4.1235\n",
      "Epoch 29/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 4.0050\n",
      "Epoch 30/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 4.1869\n",
      "Epoch 31/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 3.8355\n",
      "Epoch 32/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 3.9055\n",
      "Epoch 33/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 3.9588\n",
      "Epoch 34/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 3.6771\n",
      "Epoch 35/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 3.5455\n",
      "Epoch 36/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 3.9328\n",
      "Epoch 37/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 3.6124\n",
      "Epoch 38/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.5803\n",
      "Epoch 39/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.5800\n",
      "Epoch 40/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.5832\n",
      "Epoch 41/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 3.3981\n",
      "Epoch 42/403\n",
      "1295/1295 [==============================] - 0s 40us/step - loss: 3.3185\n",
      "Epoch 43/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.2938\n",
      "Epoch 44/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 3.2677\n",
      "Epoch 45/403\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 3.498 - 0s 20us/step - loss: 3.4954\n",
      "Epoch 46/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 3.7172\n",
      "Epoch 47/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.2486\n",
      "Epoch 48/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 3.1761\n",
      "Epoch 49/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.4167\n",
      "Epoch 50/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 3.3784\n",
      "Epoch 51/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.2229\n",
      "Epoch 52/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 3.1910\n",
      "Epoch 53/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 3.2259\n",
      "Epoch 54/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 3.0394\n",
      "Epoch 55/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 3.1124\n",
      "Epoch 56/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 3.1047\n",
      "Epoch 57/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 3.0512\n",
      "Epoch 58/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 3.0267\n",
      "Epoch 59/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 3.0021\n",
      "Epoch 60/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.9859\n",
      "Epoch 61/403\n",
      "1295/1295 [==============================] - 0s 31us/step - loss: 2.9906\n",
      "Epoch 62/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 3.3286\n",
      "Epoch 63/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 3.3706\n",
      "Epoch 64/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.9708\n",
      "Epoch 65/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.9455\n",
      "Epoch 66/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.9394\n",
      "Epoch 67/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.9339\n",
      "Epoch 68/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.9596\n",
      "Epoch 69/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.1122\n",
      "Epoch 70/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.4989\n",
      "Epoch 71/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 3.0921\n",
      "Epoch 72/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.9821\n",
      "Epoch 73/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.9767\n",
      "Epoch 74/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 3.0163\n",
      "Epoch 75/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 3.1131\n",
      "Epoch 76/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.8917\n",
      "Epoch 77/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.8948\n",
      "Epoch 78/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 3.1163\n",
      "Epoch 79/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.9243\n",
      "Epoch 80/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.9958\n",
      "Epoch 81/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 3.1587\n",
      "Epoch 82/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 3.5196\n",
      "Epoch 83/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.9062\n",
      "Epoch 84/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8484\n",
      "Epoch 85/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.8299\n",
      "Epoch 86/403\n",
      "1295/1295 [==============================] - 0s 31us/step - loss: 2.8417\n",
      "Epoch 87/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.9453\n",
      "Epoch 88/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 3.1202\n",
      "Epoch 89/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.0102\n",
      "Epoch 90/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 3.0195\n",
      "Epoch 91/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.9001\n",
      "Epoch 92/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.8915\n",
      "Epoch 93/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.9821\n",
      "Epoch 94/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.8964\n",
      "Epoch 95/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.2123\n",
      "Epoch 96/403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.9187\n",
      "Epoch 97/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.8727\n",
      "Epoch 98/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.8964\n",
      "Epoch 99/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.8015\n",
      "Epoch 100/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.9052\n",
      "Epoch 101/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 3.1533\n",
      "Epoch 102/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8282\n",
      "Epoch 103/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.9723\n",
      "Epoch 104/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 3.2924\n",
      "Epoch 105/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.8611\n",
      "Epoch 106/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 3.1214\n",
      "Epoch 107/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.9612\n",
      "Epoch 108/403\n",
      "1295/1295 [==============================] - 0s 39us/step - loss: 2.7546\n",
      "Epoch 109/403\n",
      "1295/1295 [==============================] - 0s 51us/step - loss: 2.7909\n",
      "Epoch 110/403\n",
      "1295/1295 [==============================] - 0s 36us/step - loss: 2.7629\n",
      "Epoch 111/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.8285\n",
      "Epoch 112/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.7633\n",
      "Epoch 113/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.8947\n",
      "Epoch 114/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.7816\n",
      "Epoch 115/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.7785\n",
      "Epoch 116/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8592\n",
      "Epoch 117/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.8466\n",
      "Epoch 118/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7723\n",
      "Epoch 119/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.7749\n",
      "Epoch 120/403\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.917 - 0s 27us/step - loss: 2.9080\n",
      "Epoch 121/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.8331\n",
      "Epoch 122/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.8737\n",
      "Epoch 123/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8517\n",
      "Epoch 124/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.8177\n",
      "Epoch 125/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8235\n",
      "Epoch 126/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.7212\n",
      "Epoch 127/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7146\n",
      "Epoch 128/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.8618\n",
      "Epoch 129/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.7574\n",
      "Epoch 130/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6932\n",
      "Epoch 131/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.7388\n",
      "Epoch 132/403\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.7920\n",
      "Epoch 133/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.9146\n",
      "Epoch 134/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.7889\n",
      "Epoch 135/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.7453\n",
      "Epoch 136/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6881\n",
      "Epoch 137/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.9277\n",
      "Epoch 138/403\n",
      "1295/1295 [==============================] - 0s 35us/step - loss: 3.0714\n",
      "Epoch 139/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8591\n",
      "Epoch 140/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.7437\n",
      "Epoch 141/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7885\n",
      "Epoch 142/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.8706\n",
      "Epoch 143/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.8409\n",
      "Epoch 144/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.9312\n",
      "Epoch 145/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.9218\n",
      "Epoch 146/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.6781\n",
      "Epoch 147/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6548\n",
      "Epoch 148/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.7949\n",
      "Epoch 149/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 3.0160\n",
      "Epoch 150/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.8427\n",
      "Epoch 151/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7492\n",
      "Epoch 152/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7594\n",
      "Epoch 153/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6700\n",
      "Epoch 154/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6522\n",
      "Epoch 155/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6794\n",
      "Epoch 156/403\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.6687\n",
      "Epoch 157/403\n",
      "1295/1295 [==============================] - 0s 46us/step - loss: 2.7637\n",
      "Epoch 158/403\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.7181\n",
      "Epoch 159/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6958\n",
      "Epoch 160/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6383\n",
      "Epoch 161/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7389\n",
      "Epoch 162/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.7757\n",
      "Epoch 163/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7968\n",
      "Epoch 164/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6889\n",
      "Epoch 165/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6330\n",
      "Epoch 166/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6860\n",
      "Epoch 167/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.9206\n",
      "Epoch 168/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6586\n",
      "Epoch 169/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8207\n",
      "Epoch 170/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6502\n",
      "Epoch 171/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.6191\n",
      "Epoch 172/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.7862\n",
      "Epoch 173/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.6774\n",
      "Epoch 174/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.6267\n",
      "Epoch 175/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6340\n",
      "Epoch 176/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.6223\n",
      "Epoch 177/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6149\n",
      "Epoch 178/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6561\n",
      "Epoch 179/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.8145\n",
      "Epoch 180/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8455\n",
      "Epoch 181/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7366\n",
      "Epoch 182/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.6789\n",
      "Epoch 183/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.7443\n",
      "Epoch 184/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6510\n",
      "Epoch 185/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.6137\n",
      "Epoch 186/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.7588\n",
      "Epoch 187/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8141\n",
      "Epoch 188/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.8490\n",
      "Epoch 189/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.9442\n",
      "Epoch 190/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6946\n",
      "Epoch 191/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6823\n",
      "Epoch 192/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7856\n",
      "Epoch 193/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7619\n",
      "Epoch 194/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.7386\n",
      "Epoch 195/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6079\n",
      "Epoch 196/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6251\n",
      "Epoch 197/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6831\n",
      "Epoch 198/403\n",
      "1295/1295 [==============================] - 0s 30us/step - loss: 2.7435\n",
      "Epoch 199/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.7876\n",
      "Epoch 200/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5932\n",
      "Epoch 201/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6211\n",
      "Epoch 202/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6541\n",
      "Epoch 203/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5944\n",
      "Epoch 204/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.7081\n",
      "Epoch 205/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6200\n",
      "Epoch 206/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6407\n",
      "Epoch 207/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.8965\n",
      "Epoch 208/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.7050\n",
      "Epoch 209/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.7521\n",
      "Epoch 210/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.8013\n",
      "Epoch 211/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.7113\n",
      "Epoch 212/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5692\n",
      "Epoch 213/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.5494\n",
      "Epoch 214/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.7098\n",
      "Epoch 215/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 3.1346\n",
      "Epoch 216/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.5747\n",
      "Epoch 217/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6010\n",
      "Epoch 218/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5912\n",
      "Epoch 219/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5761\n",
      "Epoch 220/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.8049\n",
      "Epoch 221/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6621\n",
      "Epoch 222/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5789\n",
      "Epoch 223/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6635\n",
      "Epoch 224/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.8092\n",
      "Epoch 225/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5674\n",
      "Epoch 226/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5856\n",
      "Epoch 227/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5999\n",
      "Epoch 228/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5430\n",
      "Epoch 229/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5662\n",
      "Epoch 230/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.6665\n",
      "Epoch 231/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5373\n",
      "Epoch 232/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.7459\n",
      "Epoch 233/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6418\n",
      "Epoch 234/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5189\n",
      "Epoch 235/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6251\n",
      "Epoch 236/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.6162\n",
      "Epoch 237/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5414\n",
      "Epoch 238/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5622\n",
      "Epoch 239/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5323\n",
      "Epoch 240/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6041\n",
      "Epoch 241/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6389\n",
      "Epoch 242/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6697\n",
      "Epoch 243/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6459\n",
      "Epoch 244/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.5214\n",
      "Epoch 245/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5823\n",
      "Epoch 246/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.7767\n",
      "Epoch 247/403\n",
      "1295/1295 [==============================] - 0s 32us/step - loss: 2.7879\n",
      "Epoch 248/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5853\n",
      "Epoch 249/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5316\n",
      "Epoch 250/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5236\n",
      "Epoch 251/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5740\n",
      "Epoch 252/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5608\n",
      "Epoch 253/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.6996\n",
      "Epoch 254/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5946\n",
      "Epoch 255/403\n",
      "1295/1295 [==============================] - 0s 7us/step - loss: 2.6436\n",
      "Epoch 256/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.7114\n",
      "Epoch 257/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.8485\n",
      "Epoch 258/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7147\n",
      "Epoch 259/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5493\n",
      "Epoch 260/403\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.5680\n",
      "Epoch 261/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5492\n",
      "Epoch 262/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.7501\n",
      "Epoch 263/403\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.5863\n",
      "Epoch 264/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5784\n",
      "Epoch 265/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6475\n",
      "Epoch 266/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6488\n",
      "Epoch 267/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.7202\n",
      "Epoch 268/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.4961\n",
      "Epoch 269/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6899\n",
      "Epoch 270/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.5400\n",
      "Epoch 271/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5840\n",
      "Epoch 272/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6893\n",
      "Epoch 273/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6310\n",
      "Epoch 274/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.5977\n",
      "Epoch 275/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5100\n",
      "Epoch 276/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.5039\n",
      "Epoch 277/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5731\n",
      "Epoch 278/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5649\n",
      "Epoch 279/403\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.5690\n",
      "Epoch 280/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.5851\n",
      "Epoch 281/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5149\n",
      "Epoch 282/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.7440\n",
      "Epoch 283/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5854\n",
      "Epoch 284/403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5001\n",
      "Epoch 285/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.5724\n",
      "Epoch 286/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5308\n",
      "Epoch 287/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4946\n",
      "Epoch 288/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.4975\n",
      "Epoch 289/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.5384\n",
      "Epoch 290/403\n",
      "1295/1295 [==============================] - 0s 46us/step - loss: 2.6307\n",
      "Epoch 291/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6714\n",
      "Epoch 292/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5664\n",
      "Epoch 293/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5693\n",
      "Epoch 294/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6955\n",
      "Epoch 295/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.4939\n",
      "Epoch 296/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.6305\n",
      "Epoch 297/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5634\n",
      "Epoch 298/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.5464\n",
      "Epoch 299/403\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.7649\n",
      "Epoch 300/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4990\n",
      "Epoch 301/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4795\n",
      "Epoch 302/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.7163\n",
      "Epoch 303/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5166\n",
      "Epoch 304/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4712\n",
      "Epoch 305/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6283\n",
      "Epoch 306/403\n",
      "1295/1295 [==============================] - ETA: 0s - loss: 2.469 - 0s 12us/step - loss: 2.4878\n",
      "Epoch 307/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.8237\n",
      "Epoch 308/403\n",
      "1295/1295 [==============================] - 0s 29us/step - loss: 2.4699\n",
      "Epoch 309/403\n",
      "1295/1295 [==============================] - 0s 24us/step - loss: 2.6241\n",
      "Epoch 310/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.6245\n",
      "Epoch 311/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5207\n",
      "Epoch 312/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.5314\n",
      "Epoch 313/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.4910\n",
      "Epoch 314/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4741\n",
      "Epoch 315/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4606\n",
      "Epoch 316/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4865\n",
      "Epoch 317/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.7113\n",
      "Epoch 318/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.4918\n",
      "Epoch 319/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5394\n",
      "Epoch 320/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4452\n",
      "Epoch 321/403\n",
      "1295/1295 [==============================] - 0s 27us/step - loss: 2.4634\n",
      "Epoch 322/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4422\n",
      "Epoch 323/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.6300\n",
      "Epoch 324/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.9527\n",
      "Epoch 325/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.5149\n",
      "Epoch 326/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5644\n",
      "Epoch 327/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5862\n",
      "Epoch 328/403\n",
      "1295/1295 [==============================] - 0s 8us/step - loss: 2.4798\n",
      "Epoch 329/403\n",
      "1295/1295 [==============================] - 0s 26us/step - loss: 2.4847\n",
      "Epoch 330/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4673\n",
      "Epoch 331/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5882\n",
      "Epoch 332/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.6233\n",
      "Epoch 333/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.5725\n",
      "Epoch 334/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 3.0673\n",
      "Epoch 335/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5245\n",
      "Epoch 336/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.5842\n",
      "Epoch 337/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5777\n",
      "Epoch 338/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5788\n",
      "Epoch 339/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.5694\n",
      "Epoch 340/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5596\n",
      "Epoch 341/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.5595\n",
      "Epoch 342/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.4430\n",
      "Epoch 343/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4494\n",
      "Epoch 344/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4491\n",
      "Epoch 345/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5746\n",
      "Epoch 346/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.4756\n",
      "Epoch 347/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.5065\n",
      "Epoch 348/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.4920\n",
      "Epoch 349/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4992\n",
      "Epoch 350/403\n",
      "1295/1295 [==============================] - 0s 23us/step - loss: 2.5999\n",
      "Epoch 351/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.4762\n",
      "Epoch 352/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.5007\n",
      "Epoch 353/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.4962\n",
      "Epoch 354/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6297\n",
      "Epoch 355/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.5291\n",
      "Epoch 356/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.5317\n",
      "Epoch 357/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.4568\n",
      "Epoch 358/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.4420\n",
      "Epoch 359/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4219\n",
      "Epoch 360/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.4756\n",
      "Epoch 361/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6020\n",
      "Epoch 362/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.5827\n",
      "Epoch 363/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.5706\n",
      "Epoch 364/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4919\n",
      "Epoch 365/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4975\n",
      "Epoch 366/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.4735\n",
      "Epoch 367/403\n",
      "1295/1295 [==============================] - 0s 31us/step - loss: 2.5354\n",
      "Epoch 368/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.5055\n",
      "Epoch 369/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.6106\n",
      "Epoch 370/403\n",
      "1295/1295 [==============================] - 0s 20us/step - loss: 2.5625\n",
      "Epoch 371/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5419\n",
      "Epoch 372/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5422\n",
      "Epoch 373/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.5682\n",
      "Epoch 374/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5096\n",
      "Epoch 375/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.5467\n",
      "Epoch 376/403\n",
      "1295/1295 [==============================] - 0s 25us/step - loss: 2.5247\n",
      "Epoch 377/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4745\n",
      "Epoch 378/403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.6469\n",
      "Epoch 379/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.6713\n",
      "Epoch 380/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.6386\n",
      "Epoch 381/403\n",
      "1295/1295 [==============================] - 0s 14us/step - loss: 2.4455\n",
      "Epoch 382/403\n",
      "1295/1295 [==============================] - 0s 12us/step - loss: 2.4526\n",
      "Epoch 383/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4479\n",
      "Epoch 384/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.4781\n",
      "Epoch 385/403\n",
      "1295/1295 [==============================] - 0s 15us/step - loss: 2.3857\n",
      "Epoch 386/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.4334\n",
      "Epoch 387/403\n",
      "1295/1295 [==============================] - 0s 34us/step - loss: 2.6638\n",
      "Epoch 388/403\n",
      "1295/1295 [==============================] - 0s 21us/step - loss: 2.5599\n",
      "Epoch 389/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.3936\n",
      "Epoch 390/403\n",
      "1295/1295 [==============================] - 0s 16us/step - loss: 2.4804\n",
      "Epoch 391/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5809\n",
      "Epoch 392/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4509\n",
      "Epoch 393/403\n",
      "1295/1295 [==============================] - 0s 10us/step - loss: 2.4113\n",
      "Epoch 394/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.4542\n",
      "Epoch 395/403\n",
      "1295/1295 [==============================] - 0s 22us/step - loss: 2.4395\n",
      "Epoch 396/403\n",
      "1295/1295 [==============================] - 0s 9us/step - loss: 2.4148\n",
      "Epoch 397/403\n",
      "1295/1295 [==============================] - 0s 11us/step - loss: 2.4027\n",
      "Epoch 398/403\n",
      "1295/1295 [==============================] - 0s 18us/step - loss: 2.3815\n",
      "Epoch 399/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.4127\n",
      "Epoch 400/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.4192\n",
      "Epoch 401/403\n",
      "1295/1295 [==============================] - 0s 13us/step - loss: 2.5181\n",
      "Epoch 402/403\n",
      "1295/1295 [==============================] - 0s 19us/step - loss: 2.5158\n",
      "Epoch 403/403\n",
      "1295/1295 [==============================] - 0s 17us/step - loss: 2.4109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [57.15279006958008,\n",
       "  49.91223373413086,\n",
       "  37.18602981567383,\n",
       "  22.587818908691407,\n",
       "  16.967239379882812,\n",
       "  15.37314567565918,\n",
       "  13.70934829711914,\n",
       "  11.527245140075683,\n",
       "  9.750591468811034,\n",
       "  9.000824737548829,\n",
       "  8.629863166809082,\n",
       "  8.40115737915039,\n",
       "  8.13329200744629,\n",
       "  7.877731132507324,\n",
       "  7.616835021972657,\n",
       "  7.357122993469238,\n",
       "  7.077778911590576,\n",
       "  6.761023426055909,\n",
       "  6.45189905166626,\n",
       "  6.256530094146728,\n",
       "  5.902564239501953,\n",
       "  5.59234037399292,\n",
       "  5.296206474304199,\n",
       "  5.087981796264648,\n",
       "  4.765192604064941,\n",
       "  4.49569091796875,\n",
       "  4.2762017250061035,\n",
       "  4.123544597625733,\n",
       "  4.0050146102905275,\n",
       "  4.186907386779785,\n",
       "  3.835459756851196,\n",
       "  3.9054887771606444,\n",
       "  3.958791732788086,\n",
       "  3.67708044052124,\n",
       "  3.545460891723633,\n",
       "  3.9327532768249513,\n",
       "  3.6124481678009035,\n",
       "  3.5802958011627197,\n",
       "  3.579975128173828,\n",
       "  3.5831503868103027,\n",
       "  3.3980677127838135,\n",
       "  3.31854510307312,\n",
       "  3.293759489059448,\n",
       "  3.2676854610443113,\n",
       "  3.49540057182312,\n",
       "  3.717167615890503,\n",
       "  3.248642587661743,\n",
       "  3.1761351585388184,\n",
       "  3.4167247295379637,\n",
       "  3.3784246921539305,\n",
       "  3.2229095935821532,\n",
       "  3.191049909591675,\n",
       "  3.2259172439575194,\n",
       "  3.0394218444824217,\n",
       "  3.1124401092529297,\n",
       "  3.104679822921753,\n",
       "  3.0511781692504885,\n",
       "  3.0266705989837646,\n",
       "  3.002132940292358,\n",
       "  2.985878276824951,\n",
       "  2.9905880451202393,\n",
       "  3.328635883331299,\n",
       "  3.370576810836792,\n",
       "  2.970781850814819,\n",
       "  2.9455429553985595,\n",
       "  2.9393593788146974,\n",
       "  2.9339308738708496,\n",
       "  2.959562873840332,\n",
       "  3.1122448444366455,\n",
       "  3.4988802433013917,\n",
       "  3.0920648097991945,\n",
       "  2.982051467895508,\n",
       "  2.976709508895874,\n",
       "  3.016287326812744,\n",
       "  3.1130949020385743,\n",
       "  2.8916587352752687,\n",
       "  2.8947594165802,\n",
       "  3.116253471374512,\n",
       "  2.924261140823364,\n",
       "  2.9957528114318848,\n",
       "  3.1587188243865967,\n",
       "  3.519621658325195,\n",
       "  2.9061552047729493,\n",
       "  2.8483847618103026,\n",
       "  2.8299341678619383,\n",
       "  2.841729688644409,\n",
       "  2.9453383922576903,\n",
       "  3.1201799869537354,\n",
       "  3.0102415084838867,\n",
       "  3.0194617748260497,\n",
       "  2.900092363357544,\n",
       "  2.891536569595337,\n",
       "  2.9820680141448976,\n",
       "  2.89638032913208,\n",
       "  3.212271213531494,\n",
       "  2.9186963558197023,\n",
       "  2.8726761817932127,\n",
       "  2.8963832378387453,\n",
       "  2.801505613327026,\n",
       "  2.905189943313599,\n",
       "  3.153338670730591,\n",
       "  2.8281602382659914,\n",
       "  2.972311544418335,\n",
       "  3.2923580169677735,\n",
       "  2.8611438274383545,\n",
       "  3.121390676498413,\n",
       "  2.961178493499756,\n",
       "  2.7545877933502196,\n",
       "  2.790890598297119,\n",
       "  2.7629441738128664,\n",
       "  2.8285477638244627,\n",
       "  2.7632781982421877,\n",
       "  2.894747734069824,\n",
       "  2.781623697280884,\n",
       "  2.7784893035888674,\n",
       "  2.8592191696166993,\n",
       "  2.8465935230255126,\n",
       "  2.772329044342041,\n",
       "  2.774863052368164,\n",
       "  2.9080320835113525,\n",
       "  2.8330917835235594,\n",
       "  2.8736963272094727,\n",
       "  2.8517147064208985,\n",
       "  2.81767201423645,\n",
       "  2.823489046096802,\n",
       "  2.7211541175842284,\n",
       "  2.7146048545837402,\n",
       "  2.861832332611084,\n",
       "  2.7574427127838135,\n",
       "  2.6931806564331056,\n",
       "  2.7388001918792724,\n",
       "  2.7919694423675536,\n",
       "  2.9146159172058104,\n",
       "  2.788929843902588,\n",
       "  2.745263624191284,\n",
       "  2.6880611419677733,\n",
       "  2.9276758193969727,\n",
       "  3.071374034881592,\n",
       "  2.859129810333252,\n",
       "  2.743739604949951,\n",
       "  2.788513946533203,\n",
       "  2.8706208229064942,\n",
       "  2.8408913135528566,\n",
       "  2.9311588764190675,\n",
       "  2.9217724800109863,\n",
       "  2.678110408782959,\n",
       "  2.6547531127929687,\n",
       "  2.794897127151489,\n",
       "  3.015981101989746,\n",
       "  2.8426600456237794,\n",
       "  2.749154806137085,\n",
       "  2.759413242340088,\n",
       "  2.670048236846924,\n",
       "  2.652193880081177,\n",
       "  2.6794354915618896,\n",
       "  2.6686554908752442,\n",
       "  2.7637279510498045,\n",
       "  2.718122673034668,\n",
       "  2.6958200931549072,\n",
       "  2.6382779121398925,\n",
       "  2.7389304637908936,\n",
       "  2.7757097244262696,\n",
       "  2.79678635597229,\n",
       "  2.6888640880584718,\n",
       "  2.6329852104187013,\n",
       "  2.6859604835510256,\n",
       "  2.9205672264099123,\n",
       "  2.6586469173431397,\n",
       "  2.820745897293091,\n",
       "  2.650166940689087,\n",
       "  2.619099426269531,\n",
       "  2.786159896850586,\n",
       "  2.67742600440979,\n",
       "  2.6267027854919434,\n",
       "  2.63401312828064,\n",
       "  2.6223003387451174,\n",
       "  2.6148754119873048,\n",
       "  2.6560973644256594,\n",
       "  2.814481735229492,\n",
       "  2.845463752746582,\n",
       "  2.736605501174927,\n",
       "  2.678914022445679,\n",
       "  2.7443384170532226,\n",
       "  2.651026391983032,\n",
       "  2.6137351036071776,\n",
       "  2.7587812900543214,\n",
       "  2.8140639781951906,\n",
       "  2.84899959564209,\n",
       "  2.944213628768921,\n",
       "  2.6945560455322264,\n",
       "  2.68231463432312,\n",
       "  2.785617780685425,\n",
       "  2.7619128704071043,\n",
       "  2.738625478744507,\n",
       "  2.6078985214233397,\n",
       "  2.625125503540039,\n",
       "  2.6830556869506834,\n",
       "  2.7434691429138183,\n",
       "  2.78759651184082,\n",
       "  2.593237209320068,\n",
       "  2.6210797309875487,\n",
       "  2.654076671600342,\n",
       "  2.59436149597168,\n",
       "  2.708107900619507,\n",
       "  2.619971752166748,\n",
       "  2.6406569480895996,\n",
       "  2.896474838256836,\n",
       "  2.705028200149536,\n",
       "  2.752075958251953,\n",
       "  2.8013181686401367,\n",
       "  2.711342477798462,\n",
       "  2.5692395210266112,\n",
       "  2.5493911266326905,\n",
       "  2.709847259521484,\n",
       "  3.1346063137054445,\n",
       "  2.574655866622925,\n",
       "  2.600965213775635,\n",
       "  2.59124493598938,\n",
       "  2.5761111259460447,\n",
       "  2.8049020290374758,\n",
       "  2.6621182918548585,\n",
       "  2.57885947227478,\n",
       "  2.66345796585083,\n",
       "  2.8091720581054687,\n",
       "  2.5673923969268797,\n",
       "  2.5856454372406006,\n",
       "  2.599932622909546,\n",
       "  2.5430053234100343,\n",
       "  2.5661740779876707,\n",
       "  2.6664620876312255,\n",
       "  2.537322187423706,\n",
       "  2.74589467048645,\n",
       "  2.641798734664917,\n",
       "  2.518926906585693,\n",
       "  2.625140380859375,\n",
       "  2.616229200363159,\n",
       "  2.5413764476776124,\n",
       "  2.56220121383667,\n",
       "  2.532266092300415,\n",
       "  2.604102897644043,\n",
       "  2.638868284225464,\n",
       "  2.6696510791778563,\n",
       "  2.645891046524048,\n",
       "  2.5213551998138426,\n",
       "  2.582252311706543,\n",
       "  2.7766512393951417,\n",
       "  2.7879072189331056,\n",
       "  2.585310697555542,\n",
       "  2.5316086292266844,\n",
       "  2.5236063957214356,\n",
       "  2.574043846130371,\n",
       "  2.5607999324798585,\n",
       "  2.6995935440063477,\n",
       "  2.5945512294769286,\n",
       "  2.6435577392578127,\n",
       "  2.7114123344421386,\n",
       "  2.8485326290130617,\n",
       "  2.714722728729248,\n",
       "  2.5492912769317626,\n",
       "  2.568008470535278,\n",
       "  2.549225616455078,\n",
       "  2.750118923187256,\n",
       "  2.586278200149536,\n",
       "  2.578425741195679,\n",
       "  2.6475046157836912,\n",
       "  2.64878396987915,\n",
       "  2.7202478408813477,\n",
       "  2.4961015701293947,\n",
       "  2.689935255050659,\n",
       "  2.539974308013916,\n",
       "  2.5839867115020754,\n",
       "  2.6893399238586424,\n",
       "  2.630957746505737,\n",
       "  2.5976943969726562,\n",
       "  2.510044717788696,\n",
       "  2.503916883468628,\n",
       "  2.5731157779693605,\n",
       "  2.5648844242095947,\n",
       "  2.569004011154175,\n",
       "  2.5850924491882323,\n",
       "  2.5149121284484863,\n",
       "  2.7439685344696043,\n",
       "  2.5854207992553713,\n",
       "  2.5000987529754637,\n",
       "  2.5723782539367677,\n",
       "  2.5307985305786134,\n",
       "  2.4945908069610594,\n",
       "  2.497547674179077,\n",
       "  2.5384035110473633,\n",
       "  2.630690336227417,\n",
       "  2.6714286327362062,\n",
       "  2.5663809299468996,\n",
       "  2.5692802906036376,\n",
       "  2.6955212116241456,\n",
       "  2.493899917602539,\n",
       "  2.6305182933807374,\n",
       "  2.5634052753448486,\n",
       "  2.5463991165161133,\n",
       "  2.764903259277344,\n",
       "  2.4990434646606445,\n",
       "  2.4794928550720217,\n",
       "  2.7163200855255125,\n",
       "  2.5166085720062257,\n",
       "  2.471209907531738,\n",
       "  2.6283409118652346,\n",
       "  2.4878009796142577,\n",
       "  2.8236578941345214,\n",
       "  2.469899559020996,\n",
       "  2.6241347789764404,\n",
       "  2.6245102405548097,\n",
       "  2.5207240104675295,\n",
       "  2.5314420223236085,\n",
       "  2.490967893600464,\n",
       "  2.4741257667541503,\n",
       "  2.460577917098999,\n",
       "  2.4865307807922363,\n",
       "  2.711275911331177,\n",
       "  2.4917810440063475,\n",
       "  2.5393749237060548,\n",
       "  2.4452389240264893,\n",
       "  2.4634352207183836,\n",
       "  2.4422109603881834,\n",
       "  2.629987907409668,\n",
       "  2.9527392387390137,\n",
       "  2.514924669265747,\n",
       "  2.5644340038299562,\n",
       "  2.586233615875244,\n",
       "  2.479813003540039,\n",
       "  2.4846913814544678,\n",
       "  2.4673030853271483,\n",
       "  2.5881910800933836,\n",
       "  2.6233243465423586,\n",
       "  2.5725287437438964,\n",
       "  3.0672680377960204,\n",
       "  2.524544668197632,\n",
       "  2.5841660022735597,\n",
       "  2.577744245529175,\n",
       "  2.5787673950195313,\n",
       "  2.569365072250366,\n",
       "  2.559646987915039,\n",
       "  2.5594954013824465,\n",
       "  2.4430051326751707,\n",
       "  2.4494152545928953,\n",
       "  2.4490723609924316,\n",
       "  2.5745651721954346,\n",
       "  2.475608253479004,\n",
       "  2.506506586074829,\n",
       "  2.4920269012451173,\n",
       "  2.4992426872253417,\n",
       "  2.599864673614502,\n",
       "  2.4762386322021483,\n",
       "  2.5006659030914307,\n",
       "  2.496186685562134,\n",
       "  2.629686641693115,\n",
       "  2.52907075881958,\n",
       "  2.5317368507385254,\n",
       "  2.456830358505249,\n",
       "  2.441990613937378,\n",
       "  2.421881151199341,\n",
       "  2.4756076335906982,\n",
       "  2.602002191543579,\n",
       "  2.582696771621704,\n",
       "  2.5706125259399415,\n",
       "  2.491949701309204,\n",
       "  2.497496747970581,\n",
       "  2.4735387325286866,\n",
       "  2.535352039337158,\n",
       "  2.5055257797241213,\n",
       "  2.6106298923492433,\n",
       "  2.562498617172241,\n",
       "  2.5419177055358886,\n",
       "  2.542194890975952,\n",
       "  2.5681832313537596,\n",
       "  2.5096359252929688,\n",
       "  2.5466814994812013,\n",
       "  2.5246707916259767,\n",
       "  2.4745255947113036,\n",
       "  2.64688982963562,\n",
       "  2.6712769985198976,\n",
       "  2.6385584831237794,\n",
       "  2.445522832870483,\n",
       "  2.4525879859924316,\n",
       "  2.447891283035278,\n",
       "  2.478105163574219,\n",
       "  2.3857203483581544,\n",
       "  2.4333813190460205,\n",
       "  2.663821887969971,\n",
       "  2.559903621673584,\n",
       "  2.393597459793091,\n",
       "  2.4803855419158936,\n",
       "  2.5809489250183106,\n",
       "  2.4509065628051756,\n",
       "  2.411258506774902,\n",
       "  2.454150581359863,\n",
       "  2.439518165588379,\n",
       "  2.414762353897095,\n",
       "  2.4026983737945558,\n",
       "  2.3815303802490235,\n",
       "  2.412662220001221,\n",
       "  2.419237184524536,\n",
       "  2.5181499481201173,\n",
       "  2.515845537185669,\n",
       "  2.41086893081665]}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_ES = ModelCheckpoint('Early_stopping_BEST.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model=create_model_ES()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=min_k, \n",
    "                      batch_size=1036, callbacks=[mc_ES]).history\n",
    "\n",
    "model.save(\"Models/Early_stopping_BEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_ES_mean=np.mean(historyVal_ES, axis=0)\n",
    "historyTr_ES_mean=np.mean(historyTr_ES, axis=0)\n",
    "\n",
    "historyVal_ES_sd=np.std(historyVal_ES, axis=0)\n",
    "historyTr_ES_sd=np.std(historyTr_ES, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_ES_mean)),historyTr_ES_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_ES_mean)), historyTr_ES_mean - historyTr_ES_sd, \n",
    "                 historyTr_ES_mean + historyTr_ES_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_ES_mean)), historyVal_ES_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_ES_mean)), historyVal_ES_mean - historyVal_ES_sd, \n",
    "                 historyVal_ES_mean + historyVal_ES_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.title('Learning curves early stopping', fontsize = 18)\n",
    "plt.legend()\n",
    "plt.ylim(5,20)\n",
    "plt.xlim(-5,min_k)\n",
    "plt.yticks(np.arange(0, 20, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model with Early Stopping result:\n",
      "MEE on the validation 2.8313031673431395 with standard deviation 0.13222235544506072\n",
      "MEE on the training 2.5731084823608397 with standard deviation 0.04089528889913201\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model with Early Stopping result:\")\n",
    "print(\"MEE on the validation\",historyVal_ES_mean[-1],\"with standard deviation\",historyVal_ES_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_ES_mean[-1],\"with standard deviation\",historyTr_ES_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "43b5c63db261ca5051cb55de1c6286202162451824cb753635f4f1f1c93d73cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
